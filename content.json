{"meta":{"title":"小吴の博客","subtitle":"高冷白羊","description":"我的博客","author":"Snowman","url":"https://snowman12137.github.io","root":"/"},"pages":[{"title":"about","date":"2023-04-16T12:45:28.000Z","updated":"2023-04-16T12:46:28.171Z","comments":true,"path":"about/index.html","permalink":"https://snowman12137.github.io/about/index.html","excerpt":"","text":"关于我这是一个测试"},{"title":"标签","date":"2023-04-16T12:47:01.000Z","updated":"2023-04-16T12:50:20.107Z","comments":true,"path":"tags/index.html","permalink":"https://snowman12137.github.io/tags/index.html","excerpt":"","text":"标签页我是标签页"},{"title":"categories","date":"2023-04-16T12:47:12.000Z","updated":"2023-04-16T12:50:05.537Z","comments":true,"path":"categories/index.html","permalink":"https://snowman12137.github.io/categories/index.html","excerpt":"","text":"分类页我是分类页"}],"posts":[{"title":"metaGEM使用小记(解决各种问题)2024.1(一)","slug":"metaGEM使用小记-解决各种问题-2024-1-2a55eec867d5441b9829bd5428024882","date":"2024-01-27T16:00:00.000Z","updated":"2024-01-28T08:51:47.058Z","comments":true,"path":"2024/01/28/metaGEM使用小记-解决各种问题-2024-1-2a55eec867d5441b9829bd5428024882/","link":"","permalink":"https://snowman12137.github.io/2024/01/28/metaGEM%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0-%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E9%97%AE%E9%A2%98-2024-1-2a55eec867d5441b9829bd5428024882/","excerpt":"","text":"metaGEM使用小记(解决各种问题)2024.1前言，如何把数据从百度云上下载到linux服务器上可以直接通过pip下载：pip install bypy -y 第一次使用时需要随便输入一个命令以激活授权界面，如输入 bypy info 然后打开提示的连接 将复制的内容粘贴到终端后回车，等待即可。 登陆成功后会提示如下信息 登录百度网盘(我的应用数据&#x2F;bypy&#x2F;你的文件名&#x2F;*&#x2F;文件) 有多个文件建议十个放到一个文件夹里，这样下载出错方便排查 我写了一个脚本可以自动安装download.sh 1234567bypy downdir /RAW-NCBI/0/ ./0/bypy downdir /RAW-NCBI/1/ ./1/bypy downdir /RAW-NCBI/2/ ./2/bypy downdir /RAW-NCBI/3/ ./3/bypy downdir /RAW-NCBI/4/ ./4/bypy downdir /RAW-NCBI/5/ ./5/bypy downdir /RAW-NCBI/6/ ./6/ 因为我有74个文件，因此分成了7组进行下载 在你想存储的文件夹里输入 1nohup bash [download.sh](http://download.sh) &gt; temp.txt &amp; (nohup是在Linux中永久运行的命令，&amp;和其他方式均会因为终端退出而中断。&gt;把下载的过程信息存储到temp.txt，&amp;并放在后台运行，这样下载文件的任务就会自动放到后台了) A.安装123git clone https://github.com/franciscozorrilla/metaGEM.gitcd metaGEMbash env_setup.sh 1.找不到env_setup.sh路径env_setup.sh放在了metaGEM&#x2F;workflow&#x2F;scripts&#x2F;env_setup.sh 但是env_setup.sh里面所有的文件路径是在metaGEM&#x2F;workflow&#x2F;下面的，所以要把env_setup.sh复制到metaGEM&#x2F;workflow&#x2F;下面运行 详解env_setup.sh文件(提取其中关键部分)(已安装anaconda) 123456789conda create -n mamba mamba -c conda-forge #创建新环境下载mabaconda activate mamba #进入mamba环境mamba env create --prefix ./envs/metagem -f envs/metaGEM_env.yml &amp;&amp; source activate envs/metagem &amp;&amp; pip install --user memote carveme smetana #创建metaGEM环境mamba env create --prefix ./envs/metawrap -f envs/metaWRAP_env.yml #创建metaWRAP环境mamba env create --prefix ./envs/prokkaroary -f envs/prokkaroary_env.yml #创建prokkaroary环境#不重要:download-db.sh &amp;&amp; source deactivate &amp;&amp; source activate mamba#下载GTDB-tk database (~25 Gb)数据集(丢失download-db.sh文件无法下载)wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz #下载CheckM(275MB) 数据集 因为conda install 下载速度很慢，所以脚本里使用了mamba方式进行下载。使用方法为下载mamba包，然后在此环境下进行下载，如 mamba install requests 上述文件中mamba env create为创建新环境的语句，-f后面的.yml文件为导出的conda标准环境文件，–prefix 为新创建环境的路径 2.在安装metagem时以下界面卡住 在正常加载时，以下界面至少卡住了20h以上，因此排除网络问题 我们看一下对应的metaGEM_env.yml文件内容 该文件为conda标准创建环境的文件格式 123456789101112131415161718192021name: metagemchannels: - conda-forge - bioconda - defaultsdependencies: - bedtools&gt;=2.29.2 - bwa&gt;=0.7.17 - concoct&gt;=1.1.0 - diamond&gt;=2.0.6 - fastp&gt;=0.20.1 - gtdbtk&gt;=1.4.0 - maxbin2&gt;=2.2.7 - megahit&gt;=1.2.9 - metabat2&gt;=2.15 - r-base&gt;=3.5.1 - r-gridextra&gt;=2.2.1 - r-tidyverse - r-tidytext - samtools&gt;=1.9 - snakemake&gt;=5.10.0,&lt;5.31.1 name指的是创建环境的名称，channels指的是下载通道，其中这个conda-forge是比较重要的一个，其是一个用于托管和发布科学计算、数据分析和机器学习的Python 包的社区项目。在conda-forge通道中，您可以找到为conda构建但尚未成为官方Anaconda发行版一部分的包。有一些Python库不能用简单的conda install安装，因为除非应用conda-forge，否则它们的通道是不可用的。根据我的经验，pip比conda更适合研究不同的通道源。例如，如果你想安装python-constraint，你可以通过pip install来安装，但是要通过cond 来安装。您必须指定通道- conda-forge。 我在网上看到有人说用forge走的是外网，因此很慢导致加载卡住，因此，我把channels全部注释掉(默认没有channels项)然后测试，依然失败。 注：如果安装出现了Solving environment: failed with initial frozen solve. Retrying with flexible solve 错误 解决博客 12conda update -n base condaconda update --all 即可解决 在后面的测试中，我发现metawrap和prokkaroary的安装 于是我们把后面的大于等于去掉在进行测试，等了一个小时也没有结果也失败了。 于是我们用土办法，创建环境然后一个一个手动输入加载 12345conda create -p ./envs/metagem python=3.10conda info --envconda activate /home/gc/metaGEM-master/workflow/envs/metagem conda install -c conda-forge bedtools bwa concoct diamond fastp gtdbtk maxbin2 megahit metabat2 r-base r-gridextra r-tidyverse r-tidytext samtools snakemake=5.10.0 -ypip install --user memote carveme smetana B.执行metaGEM.sh在dataset文件夹中的子目录中存放paierd-end的fastq数据，如下所示。MetGEM 将基于dataset文件夹中存在的子文件夹读取示例 ID，并将这些 ID 提供给 Snakefile 作为作业提交的通配符。 我的运行文件树如下图所示 12345678910111213141516171819202122232425262728├── colab│ └── assemblies├── config.yaml├── dataset│ ├── L1EFG190305--AM43│ ├── L1EFG190306--AM51│ ├── L1EFG190309_L1EFG190309--AM61│ ├── L1EFG190324--AW1│ ├── L1EFG190325--AW2│ ├── L1EFG190326--AW3│ └── L1EFG190327--AW4├── envs│ ├── metagem│ ├── metaGEM_env_long.yml│ ├── metaGEM_env.yml│ ├── metaWRAP_env.yml│ ├── prokkaroary│ └── prokkaroary_env.yml├── env_setup.sh├── metaGEM.sh├── rules│ ├── kallisto2concoctTable.smk│ ├── maxbin_single.smk│ ├── metabat_single.smk│ ├── Snakefile_experimental.smk.py│ └── Snakefile_single_end.smk.py├── scripts/└── Snakefile 使用fastp质量过滤reads 每个样本提交一个质量过滤工作，每个过滤工作有2个CPU和20GB 内存，最大运行时间为2小时 1bash metaGEM.sh -t fastp -j 2 -c 2 -m 20 -h 2 1.报错找不到路径 更改config/config.yaml 文件第一行的执行路径即可 如果还是找不到路径则在Snakefile中第一行更改config的路径 如果还是读取不到，则把config.yaml 文件复制到当前文件夹下 2.报错Error parsing number of cores (–cores, -c, -j): must be integer, empty, or ‘all’.第一种情况snakemake版本过高，降低到5.10.0即可解决 第二种情况未执行pip install –user memote carveme smetana 3.报错找不到分析的数据在snakefile文件中，我们可以看到第16&#x2F;17行表示在dataset文件夹下，每一个文件名下面有两个同名加_R1,_R2的文件夹，因此我们要现将文件进行标准化操作， 以下是我们的原始文件数据 我们要把他们进行分类，并且进行改名操作，因此我写了一个自动化分类脚本 123456789101112131415161718192021dataset_path=&quot;/home/gc/bash_all/0&quot; #标准数据的文件夹path_name=&quot;z&quot;tar_path=&quot;/home/gc/metaGEM-master/workflow/dataset&quot; #目标dataset文件夹temp=0for file in `ls $&#123;dataset_path&#125;/`do echo $&#123;file&#125; if (($temp==0)) then mid_temp=$file temp=1 else #命令执行处 #echo $&#123;mid_temp:0:-16&#125; #!!!注意这里，因为我的文件里面后16个字符是固定的，需要替换成如下格式，因此我们把字符串截到16$&#123;mid_temp:0:-16&#125;，如果你的文件后缀不一样，请按照需要更改所有&#x27;16&#x27;的地方 mkdir $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125; mv $&#123;dataset_path&#125;/$&#123;mid_temp:0:-16&#125;.R1.raw.fastq.gz $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125;/$&#123;mid_temp:0:-16&#125;_R1.fastq.gz mv $&#123;dataset_path&#125;/$&#123;mid_temp:0:-16&#125;.R2.raw.fastq.gz $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125;/$&#123;mid_temp:0:-16&#125;_R2.fastq.gz temp=0 fidone 4.提交任务后，nohup.out提示sbatch: error: s_p_parse_file: unable to status file &#x2F;etc&#x2F;slurm-llnl&#x2F;slurm.conf: No such file or directory, retrying in 1sec up to 60sec(有的同学会提示&#x2F;bin&#x2F;sh: sbatch: command not found 之类的，这是没有安装slurm导致的看到有一篇文章写到需要这样安装https://www.thegeekdiary.com/sbatch-command-not-found/) Distribution Command Debian apt-get install slurm-client Ubuntu apt-get install slurm-client Kali Linux apt-get install slurm-client Fedora dnf install slurm OS X brew install slurm Raspbian apt-get install slurm-client 配置slurm有问题，slurm是一个linux服务器中的集群管理和作业调度系统，是项目里很关键的一点，因此要好好学习这里的配置信息 先看看文件 1ls -l /etc/slurm/ 报错没有此文件，表明还没有安装slurm 弯路(后面还有未解决的报错) ———————————————分割线以内的不要使用！！————————————————— (失败的教程) https://blog.csdn.net/r1141207831/article/details/125272108 先从https://www.schedmd.com/这里下载，选择对应的版本 12345678910#编译安装前需安装gccyum -y install gcc#接着解压安装tar -jxvf slurm-16.05.11.tar.bz2cd /root/slurm-16.05.11./configuremakemake install#安装成功！ 在make和make install时出现 Ld 返回的1退出状态错误是以前错误的结果。有一个更早的错误ーー对‘hdf5各种方法的未定义引用造成的，因此我猜测是linux版本与slurm版本不同造成的，因此我找了一个适用于Ubuntu20.04 的slurm安装教程 最全slurm安装包列表如下https://src.fedoraproject.org/lookaside/extras/slurm/ a、安装必要文件 12sudo suapt-get install make hwloc libhwloc-dev libmunge-dev libmunge2 munge mariadb-server libmysqlclient-dev -y b、启动启动munge服务 123systemctl enable munge // 设置munge开机自启动systemctl start munge // 启动munge服务systemctl status munge // 查看munge状态 c、编译安装slurm 1234567# 将slurm-21.08.6.tar.bz2源码包放置在/home/fz/package目录下cd /home/fz/packagetar -jxvf slurm-21.08.6.tar.bz2cd slurm-21.08.6/./configure --prefix=/opt/slurm/21.08.6 --sysconfdir=/opt/slurm/21.08.6/etcmake -j16make install 在make时发现缺少hdf5包 我又尝试使用spack高效的包管理器安装hdf5 https://hpc.pku.edu.cn/_book&#x2F;guide&#x2F;soft_env&#x2F;spack.html（教程） 但是报错如下，只能进行手动下载编译 官网下载hdf5 https://support.hdfgroup.org/ftp/HDF5/releases/ 12345678910111213141516171819sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压cd hdf5-1.8.21/ #sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压#依次执行sudo ./configure --prefix=/usr/local/hdf5sudo make #会有很多五颜六色的警告，忽略掉，sudo make check sudo make install#安装成功后，在安装目录/usr/local下出现hdf5文件夹，打开后再切换到该目录下cd /usr/local/hdf5/share/hdf5_examples/csudo ./run-c-ex.sh#执行命令sudo h5cc -o h5_extend h5_extend #如果显示错误，则安装：sudo apt install hdf5-helperssudo apt-get install libhdf5-serial-dev#再执行 sudo h5cc -o h5_extend h5_extend.c #直到执行后没有错误显示#执行命令 sudo ./h5_extend 12345678910111213141516171819sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压cd hdf5-1.8.21/ #sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压#依次执行sudo ./configure --prefix=/usr/local/hdf5sudo make #会有很多五颜六色的警告，忽略掉，sudo make check sudo make install#安装成功后，在安装目录/usr/local下出现hdf5文件夹，打开后再切换到该目录下cd /usr/local/hdf5/share/hdf5_examples/csudo ./run-c-ex.sh#执行命令sudo h5cc -o h5_extend h5_extend #如果显示错误，则安装：sudo apt install hdf5-helperssudo apt-get install libhdf5-serial-dev#再执行 sudo h5cc -o h5_extend h5_extend.c #直到执行后没有错误显示#执行命令 sudo ./h5_extend 装完hdf5后继续make slurm 没有报错！！！完成安装 完成安装，下面进行配置 d、启动数据库 后面是无限的hostname报错，我又重新找了一个教程，这个环境被污染了，如果使用另一个教程装slurm会冲突，但是这个又卸载不干净，于是重做了系统，在另一个教程上面成功了 ———————————————分割线以内的不要使用！！————————————————— 正确的教程如下来自https://wxyhgk.com/article%2Fubuntu-slurm(这个文章讲的太好了) 安装与配置1234#安装slurmsudo apt install slurm-wlm slurm-wlm-doc -y#检查是否安装成功slurmd --version 配置slurm 配置文件是放在 /etc/slurm-llnl/ 下面的，使用命令 1sudo vi /etc/slurm-llnl/slurm.conf 填写如下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687ClusterName=coolControlMachine=master#ControlAddr=#BackupController=#BackupAddr=#MailProg=/usr/bin/s-nailSlurmUser=root#SlurmdUser=rootSlurmctldPort=6817SlurmdPort=6818AuthType=auth/munge#JobCredentialPrivateKey=#JobCredentialPublicCertificate=StateSaveLocation=/var/spool/slurmctldSlurmdSpoolDir=/var/spool/slurmdSwitchType=switch/noneMpiDefault=noneSlurmctldPidFile=/var/run/slurmctld.pidSlurmdPidFile=/var/run/slurmd.pidProctrackType=proctrack/pgid#PluginDir=#FirstJobId=ReturnToService=0#MaxJobCount=#PlugStackConfig=#PropagatePrioProcess=#PropagateResourceLimits=#PropagateResourceLimitsExcept=#Prolog=#Epilog=#SrunProlog=#SrunEpilog=#TaskProlog=#TaskEpilog=#TaskPlugin=#TrackWCKey=no#TreeWidth=50#TmpFS=#UsePAM=## TIMERSSlurmctldTimeout=300SlurmdTimeout=300InactiveLimit=0MinJobAge=300KillWait=30Waittime=0## SCHEDULINGSchedulerType=sched/backfill#SchedulerAuth=#SelectType=select/linear#PriorityType=priority/multifactor#PriorityDecayHalfLife=14-0#PriorityUsageResetPeriod=14-0#PriorityWeightFairshare=100000#PriorityWeightAge=1000#PriorityWeightPartition=10000#PriorityWeightJobSize=1000#PriorityMaxAge=1-0## LOGGINGSlurmctldDebug=infoSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.logSlurmdDebug=infoSlurmdLogFile=/var/log/slurm-llnl/slurmd.logJobCompType=jobcomp/none#JobCompLoc=## ACCOUNTING#JobAcctGatherType=jobacct_gather/linux#JobAcctGatherFrequency=30##AccountingStorageType=accounting_storage/slurmdbd#AccountingStorageHost=#AccountingStorageLoc=#AccountingStoragePass=#AccountingStorageUser=## COMPUTE NODESPartitionName=master Nodes=master Default=NO MaxTime=INFINITE State=UP#NodeName=master State=UNKNOWNNodeName=master Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN 上面的代码中的 ControlMachine&#x3D;master PartitionName&#x3D;master Nodes&#x3D;master Default&#x3D;NO MaxTime&#x3D;INFINITE State&#x3D;UP#NodeName&#x3D;master State&#x3D;UNKNOWNNodeName&#x3D;master Sockets&#x3D;2 CoresPerSocket&#x3D;16 ThreadsPerCore&#x3D;1 State&#x3D;UNKNOWN 我 黑体 和 斜体 的地方需要修改，这两部分是是需要修改的，其他的别动。 黑体部分修改 使用 hostname 命令可以查看到你的名字，然后把你的到的名字替换上面的 master 斜体部分修改 这部分稍微有点复杂，首先来解释各个名字的意思 Sockets 代表你服务器cpu的个数 CoresPerSocket 代表每个cpu的核数 ThreadsPerCore 代表是否开启超线程，如果开启了超线程就是2，没有开启就是1 使用vi [cxc.sh](http://cxc.sh/) 写以下脚本 1234567891011121314#!/bin/bashcpunum=`cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l`echo &quot;CPU 个数: $cpunum&quot;;cpuhx=`cat /proc/cpuinfo | grep &quot;cores&quot; | uniq | awk -F&quot;:&quot; &#x27;&#123;print $2&#125;&#x27;`echo &quot;CPU 核心数：$cpuhx&quot; ; cpuxc=`cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l`echo &quot;CPU 线程数：$cpuxc&quot; ;if [[ `expr $cpunum\\*$[cpuhx*2] ` -eq $cpuxc ]];then echo &quot;开启了超线程&quot;else echo &quot;未开启超线程&quot;fi 然后使用命令 bash [cxc.sh](http://cxc.sh/) 运行脚本，看看线程数是不是核心数的两倍，如果是就开启了，没有就没开启。 完成上面的之后吧对应的数字填写上去就可以了。 完成上述所有的设置之后就能启动服务了shell 12sudo systemctl enable slurmctld --nowsudo systemctl enable slurmd --now 查看slurm队列信息 1sinfo 如果这部分是 idle 就说明是可以的,如果不是 idle 请看这个 如果还是解决不了 比如是drain 其意思是用尽资源 解决文章 sinfo -R 报错Low socket***core***thre 那么直接把Sockets=2 CoresPerSocket=16 这两个参数减少，比如说除以2，留出一定的资源给系统使用，问题就解决了 确定目前队列里没有程序时，执行下列语句就好了（NodeName是上面设置的） 1scontrol update NodeName=m1 State=idle 至此就已经安装完成了 到这里配置slurm就已经结束了 5.提交后sbatch: error: Batch job submission failed: No partition specified or system default partition这个错误也是排查了好久，排查到这个文章 1234[username@master1 ~]# sbatch example.sh --partition computeq #Note that ordering matters here!sbatch: error: Batch job submission failed: No partition specified or system default partition[username@master1 ~]# sbatch --partition=computeq example.shSubmitted batch job 114499 猜测是运行顺序错误导致的问题，于是我们到metaGEM.sh 中排查一下，核心的运行语句如下 1echo &quot;nohup snakemake all -j $njobs -k --cluster-config ../config/cluster_config.json -c &#x27;sbatch -A &#123;cluster.account&#125; -t &#123;cluster.time&#125; --mem &#123;cluster.mem&#125; -n &#123;cluster.n&#125; --ntasks &#123;cluster.tasks&#125; --cpus-per-task &#123;cluster.n&#125; --output &#123;cluster.output&#125;&#x27; &amp;&quot;|bash; break;; 可以看到在后面的sbatch里面没有关于–partition的语句，于是我们手动添加，partition后面的名字就是前面我们设置的主机名 1--partition=你的主机名 1sbatch --partition=的主机名 -A &#123;cluster.account&#125; -t &#123;cluster.time&#125; --mem &#123;cluster.mem&#125; -n &#123;cluster.n&#125; --ntasks &#123;cluster.tasks&#125; --cpus-per-task &#123;cluster.n&#125; --output &#123;cluster.output&#125; 代码中搜索sbatch 有三个地方需要添加，添加后即可正常运行 然后我们运行文章开头的语句 -j 任务数量 -c 每个任务CPU数量 -m 每个任务分配的内存大小 -h 每个任务运行的时间 注:注意CPU过大也不行 1bash metaGEM.sh -t fastp -j 5 -c 4 -m 20 -h 20 然后我们输入squeue 查看刚才提交的任务 到这里我们的环境配置完毕","categories":[],"tags":[{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"}],"author":"Sn0wma1n"},{"title":"解决Gitee图床不显示问题(使用阿里云OSS搭建图床)","slug":"解决Gitee图床不显示问题","date":"2023-09-02T08:51:50.000Z","updated":"2023-09-02T14:31:33.992Z","comments":true,"path":"2023/09/02/解决Gitee图床不显示问题/","link":"","permalink":"https://snowman12137.github.io/2023/09/02/%E8%A7%A3%E5%86%B3Gitee%E5%9B%BE%E5%BA%8A%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题前段时间搞博客弄了一个Gitee图床，图片可以上传成功，外部链接可以打开图片，但是hexo s后本地都打开不了图片，但是相应的源码标签里依然有图片链接地址。这就让我很疑惑。 报错是class=lazyload error 可以看到下图这里是有图片的，就是不能显示。 在网上并没有找到相关的报错解释，然后我又查找了lazyload是懒加载，但是也没有查明相关的报错解决方法，于是我又怀疑是主题的问题。 在_config.yml里注释掉theme以后发现报错 错误是不能加载，有可能是Gitee的权限问题(是公共库)，导致服务器不能访问库里面的图片。 然后我找到了这样一篇文章： gitee图床不能用了，心态崩了 原来是Gitee从去年开始已经不支持图床了。然后发现阿里云是比较好用的一个图床，毕竟是付费的(40G 1年 9RMB) 搭建阿里云OSS图床+PicGO1.购买阿里云OSS服务 登录阿里云 打开侧边栏选择对象存储OSS 进入管理控制台 点击创建Bucket 有的人在区域一栏没有买过流量包，会提醒购买 读写权限注意是公共读，博客需要读取你上传后的图片 购买资源包 2.添加用户权限我们需要添加一个可以操作OSS的用户，在配置好PicGO后使用这个用户对图片进行自动添加。 点击访问控制 点击用户，新建用户 输入名称，注意勾选OpenAPI调用访问 这里要记下AccessKey ID 和 AccessKeySecret，之后配置PICGO用到，因为这个界面关掉之后就不好找了，所以最好 记在记事本里 设置用户权限 选择管理对象存储OSS服务权限，点击确定，如下图所示： 3.配置PICGO 下载PICGO里面有相应操作系统的安装包文件以及源码，点击下载安装文件即可。 安装完成后，打开图床设置，点击阿里云OSS，得到如下界面 注意设定keyid，就是创建用户的AccessKey ID，KeySecret 就是AccessKeySecret，存储空间名就是创建Bucket的名字，存储区域也是创建时设定的， 忘记的可以通过Bucket概览查看，如下图所示： 存储路径默认设置img&#x2F;即可，如果自己有已经备案的域名，可以填写设定自定义域名，如果没有不填即可。 可以看到PICGO能够文件上传，也支持剪贴板上传。上传过程为： 拖拽文件或点击上传文件或点击剪贴板图片上传。 上传完成后电脑剪贴板里就有了所选链接格式的图片链接。 到相应的地方粘贴即可。 开始可能有疑问，我的图片存到哪里了呢？很简单，点击文件管理，如下图所示： 看到img文件夹了吗？就在里面了，你可以在文件夹里对图片进行删除等操作。 4.配置Typora 打开typora点击偏好设置 然后配置如下选项 1.点击图像 2.选择上传图片 3.上传服务选择PICGO 4.找到PICGO安装的位置，即PicGo.exe的位置 点击验证图片上传选项即可 注意可以1.右键桌面图标 2.点击属性 3.负值路径 Over 下课","categories":[],"tags":[{"name":"博客问题","slug":"博客问题","permalink":"https://snowman12137.github.io/tags/%E5%8D%9A%E5%AE%A2%E9%97%AE%E9%A2%98/"}],"author":"Sn0wma1n"},{"title":"SGX(Software Guard eXtensions)_linux构建可信执行环境(一)","slug":"SGX-Software-Guard-eXtensions-构建可信执行环境-一","date":"2023-09-02T08:42:16.000Z","updated":"2023-09-06T09:55:34.893Z","comments":true,"path":"2023/09/02/SGX-Software-Guard-eXtensions-构建可信执行环境-一/","link":"","permalink":"https://snowman12137.github.io/2023/09/02/SGX-Software-Guard-eXtensions-%E6%9E%84%E5%BB%BA%E5%8F%AF%E4%BF%A1%E6%89%A7%E8%A1%8C%E7%8E%AF%E5%A2%83-%E4%B8%80/","excerpt":"","text":"SGX(Software Guard eXtensions)_linux构建可信执行环境(一)：如何开发第一个最简单的 SGX 应用 HelloWorld一、了解SGX1.1SGX定义 Intel 软件防护扩展SGX（Software Guard Extension）是一项针对台式机和服务器平台的旨在满足可信计算需求的技术。Intel SGX是Intel架构新的扩展，在原有架构上增加了一组新的指令集和内存访问机制。 Intel在2015年从第6代Intel酷睿处理器平台开始引入了Intel软件防护扩展新指令集，使用特殊指令和软件可将应用程序代码放入一个enclave中执行。Enclave可以提供一个隔离的可信执行环境，可以在BIOS、虚拟机监控器、主操作系统和驱动程序均被恶意代码攻陷的情况下，仍对enclave内的代码和内存数据提供保护，防止恶意软件影响enclave内的代码和数据，从而保障用户的关键代码和数据的机密性和完整性。 1.2基本原理 SGX应用由两部分组成： untrusted 不可信区：代码和数据运行在普通非加密内存区域，程序main入口必须在非可信区。 trusted 可信区：代码和数据运行在硬件加密内存区域，此区域由CPU创建的且只有CPU有权限访问。 当一个安全区域函数被调用时，只有安全区域内的代码才能看到其数据，外部访问总是被拒绝；返回时，安全区数据将保留在受保护的内存中。 非可信区只能通过ECALL函数调用可信区内的函数，可信区只能通过OCALL函数调用非可信区的函数，ECALL函数和OCALL函数通过EDL文件声明。 1.3两大机制1.3.1保护机制针对enclave的保护机制主要包括两个部分：一是enclave内存访问语义的变化，二是应用程序地址映射关系的保护。这两项功能共同完成对enclave的机密性和完整性的保护。 1.3.2 认证机制SGX 提出了两种类型的身份认证方式：一种是平台内部 enclave 间的认证，用来认证进行报告的 enclave 和自己是否运行在同一个平台上；另一种是平台间的远程认证，用于远程的认证者认证 enclave 的身份信息。 本地证明：同一平台的两个Enclave之间的证明过程。 远程证明：Enclave和不在平台上的第三方之间的证明过程。 二、Linux下安装SGX 简介：我们需要安装几个东西。第一是驱动Drive，有了驱动才可以调用intel芯片里面的硬件部分。第二个是PSW(平台软件)，其是允许支持SGX的应用程序在目标平台上执行的软件栈，包含四部分1.提供对硬件功能进行访问的驱动程序；2.为执行和认证提供多个支持库；3.运行所必需的架构型enclave；4.加载并与enclave进行通信的服务；。第三个是软件开发工具包（SDK）为开发人员提供了开发支持SGX的应用程序所需的一切，它由一个生成应用程序和enclave之间的接口函数的工具，一个在使用它之前对enclave进行签名的工具，一个调试它的工具以及一个性能检查的工具组成。另外，它还包含模板和样本参数，用于在Windows下使用Visual Studio开发enclave，或在Linux下使用Makefile。 有了这三样东西我们才可以进行代码开发 以Ubuntu20.04 LTS为例 硬件需求仅当安装sgx驱动和PSW时需要，安装sgx sdk并不需要硬件支持。 硬件不支持的情况下，可以在模拟环境下编写测试SGX程序，其中makefile里SGX_MODE&#x3D;SIM。（虽然 SGX是基于硬件的，但是依旧可以使用软件条件进行模拟） 先安装驱动、PSW、SDK所需依赖 123sudo apt-get updatesudo apt-get install libssl-dev libcurl4-openssl-dev libprotobuf-devsudo apt-get install build-essential python 下载Intel SGX驱动并安装 注意所有版本的驱动、SPW、SDK都在这里https://download.01.org/intel-sgx/sgx-linux/2.21/distro/ 1234wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/sgx_linux_x64_driver_2.6.0_4f5bb63.binchmod +x sgx_linux_x64_driver_2.6.0_4f5bb63.binsudo ./sgx_linux_x64_driver_2.6.0_4f5bb63.binsudo reboot 注意安装完重启才生效 下载Intel SGX PSW并安装 注意：有的版本的没有PSW文件，也可以不装 12wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/libsgx-enclave-common_2.7.101.3-bionic1_amd64.debsudo dpkg -i ./libsgx-enclave-common_2.7.101.3-bionic1_amd64.deb 下载并安装Intel SGX SDK 安装过程中可以手动输入SDK要安装到的目标位置 123wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/sgx_linux_x64_sdk_2.7.101.3.binchmod +x ./sgx_linux_x64_sdk_2.7.101.3.bin./sgx_linux_x64_sdk_2.7.101.3.bin 添加环境变量，执行上一步结束时输出的命令 1source /path/to/sgxsdk/environment 接下来进入到sgxsdk&#x2F;SampleCode&#x2F;SampleEnclave文件夹里 1cd /path/to/sgxsdk/SampleCode/SampleEnclave 编辑一下Makefile 12345678# Intel SGX SDK 的安装位置SGX_SDK ?= /home/luoyhang003/SGX/sgxsdk# 运行类型：HW 真实环境；SIM 模拟器环境SGX_MODE ?= SIM# 运行架构：仅支持 64 位SGX_ARCH ?= x64# 是否为：Debug 调试模式SGX_DEBUG ?= 1 退出然后运行示例 1234# 编译sudo make# 运行./app 注意一定要添加环境变量source &#x2F;path&#x2F;to&#x2F;sgxsdk&#x2F;environment，这行命令是临时环境变量，即存活时间为一个终端的市场，一旦重启或开启新的终端环境变量即会失效，需要手动添加。也可以永久化环境变量。 成功运行截图 三、文件结构与代码目录结构 编译&amp;运行 1234567891011121314151617181920212223242526272829303132$ makeGEN =&gt; App/Enclave_u.hCC &lt;= App/Enclave_u.cCXX &lt;= App/App.cppLINK =&gt; appGEN =&gt; Enclave/Enclave_t.hCC &lt;= Enclave/Enclave_t.cCXX &lt;= Enclave/Enclave.cppLINK =&gt; enclave.so&lt;EnclaveConfiguration&gt; &lt;ProdID&gt;0&lt;/ProdID&gt; &lt;ISVSVN&gt;0&lt;/ISVSVN&gt; &lt;StackMaxSize&gt;0x40000&lt;/StackMaxSize&gt; &lt;HeapMaxSize&gt;0x100000&lt;/HeapMaxSize&gt; &lt;TCSNum&gt;10&lt;/TCSNum&gt; &lt;TCSPolicy&gt;1&lt;/TCSPolicy&gt; &lt;!-- Recommend changing &#x27;DisableDebug&#x27; to 1 to make the enclave undebuggable for enclave release --&gt; &lt;DisableDebug&gt;0&lt;/DisableDebug&gt; &lt;MiscSelect&gt;0&lt;/MiscSelect&gt; &lt;MiscMask&gt;0xFFFFFFFF&lt;/MiscMask&gt;&lt;/EnclaveConfiguration&gt;tcs_num 10, tcs_max_num 10, tcs_min_pool 1The required memory is 3960832B.The required memory is 0x3c7000, 3868 KB.Succeed.SIGN =&gt; enclave.signed.soThe project has been built in debug hardware mode.$ ./appHello worldInfo: SampleEnclave successfully returned.Enter a character before exit ... 编译流程(Makefile) 通过 sgx_edger8r 工具在 App/ 目录下生成不可信代码(Enclave_u.c 和 Enclave_u.h)，这部分生成代码主要会调用 ECALL (sgx_ecall)； 编译不可信部分 Binary: app； 通过sgx_edger8r 工具在 Enclave/ 目录下生成可信代码(Enclave_t.c 和 Enclave_t.h)； 编译可信动态链接库(enclave.so)； 通过sgx_sing工具签名可信动态链接库(enclave.signed.so)； 结束。 编译后目录结构 123456789101112131415161718192021222324HelloWorld├── app├── App│ ├── App.cpp│ ├── App.h│ ├── App.o #[generated]│ ├── Enclave_u.c #[generated] │ ├── Enclave_u.h #[generated] │ └── Enclave_u.o #[generated]├── Enclave│ ├── Enclave.config.xml│ ├── Enclave.cpp│ ├── Enclave.edl│ ├── Enclave.h│ ├── Enclave.lds│ ├── Enclave.o #[generated]│ ├── Enclave_private.pem│ ├── Enclave_t.c #[generated]│ ├── Enclave_t.h #[generated]│ └── Enclave_t.o #[generated]├── enclave.signed.so #[generated]├── enclave.so #[generated]├── Include└── Makefile HelloWorld 大概流程如下 1.添加可信函数Encalve&#x2F;Enclave.edl 12345enclave &#123; trusted &#123; public void ecall_hello_from_enclave([out, size=len] char* buf, size_t len); &#125;;&#125;; 2.在可信区域定义可信函数Enclave&#x2F;Enclave.cpp 1234567891011121314151617#include &quot;Enclave.h&quot;#include &quot;Enclave_t.h&quot; /* print_string */#include &lt;string.h&gt;void ecall_hello_from_enclave(char *buf, size_t len)&#123; const char *hello = &quot;Hello world&quot;; size_t size = len; if(strlen(hello) &lt; len) &#123; size = strlen(hello) + 1; &#125; memcpy(buf, hello, size - 1); buf[size-1] = &#x27;\\0&#x27;;&#125; 3.调用可信函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;assert.h&gt;# include &lt;unistd.h&gt;# include &lt;pwd.h&gt;# define MAX_PATH FILENAME_MAX#include &quot;sgx_urts.h&quot;#include &quot;App.h&quot;#include &quot;Enclave_u.h&quot;/* Global EID shared by multiple threads */sgx_enclave_id_t global_eid = 0;int initialize_enclave(void)&#123; sgx_status_t ret = SGX_ERROR_UNEXPECTED; /* 调用 sgx_create_enclave 创建一个 Enclave 实例 */ /* Debug Support: set 2nd parameter to 1 */ ret = sgx_create_enclave(ENCLAVE_FILENAME, SGX_DEBUG_FLAG, NULL, NULL, &amp;global_eid, NULL); if (ret != SGX_SUCCESS) &#123; printf(&quot;Failed to create enclave, ret code: %d\\n&quot;, ret); return -1; &#125; return 0;&#125;/* 应用程序入口 */int SGX_CDECL main(int argc, char *argv[])&#123; (void)(argc); (void)(argv); const size_t max_buf_len = 100; char buffer[max_buf_len] = &#123;0&#125;; /* 创建并初始化 Enclave */ if(initialize_enclave() &lt; 0)&#123; printf(&quot;Enter a character before exit ...\\n&quot;); getchar(); return -1; &#125;//-------------------------添加代码区域------------------------------------ /* ECALL 调用 */ ecall_hello_from_enclave(global_eid, buffer, max_buf_len); printf(&quot;%s\\n&quot;, buffer);//------------------------------------------------------------------------ /* 销毁 Enclave */ sgx_destroy_enclave(global_eid); printf(&quot;Info: SampleEnclave successfully returned.\\n&quot;); printf(&quot;Enter a character before exit ...\\n&quot;); getchar(); return 0;&#125; 总结即便最简单的 SGX HelloWold 也比较复杂，当然“安全性”和“成本”（技术壁垒门槛、开发成本、维护成本、物料成本等）总是成正比的，和“效率”成反比的。希望这篇文章对那些想入门开发 SGX 应用的用户有所帮助。","categories":[{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/categories/SGX/"}],"tags":[{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/tags/SGX/"},{"name":"Linux","slug":"Linux","permalink":"https://snowman12137.github.io/tags/Linux/"}],"author":"Sn0wma1n"},{"title":"RSA数学原理解析","slug":"RSA数学原理解析","date":"2023-05-15T03:18:16.000Z","updated":"2023-09-02T14:32:10.629Z","comments":true,"path":"2023/05/15/RSA数学原理解析/","link":"","permalink":"https://snowman12137.github.io/2023/05/15/RSA%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/","excerpt":"","text":"1.引言 RSA算法(RSA algorithm)是一种非对称加密算法, 广泛应用在互联网和电子商务中. 它使用一对密钥进行加密和解密, 分别称为公钥(public key)和私钥(private key). 使用公钥加密的内容只能用私钥解密, 使用私钥加密的内容只能用公钥解密, 并且不能通过公钥在可行的时间内计算出私钥. 这使得加密通信不需要交换私钥, 保证了通信的安全. 那么它是怎么做到这一点的呢? 背后有哪些数学原理? 这篇文章我们来讨论这个问题. 本文会先介绍RSA算法中用到的数论概念和定理: 模算术, 最大公约数与贝祖定理, 线性同余方程, 中国余数定理, 费马小定理; 然后再介绍RSA算法的原理, 并证明其是有效的. 本文会假设你了解数论的基本概念, 如素数, 最大公约数, 互素等 2.模算术2.1整数除法用一个正整数去除一个整数，可以得到一个商和一个余数，数学符号定义为： 定理1：令 a 为整数, d 为正整数, 则存在唯一的整数 q 和 r, 满足 $0\\leq r&lt;d$, 使得 $a&#x3D;dq+r$ 当r&#x3D;0时，我们称 d 整除 a, 记作$d|a$; 否则称 d 不整除 a, 记作 $d\\nmid a$，整除有以下基本性质: 定理2：令a,b,c,为整数，其中$a\\neq0$,则：如果$a|b$且$a|c$则$a|(a+b)$ 2.2模算术在数论中我们特别关心一个整数被一个正整数除时的余数. 我们用 $a\\bmod m&#x3D;b$ 表示整数 a 除以正整数 m 的余数是 b. 为了表示两个整数被一个正整数除时的余数相同, 人们又提出了同余式(congruence). 定义1:如果 a 和 b 是整数而 m 是正整数, 则当 m 整除 a - b 时称 a 模 m 同余 b. 记作 $a\\equiv b(\\bmod m)$ $a\\equiv b(\\bmod m)$ 和 $a\\bmod m&#x3D;b$很相似. 事实上, 如果 $a\\bmod m&#x3D;b$, 则 $a\\equiv b(\\bmod m)$. 但他们本质上是两个不同的概念.$a\\bmod m&#x3D;b$ 表达的是一个函数, 而 $a\\equiv b(\\bmod m)$ 表达的是两个整数之间的关系. 另外，同余式$a\\equiv b(\\bmod m)$还可以表示为$m|(b-a)$，同时符合上式的式子可以转化为同余式 模算术的性质： 定理3：如果m是正整数，a,b是整数，则有$$\\begin{aligned}(a+b)\\bmod m &amp;&#x3D;((a\\bmod m)+(b\\bmod m))\\bmod m\\ab\\bmod m &amp;&#x3D;(a\\bmod m)(b\\bmod m)\\bmod m\\end{aligned}$$根据定理3，可得一下推论 推论1：设m是正整数，a,b,c是整数；如果$a\\equiv b(\\bmod m)$则$ac\\equiv bc(\\bmod m)$ 证明： $\\because a\\equiv b(\\bmod m)$ $\\therefore m|(b-a)$ 所以右端再乘以任何整数m都可以整除 即$m|(b-a)c$ 同理，既然$m|(b-a)$且$c|c$ 也可以推出$mc|(b-a)c$ 结论：若$a\\equiv b(\\bmod m)$，则$ac\\equiv bc(\\bmod m)$且$ac\\equiv bc(\\bmod mc)$同时成立 推论2设 m 是正整数, a, b 是整数, c 是不能被 m 整除的整数; 如果 $ac\\equiv bc(\\bmod m)$, 则$a\\equiv b(\\bmod m)$,依据推论1的证明，也是显而易见的。 3.最大公约数如果一个整数 d 能够整除另一个整数 a, 则称 d 是 a 的一个约数(divisor); 如果 d 既能整除 a 又能整除 b, 则称 d 是 a 和 b 的一个公约数(common divisor). 能整除两个整数的最大整数称为这两个整数的最大公约数(greatest common divisor). 定义2：令a和b是不全为0的两个整数，能使$d|a$和$d|b$的最大整数d成为a和b的最大公约数，记作$gcd(a,b)$ 3.1 求最大公约数如何求两个已知整数的最大公约数呢? 这里我们讨论一个高效的求最大公约数的算法, 称为辗转相除法. 因为这个算法是欧几里得发明的, 所以也称为欧几里得算法. 辗转相除法基于以下定理: 引理 1 令 $a&#x3D;bq+r$, 其中 a, b, q 和 r 均为整数. 则有$gcd(a,b)&#x3D;gcd(b,r)$ 证明：我们假设 d 是 a 和 b 的公约数, 即 $d|a$且 $d|b$, 那么根据定理2, d 也能整除 $a-bq&#x3D;r$. 所以 a 和 b 的任何公约数也是 b 和 r 的公约数; 类似地, 假设 d 是 b 和 r 的公约数, 即 $d|b$且 $d|r$, 那么根据定理2, d 也能整除 $a&#x3D;bq+r$.. 所以 b 和 r 的任何公约数也是 a 和 b 的公约数; 因此, a 与 b 和 b 与 r 拥有相同的公约数. 所以 $gcd(a,b)&#x3D;gcd(b,r)$ 辗转相除法就是利用引理1, 把大数转换成小数. 例如, 求 $gcd(287,91)$, 我们就把用较大的数除以较小的数. 首先用 287 除以 91, 得$$287&#x3D;91\\cdot3+14$$我们有$gcd(287,91)&#x3D;gcd(91,14)$,问题转化成求$gcd(91,14)$,同样的，用91除以14得$$91&#x3D;14\\cdot6+7$$有$gcd(91,14)&#x3D;gcd(14,7)$,用14除以7得$$14&#x3D;7\\cdot2+0$$所以$gcd(297,91)&#x3D;gcd(91,14)&#x3D;gcd(14,7)&#x3D;7$ 代码是这样的(两个都可以) 12345678910def gcd(a,b): while b!=0 : r = a%b a = b b = r return adef gcd_new(a,b): if b==0: return a return gcd_new(b,a%b) 3.2 贝祖定理现在我们讨论最大公约数的一个重要性质: 定理 4 贝祖定理 如果整数 a, b 不全为零, 则 $gcd(a,b)$是 a 和 b 的线性组合集$ax+by|x,y\\in Z$ 中最小的元素. 这里的 x 和 y 被称为贝祖系数 证明 令 $A&#x3D;ax+by|x,y\\in Z$ 设存在 $x_0$ ,$y_0$ 使 $d_0$ 是 A 中的最小正元素, $d_0&#x3D;ax_0+by_0$; 现在用 $d_0$去除 a, 这就得到唯一的整数 q(商) 和 r(余数) 满足$$\\begin{aligned}d_0q+r &amp;&#x3D; a \\qquad 0\\leq r&lt;d_0\\(ax_0+by_0)q+r&amp;&#x3D;a\\r&amp;&#x3D;a-aqx_0-bqy_0\\r&amp;&#x3D;a(1-qx_0)+b(-qy_0)\\in A\\end{aligned}$$又$0\\leq r &lt;d_0$,$d_0$是A中最小的正元素 $\\therefore r&#x3D;0,d|a$ 同理, 用 $d_0$去除 b, 可得 $d_0|b$. 所以说 $d_0$ 是 a 和 b 的公约数. 设 a 和 b 的最大公约数是 d, 那么$d|(ax_0+by_0)$即 $d|d_0$ $\\therefore d_0$是a和b的最大公约数 我们可以对辗转相除法稍作修改, 让它在计算出最大公约数的同时计算出贝祖系数. 1234def gcd_new_2(a,b): if b==0: return a,1,0 d,x,y = gcd_new_2(b,a%b) return d,y,x-(int(a/b))*y 大家是否还记得辗转相除法：我们换一个步骤多一点的例子$gcd(963,657)$$$\\begin{aligned}963&amp;&#x3D;1\\cdot657+306 \\qquad\\qquad&amp;9&amp;&#x3D;7\\cdot657-15\\cdot(963-657)&#x3D;22\\cdot657-15\\cdot963 \\657&amp;&#x3D;2\\cdot306+45 &amp;9&amp;&#x3D;7\\cdot(657-2\\cdot306)-306&#x3D;7\\cdot657-15\\cdot963 \\306&amp;&#x3D;6\\cdot45+36 &amp;9&amp;&#x3D;45-(306-6\\cdot45)&#x3D;7\\cdot45-306\\45 &amp;&#x3D;1\\cdot36+9 &amp;9&amp;&#x3D;45-36\\36 &amp;&#x3D;4\\cdot9\\end{aligned}$$ $gcd(963,657)&#x3D;9&#x3D;22\\cdot657-15\\cdot963,x_0&#x3D;-15,y_0&#x3D;22$就是二元一次方程$963x+657y&#x3D;9$的一组解且是$963x+657y$方程的正整数中的最小解。 4.线性同余方程现在我们来讨论求解形如 $ax\\equiv b(\\bmod m)$ 的线性同余方程. 求解这样的线性同余方程是数论研究及其应用中的一项基本任务. 如何求解这样的方程呢? 我们要介绍的一个方法是通过求使得方程 $\\overline{a}a\\equiv 1(\\bmod m)$ 成立的整数 $\\overline{a}$. 我们称 $\\overline{a}$为 a 模 m 的逆. 下面的定理指出, 当 a 和 m 互素时, a 模 m 的逆必然存在. 定理5：如果 a 和 m 为互素的整数且 $m&gt;1$, 则 a 模 m 的逆存在, 并且是唯一的. 证明 由贝祖定理可知, $\\because gcd(a,m)&#x3D;1$ , ∴ 存在整数 x 和 y 使得$$ax+my&#x3D;1$$这蕴含着$$ax+my\\equiv 1(\\bmod m)\\\\because my\\equiv0(\\bmod m)，所以有\\ax\\equiv 1(\\bmod m)\\\\therefore x为a模的逆$$这样我们就可以调用辗转相除法 gcd(a, m) 求得 a 模 m 的逆. 求得了 a 模 m 的逆 �¯, 现在我们可以来解线性同余方程了. 具体的做法是这样的: 对于方程$ax\\equiv b( \\bmod m)$同时乘以$\\overline{a}$得,$$\\overline{a}ax\\equiv \\overline{a}b(\\bmod m)$$$把\\overline{a}a\\equiv 1(\\bmod m )代入上式，得\\$$$x\\equiv \\overline{a}b(\\bmod m)$$ $x\\equiv \\overline{a}b(\\bmod m)$就是方程的解，注意同余方程会有无数个W整数解, 所以我们用同余式来表示同余方程的解. 例1:求$34x\\equiv 77(\\bmod 89)$ 解调用$gcd(34,89)$,得$gcd(34,89)&#x3D;1&#x3D;1389-3434$,所以34模89的逆为-34，方程两边同时乘 -34 得$$\\begin{aligned}-34\\cdot34x&amp;\\equiv-34\\cdot77(\\bmod89)\\x&amp;\\equiv-34\\cdot77(\\bmod89)\\x&amp;\\equiv-2618\\equiv52(\\bmod89)\\end{aligned}$$ 5.中国剩余定理中国南北朝时期数学著作 孙子算经 中提出了这样一个问题: 有物不知其数，三三数之剩二，五五数之剩三，七七数之剩二。问物几何？ 用现代的数学语言表述就是: 下列同余方程组的解释多少?$$\\begin{cases}x\\equiv2(\\bmod3)\\x\\equiv3(\\bmod5)\\x\\equiv2(\\bmod7)\\\\end{cases}$$孙子算经 中首次提到了同余方程组问题及其具体解法. 因此中国剩余定理称为孙子定理. 定理 6 中国余数定理 令 $m_1,m_2,…..,m_n$为大于 1 且两两互素的正整数, $a_1,a_2,….a_n$ 是任意整数. 则同余方程组$$\\begin{cases}x\\equiv a_1(\\bmod m_1)\\x\\equiv a_2(\\bmod m_2)\\…..\\x\\equiv a_n(\\bmod m_n)\\\\end{cases}$$有唯一的模$m&#x3D;m_1m_2….m_n$的解 证明：我们使用构造证明法, 构造出这个方程组的解. 首先对于 $i&#x3D;1,2,….,n$, 令$$M_i&#x3D;\\frac{m}{m_i}$$ 即,$M_i$ 是除了$m_i$ 之外所有模数的积.$\\because m_1,m_2….m_n$两两互素, $\\therefore gcd(m_i,M_i)&#x3D;1$ 由定理5 可知, 存在整数 $y_i$ 是 $M_i$模$m_i$的逆. 即$$M_iy_i\\equiv1(\\bmod m_i)$$同时乘以$a_i$得$$a_iM_iy_i\\equiv a_i(\\bmod m_i)$$就是第 i 个方程的一个解; 那么怎么构造出方程组的解呢? 我们注意到, 根据 $M_i$ 的定义可得, 对所有的 $j\\neq i$, 都有 $a_iM_iy_i\\equiv0(\\bmod m_j)$. 因此我们令$$x&#x3D;a_1M_1y_1+a_2M_2y_2+….+a_nM_ny_n&#x3D;\\sum_{i&#x3D;1}^{n}{a_iM_iy_i}$$有了这个结论, 我们可以解答 孙子算经 中的问题了: 对方程组的每个方程, 求出 $M_i$ , 然后调用 gcd(M_i, m_i) 求出 $y_i$:$$\\begin{cases}\\begin{aligned}&amp;x\\equiv2(\\bmod3)\\quad &amp;M_1&amp;&#x3D;35\\quad y_1&#x3D;-1\\&amp;x\\equiv3(\\bmod5)\\quad &amp;M_2&amp;&#x3D;21\\quad y_2&#x3D;1\\&amp;x\\equiv2(\\bmod7)\\quad &amp;M_3&amp;&#x3D;15\\quad y_3&#x3D;-1\\\\end{aligned}\\end{cases}$$最后求出$x&#x3D;-235+321+2*15&#x3D;23\\equiv23(\\bmod 105)$ 6.费马小定理现在我们来看数论中另外一个重要的定理, 费马小定理(Fermat’s little theorem) 定理7费马小定理：如果a是一个整数，p是一个素数，那么$$a^p\\equiv a(\\bmod p)$$特别的当a不是p的倍数时（即$gcd(a,p)&#x3D;1$），有$$a^{p-1}\\equiv1(\\bmod p)\\$$ 7欧拉函数再来看一看欧拉函数的知识。对于正整数n，欧拉函数$\\varphi(n)$是小于或等于n的正整数中与n互质的数的数目 如$\\varphi(8)&#x3D;4$，1,3,5,7均与8互质 定理8：n,m为整数,$(n,m)&#x3D;1$，如果$a_1,a_2,….a_n$和$b_1,b_2,….b_n$分别是模n,m的一个完整系，则有形如$nb_i+ma_j(1\\leq i\\leq s,1\\leq j\\leq t)$的数构成模mn的一个完整缩系，特别地有$\\varphi(nm)&#x3D;\\varphi(n)\\varphi(m)$ 定理9：当n$\\geq2$时，设$n&#x3D;p_1^{e_1}….p_s^{e_s}$是n的标准分解式，则$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}(p_l^{e_l}-p_l^{e_l-1})&#x3D;n\\prod_{l&#x3D;1}^{s}(1-\\frac{1}{p})$$证明：当$(n.m)&#x3D;1$时$$\\varphi(n,m)&#x3D;\\varphi(n)\\varphi(m)$$当$n_1,n_2……n_s$两两互素时$$\\varphi(\\prod_{l&#x3D;1}^{s}n_l)&#x3D;\\prod_{l&#x3D;1}^{s}n_l$$因此$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}\\varphi(p_l^{e_l})$$问题转化成如何求$\\varphi(p_l^{e_l})$,其中$p_l$要么$p_l|a$,要么$(p_l,a)&#x3D;1$。在1,2….$p_l$当中可以被$p_l$整除的整数共有$\\frac{p_l^{e_l}}{p_l}&#x3D;p_l^{e_l-1}$个，故其中与$p_l^{e_l}$互素的个数共有$p_l^{e_l}-p_l^{e_l-1}$个，于是$$\\varphi(p_l^{e_l})&#x3D;p_l^{e_l}-p_l^{e_l-1}$$这样一来$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}(p_l^{e_l}-p_l^{e_l-1})&#x3D;\\prod_{l&#x3D;1}^{s}p_l^{e_l}(1-\\frac{1}{p_l})&#x3D;n\\prod_{l&#x3D;1}^{s}(1-\\frac{1}{p})$$其中若p为素数，则$$\\varphi(p)&#x3D;p-1&#x3D;p(1-\\frac{1}{p})$$ 8.RSA算法我们终于可以来看 RSA 算法了. 先来看 RSA 算法是怎么运作的: RSA 算法按照以下过程创建公钥和私钥: 1.随机算取两个大素数p和q，$p\\neq q$ 2.计算$n&#x3D;pq$ 3.选取一个与$(p-1)(q-1)$互素的小整数e 4.求e模$(p-1)(q-1)$的逆，记作d，即$de\\equiv 1\\bmod(p-1)(q-1)$ 5.将$P&#x3D;(e,n)$公开，是公钥。 6.将$S&#x3D;(d,n)$保密，作为私钥 想 要把明文$M $加密成密文$C$,计算$$C&#x3D;M^e\\bmod n$$要把密文解密成明文$C$$M $,计算$$M&#x3D;C^d \\bmod n$$ 下面证明RSA算法是有效的： 证明：要证明其有效性，只需要证明$C\\equiv M^e\\bmod n$也就是$M^{ed}\\equiv M(\\bmod n)$,注意到d为e模$(p-1)(1-1)$的逆，所以有$$ed\\equiv 1(\\bmod(p-1)(q-1))$$ 。","categories":[{"name":"CTF入门","slug":"CTF入门","permalink":"https://snowman12137.github.io/categories/CTF%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"现代密码学","slug":"现代密码学","permalink":"https://snowman12137.github.io/tags/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"author":"Sn0wma1n"},{"title":"大三网络安全人工智能实验报告","slug":"大三网络安全人工智能实验报告","date":"2023-05-04T12:09:16.000Z","updated":"2023-09-02T14:31:50.024Z","comments":true,"path":"2023/05/04/大三网络安全人工智能实验报告/","link":"","permalink":"https://snowman12137.github.io/2023/05/04/%E5%A4%A7%E4%B8%89%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/","excerpt":"","text":"《人工智能》课程实验报告网络与信息安全学院班 级： 20180XX 姓 名： XXX 学 号： 20009XXXX 提交时间： 2023. 4. 20 基于神经网络的MNIST手写数字识别一、实验目的 掌握运用神经网络模型解决有监督学习问题 掌握机器学习中常用的模型训练测试方法 了解不同训练方法的选择对测试结果的影响 二、实验内容MNIST数据集​ 本实验采用的数据集MNIST是一个手写数字图片数据集，共包含图像和对应的标签。数据集中所有图片都是28x28像素大小，且所有的图像都经过了适当的处理使得数字位于图片的中心位置。MNIST数据集使用二进制方式存储。图片数据中每个图片为一个长度为784（28x28x1，即长宽28像素的单通道灰度图）的一维向量，而标签数据中每个标签均为长度为10的一维向量。 分层采样方法​ 分层采样（或分层抽样，也叫类型抽样）方法，是将总体样本分成多个类别，再分别在每个类别中进行采样的方法。通过划分类别，采样出的样本的类型分布和总体样本相似，并且更具有代表性。在本实验中，MNIST数据集为手写数字集，有0~9共10种数字，进行分层采样时先将数据集按数字分为10类，再按同样的方式分别进行采样。 神经网络模型评估方法​ 通常，我们可以通过实验测试来对神经网络模型的误差进行评估。为此，需要使用一个测试集来测试模型对新样本的判别能力，然后以此测试集上的测试误差作为误差的近似值。两种常见的划分训练集和测试集的方法： ​ 留出法（hold-out）直接将数据集按比例划分为两个互斥的集合。划分时为尽可能保持数据分布的一致性，可以采用分层采样（stratified sampling）的方式，使得训练集和测试集中的类别比例尽可能相似。需要注意的是，测试集在整个数据集上的分布如果不够均匀还可能引入额外的偏差，所以单次使用留出法得到的估计结果往往不够稳定可靠。在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。 ​ k折交叉验证法（k-fold cross validation）先将数据集划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即也采用分层采样（stratified sampling）的方法。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练集和测试集，从而可以进行k次训练和测试。最终返回的是这k个测试结果的均值。显然，k折交叉验证法的评估结果的稳定性和保真性在很大程度上取决于k的取值。k最常用的取值是10，此外常用的取值还有5、20等。 三、实验方法设计 实验环境 1.VSCODE 2.anaconda&#x3D;&#x3D;4.14.0 3.python&#x3D;&#x3D;3.7 4.TensorFlow–gpu&#x3D;&#x3D;1.15.0 5.Keras&#x3D;&#x3D;2.3.1 6.实验报告编辑器：typora 介绍实验中程序的总体设计方案、关键步骤的编程方法及思路，主要包括: 因为之前用过pytorch进行机器学习的训练和学习，所以本作业使用pytorch进行建模和训练。 标准训练流程如下：导入包-&gt;设定初始值-&gt;加载数据集（预处理）-&gt;建立模型-&gt;训练-&gt;测试-&gt;评估 其中需要对加载数据集进行处理，把留出法的比例进行调整来观察结果。 其次要使用k折交叉验证法进行对比测试。 为了表明k折交叉验证法与留出法的效果对比，我建立了连个模型，一个是按照标准流程建立的优秀的模型。一个是用作对比k折交叉验证法与留出法效果对比的劣质模型。 含有tensorflow部分代码，在四、4中。 设置初始值 123456&gt;mean = [0.5]&gt;std = [0.5]&gt;# batch size&gt;BATCH_SIZE =128&gt;Iterations = 1 # epoch&gt;learning_rate = 0.01 优化器与损失函数 12&gt;criterion = torch.nn.CrossEntropyLoss() &gt;optimizer = torch.optim.SGD(model.parameters(),learning_rate) 训练代码 12345678910111213&gt;def train(model, optimizer,criterion,epoch): model.train() # setting up for training for batch_idx, (data, target) in enumerate(train_loader): # data contains the image and target contains the label = 0/1/2/3/4/5/6/7/8/9 data = data.view(-1, 28*28).requires_grad_() optimizer.zero_grad() # setting gradient to zero output = model(data) # forward loss = criterion(output, target) # loss computation loss.backward() # back propagation here pytorch will take care of it optimizer.step() # updating the weight values if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) 测试代码 1234567891011121314151617181920212223&gt;def test(model, criterion, val_loader, epoch,train= False): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for batch_idx, (data, target) in enumerate(val_loader): data = data.view(-1, 28*28).requires_grad_() output = model(data) test_loss += criterion(output, target).item() # sum up batch loss pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() # if pred == target then correct +=1 test_loss /= len(val_loader.dataset) # average test loss if train == False: print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.4f&#125;%)\\n&#x27;.format( test_loss, correct, val_loader.sampler.__len__(), 100. * correct / val_loader.sampler.__len__() )) if train == True: print(&#x27;\\nTrain set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.4f&#125;%)\\n&#x27;.format( test_loss, correct, val_loader.sampler.__len__(), 100. * correct / val_loader.sampler.__len__() )) return 100. * correct / val_loader.sampler.__len__() 1）模型构建的程序设计（伪代码或源代码截图）及说明解释 （10分）训练模型 12345678910111213141516class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output 我用了一层conv和一层pool来获取cherng图片的特征。之后把这些特征减小为10个层，所以用flatten把特征集中成vector后，再用一个全连接层连接到输出层。 使用留出法原始的训练比例，两个Epoch，得到的结果很好。达到97%。 为了对比留出法及K折验证法建立的简陋模型 1model = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),nn.Linear(256, 10)) 可以看到这个简陋模型得到的结果很差，准确率只有83%，用于之后的K折校验法的对比组。预处理和上述相同，Epoch只有一组。 2）模型迭代训练的程序设计（伪代码或源代码截图）及说明解释 （10分）123456789101112131415def train(model, optimizer,criterion,epoch): model.train() # setting up for training for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28).requires_grad_() optimizer.zero_grad() # setting gradient to zero output = model(data) # forward loss = criterion(output, target) # loss computation loss.backward() # back propagation here pytorch will take care of it optimizer.step() # updating the weight values if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) 先注意到因為 training 和 testing 時 model 會有不同行為，所以用 model.train() 把 model 調成 training 模式。 接著 iterate 過 batch_idx，每個 batch_idx會 train 過整個 training set。每個 dataset 會做 batch training。 接下來就是重點了。基本的步驟：zero_grad、model(data)、取 loss、back propagation 算 gradient、最後 update parameter。前面都介紹過了，還不熟的可以往前翻。 3）模型训练过程中周期性测试的程序设计（伪代码或源代码截图）及说明解释（周期性测试指的是每训练n个step就对模型进行一次测试，得到准确率和loss值）（10分） 我选用了每100步进行一个打印的频率，打印训练进度和Loss值，最后打印平均损失值和准确率。 4）分层采样的程序设计（伪代码或源代码截图）及说明解释 （10分）12345678910111213141516171819train_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std) ]) test_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std) ]) train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;./mnist&#x27;, train=True, download=True, transform=train_transform), batch_size=BATCH_SIZE, shuffle=True) # train dataset test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;./mnist&#x27;, train=False, transform=test_transform), batch_size=BATCH_SIZE, shuffle=False) # test dataset 123456789&gt;def hold_out(images, labels, train_percentage): test_acc = torch.zeros([Iterations]) train_acc = torch.zeros([Iterations]) ## training the logistic model for i in range(Iterations): train(model, optimizer,criterion,i) train_acc[i] = test(model, criterion, train_loader, i,train=True) #Testing the the current CNN test_acc[i] = test(model, criterion, test_loader, i) torch.save(model,&#x27;perceptron.pt&#x27;) 使用了系统自带的minist数据分类器。 5）k折交叉验证法的程序设计（伪代码或源代码截图）及说明解释 （10分） mnist数据集的训练集和测试集的合并 123456789train_init = datasets.MNIST(&#x27;./mnist&#x27;, train=True, transform=train_transform) test_init = datasets.MNIST(&#x27;./mnist&#x27;, train=False, transform=test_transform) # the dataset for k fold cross validation dataFold = torch.utils.data.ConcatDataset([train_init, test_init]) 使用Sklearn中的KFold进行数据集划分，并且转换回pytorch类型的Dataloader 123456789kf = KFold(n_splits=k_split_value,shuffle=True, random_state=0) # init KFold for train_index , test_index in kf.split(dataFold): # split # get train, val 根据索引划分 train_fold = torch.utils.data.dataset.Subset(dataFold, train_index) test_fold = torch.utils.data.dataset.Subset(dataFold, test_index) train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True) 完整的代码 1234567891011121314151617181920212223242526def train_flod_Mnist(k_split_value): different_k_mse = [] kf = KFold(n_splits=k_split_value,shuffle=True, random_state=0) # init KFold for train_index , test_index in kf.split(dataFold): # split # get train, val train_fold = torch.utils.data.dataset.Subset(dataFold, train_index) test_fold = torch.utils.data.dataset.Subset(dataFold, test_index) # package type of DataLoader train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True) # train model test_acc = torch.zeros([Iterations]) train_acc = torch.zeros([Iterations]) ## training the logistic model for i in range(Iterations): train(model, optimizer,criterion,i) train_acc[i] = test(model, criterion, train_loader, i,train=True) #Testing the the current CNN test_acc[i] = test(model, criterion, test_loader, i) #torch.save(model,&#x27;perceptron.pt&#x27;) # one epoch, all acc different_k_mse.append(np.array(test_acc)) return different_k_mse 按循序打印结果 123456testAcc_compare_map = &#123;&#125;for k_split_value in range(2, 10+1): print(&#x27;now k_split_value is:&#x27;, k_split_value) testAcc_compare_map[k_split_value] = train_flod_Mnist(k_split_value)for key in testAcc_compare_map:print(np.mean(testAcc_compare_map[key])) testAcc_compare_map是将不同k值下训练的结果保存起来，之后我们可以通过这个字典变量，计算出rmse ，比较不同k值下，实验结果的鲁棒性。 四、实验结果展示展示程序界面设计、运行结果及相关分析等，主要包括： 1）模型在验证集下的准确率（输出结果并截图）（10分） 下面的实验是k值为[2,10]下的结果，训练模型为简陋模型。 对照组：简陋模型，epoch为1，分层抽样（正确率只有83.79%） k折校验：简陋模型，epoch为1，K折交叉验证(K值为2到10)准确率越来越大 2）不同模型参数（隐藏层数、隐藏层节点数）对准确率的影响和分析 （10分） 本次实验中只探讨了简陋版模型与卷积模型的对比： 其中简陋版模型如下，先把图片变为一个以为张量，然后由一个全连接层链接，接入到ReLu层中，然后接入全连接层，可以看到，并没有使用卷积层，在epoch&#x3D;1的情况下只有83%的准确率。 1&gt;model = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),nn.Linear(256, 10)) 卷积模型如下，先定义卷积卷积层，输入通道为1，输出为32，核大小为3，一个Dropout2d层，以0.25的概率将通道输入置零，防止过拟合。然后是一个全连接层，输入为5408，输出为10，映射到10个分类结果。在forward中首先通过卷积层进行卷积，然后通过ReLU进行非线性变换，然后使用最大池化层进行采样，将图签尺寸缩小一半，然后用Dropout2d防止过拟合，接着把输出的张良展平为一维，并传入全连接层。 在 12345678910111213141516&gt;class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output 可以看到在epoch&#x3D;1的情况下准确率达到97%，拟合效果非常好 3）不同训练参数（batch size、epoch num、学习率）对准确率的影响和分析 （10分） 注：默认值：在讨论某一变化时，其他值不变 123&gt;BATCH_SIZE =64&gt;Iterations = 1 # epoch&gt;learning_rate = 0.01 原始结果（83%） BATCH_SIZE讨论（可以发现当BATCH_SIZE越大时，准确率直线下降） 1&gt;&#123;&quot;16&quot;:&quot;90.85%&quot;,&quot;32&quot;:&quot;90.1000%&quot;,&quot;64&quot;:&quot;88.2200%&quot;,&quot;128&quot;:&quot;83.7100%&quot;,&quot;256&quot;:&quot;70.7300%&quot;,&quot;512&quot;:&quot;48.2400%&quot;&#125; epoch讨论(可以发现随着epoch的增大，准确率有较大提升，但是随着epoch越来越大，准确率增长越来越慢) 12[1,2,4,6,10,15][83.83,88.370,90.290,91.260,92.420,93.36] 学习率 可以看到，当学习率增大时，准确率有所增加，但是当学习率大于0.2时，准确率急速下滑到11%左右，也就是说，10个手写体正确率只有1个，趋于随机分布，是一个非常不好的模型，可见，学习率的选择至关重要。 12&gt;[0.01,0.02,0.05,0.1,0.2,0.5,1]&gt;[83.83,88.230,90.78,91.79,91.78,11.35,11.35] 4）留出法不同比例对结果的影响和分析 （10分） 因为pytorch中的训练集是固定输出的，对其更改较难，所以本小节使用TensorFlow进行实验： 数据集划分:其中a为训练比率，总共有70000个样本，按照比例进行训练和测试，结果如下 1234567891011121314151617181920212223&gt;np.random.seed(10)&gt;a = 0.8&gt;from keras.datasets import mnist&gt;(x_train_image,y_train_label),(x_test_image,y_test_label)=mnist.load_data()&gt;# x_all = x_train_image + x_test_image&gt;temp = np.append( x_train_image , x_test_image)&gt;x_all = temp.reshape(70000,28,28)&gt;print(len(y_train_label))&gt;y_lable = np.append(y_train_label,y_test_label)&gt;train_num = int(60000*a)&gt;x_train_image = x_all[:train_num]&gt;y_train_label = y_lable[:train_num]&gt;x_test_image = x_all[train_num:]&gt;y_test_label = y_lable[train_num:]&gt;x_Train=x_train_image.reshape(train_num,784).astype(&#x27;float32&#x27;)&gt;x_Test=x_test_image.reshape(70000-train_num,784).astype(&#x27;float32&#x27;) 12&gt;[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.9999]&gt;[0.9213, 0.9381, 0.9528, 0.9620, 0.9666, 0.9673, 0.9709, 0.9752, 0.9758, 0.9784, 0.9780] 可以看到随着训练样本的比率上升，总体的准确率也对应的明显的上升了，但是最后一组0.9999比率的组，较前一组0.95有所下降，这表明过大的训练比率对结果也会产生损害。 5）k折交叉验证法不同k值对结果的影响和分析 （10分） 把k值从2-10进行迭代计算，其他参数不变，结果为： 12345678&gt;testAcc_compare_map = &#123;&#125;&gt;for k_split_value in range(2, 10+1): print(&#x27;now k_split_value is:&#x27;, k_split_value) testAcc_compare_map[k_split_value] = cross_validation(k_split_value) &gt;for key in testAcc_compare_map: print(np.mean(testAcc_compare_map[key])) 可见K值对结果影响很大，且在一定范围内，越大越好。 五、实验总结及心得 本次实验熟知了pytorch和TensorFlow的使用，还有机器学习的整体流程和处理概况，解决了出现的诸多问题，尤其是在配置TensorFlow版本时出现的问题，熟知了基本的图像处理模型，以及卷积模型的基本构建。 在参数调配方面，详细了解了分层取样法，k折交叉验证法的使用以及效果还有比例的调试。还有在关键参数如batch size、epoch num、学习率方面有着较好的经验总结。","categories":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://snowman12137.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"author":"Sn0wma1n"},{"title":"大三网络安全智能终端实验一","slug":"大三网络安全智能终端实验一","date":"2023-05-04T12:09:16.000Z","updated":"2023-09-02T14:31:41.714Z","comments":true,"path":"2023/05/04/大三网络安全智能终端实验一/","link":"","permalink":"https://snowman12137.github.io/2023/05/04/%E5%A4%A7%E4%B8%89%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E6%99%BA%E8%83%BD%E7%BB%88%E7%AB%AF%E5%AE%9E%E9%AA%8C%E4%B8%80/","excerpt":"","text":"西安电子科技大学网信院 智能终端实验实验报告（一）班级：2018XX学号：20009XXXX日期：2023.4.24一、实验摘要研究不断推动机器学习模型更快，更精确，更有效。然而，设计和训练模型的一个经常被忽视的方面是安全性和鲁棒性，特别是在面对希望欺骗模型的对手时。向图像添加不可察觉的扰动可以导致完全不同的模型性能。我们将通过一个图像分类器的例子来探讨这个主题。具体来说，我们将使用第一个和最流行的攻击方法之一，快速梯度符号攻击(FGSM) ，以欺骗 MNIST 分类器。（摘自https://pytorch.org/tutorials/beginner/ fgsm_tutorial.htm l?highlight&#x3D;a dversarial% 20example%20generation） 二、实验内容a) 实验思路#### 简述 有许多类别的对抗性攻击，每种攻击都有不同的目标和对攻击者知识的假设。然而，一般来说，总体目标是向输入数据添加最少量的扰动，从而导致所需的错误分类。有几种假设攻击者的知识，其中两个是: 白盒和黑盒。白盒攻击假设攻击者对模型有完整的知识和访问权限，包括体系结构、输入、输出和权重。黑盒攻击假设攻击者只能访问模型的输入和输出，并且对底层架构或权重一无所知。还有几种类型的目标，包括错误分类和源&#x2F;目标错误分类。错误分类的目标意味着对手只希望输出分类是错误的，而不关心新的分类是什么。源&#x2F;目标错误分类意味着对手想要更改原来属于特定源类的图像，以便将其归类为特定目标类。在这种情况下，FGSM 攻击是以错误分类为目标的白盒攻击。有了这些背景信息，我们现在可以详细讨论这次攻击了。 快速梯度 原理简述快速梯度符号攻击（FGSA），是通过基于相同的反向传播梯度来调整输入数据使损失最大化，简言之，共计使用了丢失的梯度值w.r.t输入数据，然后调整输入使数据丢失最大化。下图是实现的例子： 可以看出x是正常“熊猫”的原始输入图像，y是真实的图片输入， θ表示模型参数，J(θ,x,y)是损失函数，∇x​J(θ,x,y)是攻击将梯度反向传播回要计算的输入数据。然后，它通过一个小步骤调整输入数据，如0.007倍的sign(∇x​J(θ,x,y))添加到里面会最大化损失函数。然后得到扰动图像x’，然后被目标网络错误地归类为“长臂猿”，而它显然仍然是一只“熊猫”。 b) 实现过程1.输入只有三个输入 Epsilons-运行时使用的 epsilon 值列表。在列表中保留0很重要，因为它表示原始测试集上的模型性能。此外，直观地，我们期望更大的 ε，更明显的扰动，但更有效的攻击方面的退化模型的准确性。因为这里的数据范围是 [ 0 , 1 ] [0,1] ，ε 值不应超过1。 Pretrain _ model-path加载预训练的模型 Use _ CUDA-boolean 标志来使用 CUDA，如果需要的话。 123epsilons = [0, .05, .1, .15, .2, .25, .3]pretrained_model = &quot;mnist_cnn.pt&quot;use_cuda=True 2.模型建立a)训练模型我們用了一層 convolution layer 和 pooling layer 來擷取 image 的 feature，之後要把這些 feature map 成 10 個 node 的 output（因為有 10 個 class 要 predict），所以用 flatten 把 feature 集中成 vector 後，再用 fully-connected layer map 到 output layer。b 是 batch size，一次 train 幾張 image。 12345678910111213141516class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output b)训练函数123456789101112131415161718192021222324def train(model, train_loader, optimizer, epochs, log_interval): model.train() for epoch in range(1, epochs + 1): for batch_idx, (data, target) in enumerate(train_loader): # Clear gradient optimizer.zero_grad() # Forward propagation output = model(data) # Negative log likelihood loss loss = F.nll_loss(output, target) # Back propagation loss.backward() # Parameter update optimizer.step() # Log training info if batch_idx % log_interval == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) c)训练中的测试1234567891011121314151617181920def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): # disable gradient calculation for efficiency for data, target in test_loader: # Prediction output = model(data) # Compute loss &amp; accuracy test_loss += F.nll_loss(output, target, reduction=&#x27;sum&#x27;).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) # Log testing info print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\\n&#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) d)训练的主函数（把模型保存）12345678910111213141516171819202122232425262728293031323334def main(): # Training settings BATCH_SIZE = 64 EPOCHS = 2 LOG_INTERVAL = 10 # Define image transform transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) # mean and std for the MNIST training set ]) # Load dataset train_dataset = datasets.MNIST(&#x27;./data&#x27;, train=True, download=True, transform=transform) test_dataset = datasets.MNIST(&#x27;./data&#x27;, train=False, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE) # Create network &amp; optimizer model = Net() optimizer = optim.Adam(model.parameters()) # Train train(model, train_loader, optimizer, EPOCHS, LOG_INTERVAL) # Save and load model torch.save(model.state_dict(), &quot;mnist_cnn.pt&quot;) model = Net() model.load_state_dict(torch.load(&quot;mnist_cnn.pt&quot;)) # Test test(model, test_loader) 训练参数储存 e)训练结果 3.FGSM攻击a)模型加载加载模型 1234567891011121314151617181920# MNIST Test dataset and dataloader declarationtransform = transforms.Compose([ transforms.ToTensor(), ])train_dataset = datasets.MNIST(&#x27;./data&#x27;, train=True, download=True, transform=transform)test_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)# Define what device we are usingprint(&quot;CUDA Available: &quot;,torch.cuda.is_available())device = torch.device(&quot;cuda&quot; if (use_cuda and torch.cuda.is_available()) else &quot;cpu&quot;)# Initialize the networkmodel = Net().to(device)# Load the pretrained modelmodel.load_state_dict(torch.load(&quot;mnist_cnn.pt&quot;, map_location=&#x27;cpu&#x27;))# Set the model in evaluation mode. In this case this is for the Dropout layersmodel.eval() b)攻击函数创建现在，我们可以通过扰动原始输入来定义创建对抗性示例的函数。Fgsm 攻击函数有三个输入，图像是原始清晰图像(xx) ，ε 是像素级扰动量(εε) ，data _ grad 是损失的梯度，输入图像(∇xJ(θ,x,y))。然后，该函数创建扰动图像$$perturbed_image&#x3D;image+epsilon∗sign(data_grad)&#x3D;x+ϵ∗sign(∇x​ J(θ,x,y))$$ 12345678910# FGSM attack codedef fgsm_attack(image, epsilon, data_grad): # Collect the element-wise sign of the data gradient sign_data_grad = data_grad.sign() # Create the perturbed image by adjusting each pixel of the input image perturbed_image = image + epsilon*sign_data_grad # Adding clipping to maintain [0,1] range perturbed_image = torch.clamp(perturbed_image, 0, 1) # Return the perturbed image return perturbed_image c)开始攻击12345678accuracies = []examples = []# Run test for each epsilonfor eps in epsilons: acc, ex = test(model, device, test_loader, eps) accuracies.append(acc) examples.append(ex) d) 实验结果截图第一个结果是精度对 ε 图。正如前面提到的，随着 ε 的增加，我们预计测试的准确性会降低。这是因为更大的 ε 意味着我们在将损失最大化的方向上迈出了更大的一步。注意曲线中的趋势不是线性的，即使 ε 值是线性间隔的。例如，ε &#x3D; 0.05 ε &#x3D; 0.05的准确性仅比 ε &#x3D; 0ε &#x3D; 0低约4% ，但是 ε &#x3D; 0.2 ε &#x3D; 0.2的准确性比 ε &#x3D; 0.15 ε &#x3D; 0.15低25% 。另外，请注意模型的精度在 ε &#x3D; 0.25 ε &#x3D; 0.25和 ε &#x3D; 0.3 ε &#x3D; 0.3之间的随机精度。 打印上述数据 12345678910111213141516# Plot several examples of adversarial samples at each epsilon cnt = 0 plt.figure(figsize=(8,10)) for i in range(len(epsilons)): for j in range(len(examples[i])): cnt += 1 plt.subplot(len(epsilons),len(examples[0]),cnt) plt.xticks([], []) plt.yticks([], []) if j == 0: plt.ylabel(&quot;Eps: &#123;&#125;&quot;.format(epsilons[i]), fontsize=14) orig,adv,ex = examples[i][j] plt.title(&quot;&#123;&#125; -&gt; &#123;&#125;&quot;.format(orig, adv)) plt.imshow(ex, cmap=&quot;gray&quot;) plt.tight_layout() plt.show() 打印几个代表性的例子 三、实验结果分析分析从结果来看，随着 ε 的增加，测试精度下降，但扰动变得更容易察觉。实际上，在攻击者必须考虑的精确度下降和可感知性之间存在权衡。在这里，我们展示了一些成功的对抗例子在每个 ε 值。图的每一行显示不同的 ε 值。第一行是 ε &#x3D; 0 ε &#x3D; 0的例子，它表示原始的“干净”图像，没有扰动。每张图片的标题都显示了“原始分类-&gt; 敌对分类”注意，当 ε &#x3D; 0.15 ε &#x3D; 0.15时，扰动开始变得明显，当 ε &#x3D; 0.3 ε &#x3D; 0.3时，扰动非常明显。然而，在所有情况下，人们仍然能够识别正确的类别，尽管增加了噪音。","categories":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}]},{"title":"my_first_blog","slug":"my-first-blog","date":"2023-04-16T12:09:16.000Z","updated":"2023-05-04T13:05:17.120Z","comments":true,"path":"2023/04/16/my-first-blog/","link":"","permalink":"https://snowman12137.github.io/2023/04/16/my-first-blog/","excerpt":"","text":"我的博客 123import osprint(&quot;这是我的第一段代码&quot;)print(&quot;Hello Blog&quot;)","categories":[],"tags":[{"name":"测试博客","slug":"测试博客","permalink":"https://snowman12137.github.io/tags/%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2/"}]}],"categories":[{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/categories/SGX/"},{"name":"CTF入门","slug":"CTF入门","permalink":"https://snowman12137.github.io/categories/CTF%E5%85%A5%E9%97%A8/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"},{"name":"博客问题","slug":"博客问题","permalink":"https://snowman12137.github.io/tags/%E5%8D%9A%E5%AE%A2%E9%97%AE%E9%A2%98/"},{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/tags/SGX/"},{"name":"Linux","slug":"Linux","permalink":"https://snowman12137.github.io/tags/Linux/"},{"name":"现代密码学","slug":"现代密码学","permalink":"https://snowman12137.github.io/tags/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","permalink":"https://snowman12137.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"},{"name":"测试博客","slug":"测试博客","permalink":"https://snowman12137.github.io/tags/%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2/"}]}