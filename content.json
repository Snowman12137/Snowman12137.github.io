{"meta":{"title":"小吴の博客","subtitle":"高冷白羊","description":"我的博客","author":"Snowman","url":"https://snowman12137.github.io","root":"/"},"pages":[{"title":"about","date":"2023-04-16T12:45:28.000Z","updated":"2023-04-16T12:46:28.171Z","comments":true,"path":"about/index.html","permalink":"https://snowman12137.github.io/about/index.html","excerpt":"","text":"关于我这是一个测试"},{"title":"","date":"2023-04-16T12:47:01.000Z","updated":"2024-02-03T17:23:39.226Z","comments":true,"path":"tags/index.html","permalink":"https://snowman12137.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2023-04-16T12:47:12.000Z","updated":"2024-02-03T17:22:50.822Z","comments":true,"path":"categories/index.html","permalink":"https://snowman12137.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2024-02-04T08:25:07.508Z","updated":"2024-02-04T08:25:07.450Z","comments":true,"path":"baidu_verify_codeva-pSRT9jCE5w.html","permalink":"https://snowman12137.github.io/baidu_verify_codeva-pSRT9jCE5w.html","excerpt":"","text":"302a7483392941dc41427982527c540d"}],"posts":[{"title":"Node.js学习（三）","slug":"Node-js学习（三）","date":"2024-02-01T16:00:00.000Z","updated":"2024-02-03T17:04:09.455Z","comments":true,"path":"2024/02/02/Node-js学习（三）/","link":"","permalink":"https://snowman12137.github.io/2024/02/02/Node-js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/","excerpt":"","text":"Node.js学习（三）1.工具模块a.OS模块12345678910111213141516var os = require(&quot;os&quot;);//引入// CPU 的字节序console.log(&#x27;endianness : &#x27; + os.endianness());// 操作系统名console.log(&#x27;type : &#x27; + os.type());// 操作系统名console.log(&#x27;platform : &#x27; + os.platform());// 系统内存总量console.log(&#x27;total memory : &#x27; + os.totalmem() + &quot; bytes.&quot;);// 操作系统空闲内存量console.log(&#x27;free memory : &#x27; + os.freemem() + &quot; bytes.&quot;); b.Path模块123456789var path = require(&quot;path&quot;);// 格式化路径console.log(&#x27;normalization : &#x27; + path.normalize(&#x27;/test/test1//2slashes/1slash/tab/..&#x27;));// 连接路径console.log(&#x27;joint path : &#x27; + path.join(&#x27;/test&#x27;, &#x27;test1&#x27;, &#x27;2slashes/1slash&#x27;, &#x27;tab&#x27;, &#x27;..&#x27;));// 转换为绝对路径console.log(&#x27;resolve : &#x27; + path.resolve(&#x27;main.js&#x27;));// 路径中文件的后缀名console.log(&#x27;ext name : &#x27; + path.extname(&#x27;main.js&#x27;)); c.Net模块(Socket)server.js服务端 123456789101112var net = require(&#x27;net&#x27;);var server = net.createServer(function(connection) &#123; console.log(&#x27;client connected&#x27;); connection.on(&#x27;end&#x27;, function() &#123; console.log(&#x27;客户端关闭连接&#x27;); &#125;); connection.write(&#x27;Hello World!\\r\\n&#x27;); connection.pipe(connection);&#125;);server.listen(8080, function() &#123; console.log(&#x27;server is listening&#x27;);&#125;); client.js客户端 1234567891011var net = require(&#x27;net&#x27;);var client = net.connect(&#123;port: 8080&#125;, function() &#123; console.log(&#x27;连接到服务器！&#x27;); &#125;);client.on(&#x27;data&#x27;, function(data) &#123; console.log(data.toString()); client.end();&#125;);client.on(&#x27;end&#x27;, function() &#123; console.log(&#x27;断开与服务器的连接&#x27;);&#125;); d.DNS模块123456789101112var dns = require(&#x27;dns&#x27;);dns.lookup(&#x27;www.github.com&#x27;, function onLookup(err, address, family) &#123; console.log(&#x27;ip 地址:&#x27;, address); dns.reverse(address, function (err, hostnames) &#123; if (err) &#123; console.log(err.stack); &#125; console.log(&#x27;反向解析 &#x27; + address + &#x27;: &#x27; + JSON.stringify(hostnames));&#125;); &#125;); 执行结果 12address: 192.30.252.130reverse for 192.30.252.130: [&quot;github.com&quot;] 2.Web模块web应用架构 Client - 客户端，一般指浏览器，浏览器可以通过 HTTP 协议向服务器请求数据。 Server - 服务端，一般指 Web 服务器，可以接收客户端请求，并向客户端发送响应数据。 Business - 业务层， 通过 Web 服务器处理应用程序，如与数据库交互，逻辑运算，调用外部程序等。 Data - 数据层，一般由数据库组成。 a.创建Web服务器以下是演示一个最基本的 HTTP 服务器架构(使用 8080 端口)，创建 server.js 文件，代码如下所示： 1234567891011121314151617181920212223242526272829303132333435var http = require(&#x27;http&#x27;);var fs = require(&#x27;fs&#x27;);var url = require(&#x27;url&#x27;); // 创建服务器http.createServer( function (request, response) &#123; // 解析请求，包括文件名 var pathname = url.parse(request.url).pathname; // 输出请求的文件名 console.log(&quot;Request for &quot; + pathname + &quot; received.&quot;); // 从文件系统中读取请求的文件内容 fs.readFile(pathname.substr(1), function (err, data) &#123; if (err) &#123; console.log(err); // HTTP 状态码: 404 : NOT FOUND // Content Type: text/html response.writeHead(404, &#123;&#x27;Content-Type&#x27;: &#x27;text/html&#x27;&#125;); &#125;else&#123; // HTTP 状态码: 200 : OK // Content Type: text/html response.writeHead(200, &#123;&#x27;Content-Type&#x27;: &#x27;text/html&#x27;&#125;); // 响应文件内容 response.write(data.toString()); &#125; // 发送响应数据 response.end(); &#125;); &#125;).listen(8080); // 控制台会输出以下信息console.log(&#x27;Server running at http://127.0.0.1:8080/&#x27;); index.html 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;我的第一个标题&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; b创建Web客户端创建 12345678910111213141516171819202122232425var http = require(&#x27;http&#x27;); // 用于请求的选项var options = &#123; host: &#x27;localhost&#x27;, port: &#x27;8080&#x27;, path: &#x27;/index.html&#x27; &#125;; // 处理响应的回调函数var callback = function(response)&#123; // 不断更新数据 var body = &#x27;&#x27;; response.on(&#x27;data&#x27;, function(data) &#123; body += data; &#125;); response.on(&#x27;end&#x27;, function() &#123; // 数据接收完成 console.log(body); &#125;);&#125;// 向服务端发送请求var req = http.request(options, callback);req.end(); 3.Express框架Express 是一个简洁而灵活的 node.js Web应用框架, 提供了一系列强大特性帮助你创建各种 Web 应用，和丰富的 HTTP 工具。 使用 Express 可以快速地搭建一个完整功能的网站。 Express 框架核心特性： 可以设置中间件来响应 HTTP 请求。 定义了路由表用于执行不同的 HTTP 请求动作。 可以通过向模板传递参数来动态渲染 HTML 页面。 a.安装安装 Express 并将其保存到依赖列表中： 1cnpm install express --save 以上命令会将 Express 框架安装在当前目录的 node_modules 目录中， node_modules 目录下会自动创建 express 目录。以下几个重要的模块是需要与 express 框架一起安装的： body-parser - node.js 中间件，用于处理 JSON, Raw, Text 和 URL 编码的数据。 cookie-parser - 这就是一个解析Cookie的工具。通过req.cookies可以取到传过来的cookie，并把它们转成对象。 multer - node.js 中间件，用于处理 enctype&#x3D;”multipart&#x2F;form-data”（设置表单的MIME编码）的表单数据。 123cnpm install body-parser --savecnpm install cookie-parser --savecnpm install multer --save b.请求和响应Express 应用使用回调函数的参数： request 和 response 对象来处理请求和响应的数据。 request 和 response 对象的具体介绍： Request 对象 - request 对象表示 HTTP 请求，包含了请求查询字符串，参数，内容，HTTP 头部等属性。常见属性有： req.app：当callback为外部文件时，用req.app访问express的实例 req.baseUrl：获取路由当前安装的URL路径 req.body &#x2F; req.cookies：获得「请求主体」&#x2F; Cookies req.hostname &#x2F; req.ip：获取主机名和IP地址 req.originalUrl：获取原始请求URL req.params：获取路由的parameters req.path：获取请求路径 req.protocol：获取协议类型 req.query：获取URL的查询参数串 req.route：获取当前匹配的路由 req.subdomains：获取子域名 req.accepts()：检查可接受的请求的文档类型 req.get()：获取指定的HTTP请求头 Response 对象 - response 对象表示 HTTP 响应，即在接收到请求时向客户端发送的 HTTP 响应数据。常见属性有： res.app：同req.app一样 res.append()：追加指定HTTP头 res.set()在res.append()后将重置之前设置的头 res.cookie(name，value [，option])：设置Cookie res.clearCookie()：清除Cookie res.download()：传送指定路径的文件 res.get()：返回指定的HTTP头 res.json()：传送JSON响应 res.jsonp()：传送JSONP响应 res.location()：只设置响应的Location HTTP头，不设置状态码或者close response res.render(view,[locals],callback)：渲染一个view，同时向callback传递渲染后的字符串，如果在渲染过程中有错误发生next(err)将会被自动调用。callback将会被传入一个可能发生的错误以及渲染后的页面，这样就不会自动输出了。 res.send()：传送HTTP响应 res.sendFile(path [，options] [，fn])：传送指定路径的文件 -会自动根据文件extension设定Content-Type res.set()：设置HTTP头，传入object可以一次设置多个头 c.路由我们已经了解了 HTTP 请求的基本应用，而路由决定了由谁(指定脚本)去响应客户端请求。在HTTP请求中，我们可以通过路由提取出请求的URL以及GET&#x2F;POST参数。接下来我们扩展 Hello World，添加一些功能来处理更多类型的 HTTP 请求。创建 express_demo2.js 文件，代码如下所示： 1234567891011121314151617181920212223242526272829303132var express = require(&#x27;express&#x27;);var app = express();// 主页输出 &quot;Hello World&quot;app.get(&#x27;/&#x27;, function (req, res) &#123; console.log(&quot;主页 GET 请求&quot;); res.send(&#x27;Hello GET&#x27;);&#125;)// POST 请求app.post(&#x27;/&#x27;, function (req, res) &#123; console.log(&quot;主页 POST 请求&quot;); res.send(&#x27;Hello POST&#x27;);&#125;)// /del_user 页面响应app.get(&#x27;/del_user&#x27;, function (req, res) &#123; console.log(&quot;/del_user 响应 DELETE 请求&quot;); res.send(&#x27;删除页面&#x27;);&#125;)// /list_user 页面 GET 请求app.get(&#x27;/list_user&#x27;, function (req, res) &#123; console.log(&quot;/list_user GET 请求&quot;); res.send(&#x27;用户列表页面&#x27;);&#125;)// 对页面 abcd, abxcd, ab123cd, 等响应 GET 请求app.get(&#x27;/ab*cd&#x27;, function(req, res) &#123; console.log(&quot;/ab*cd GET 请求&quot;); res.send(&#x27;正则匹配&#x27;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) d.静态文件Express 提供了内置的中间件 express.static 来设置静态文件如：图片， CSS, JavaScript 等。 你可以使用 express.static 中间件来设置静态文件路径。例如，如果你将图片， CSS, JavaScript 文件放在 public 目录下，你可以这么写： 1app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;)); 我们可以到 public&#x2F;images 目录下放些图片,如下所示： 12345node_modulesserver.jspublic/public/imagespublic/images/logo.png 让我们再修改下 “Hello World” 应用添加处理静态文件的功能。 创建 express_demo3.js 文件，代码如下所示： 1234567891011var express = require(&#x27;express&#x27;);var app = express();app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;));app.get(&#x27;/&#x27;, function (req, res) &#123; res.send(&#x27;Hello World&#x27;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) e.GET方法以下实例演示了在表单中通过 GET 方法提交两个参数，我们可以使用 server.js 文件内的 process_get 路由器来处理输入： idnex.html 12345678910&lt;html&gt;&lt;body&gt;&lt;form action=&quot;http://127.0.0.1:8081/process_get&quot; method=&quot;GET&quot;&gt;First Name: &lt;input type=&quot;text&quot; name=&quot;first_name&quot;&gt; &lt;br&gt; Last Name: &lt;input type=&quot;text&quot; name=&quot;last_name&quot;&gt;&lt;input type=&quot;submit&quot; value=&quot;Submit&quot;&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; server.js 123456789101112131415161718192021var express = require(&#x27;express&#x27;);var app = express();app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;));app.get(&#x27;/index.html&#x27;, function (req, res) &#123; res.sendFile( __dirname + &quot;/&quot; + &quot;index.html&quot; );&#125;)app.get(&#x27;/process_get&#x27;, function (req, res) &#123; // 输出 JSON 格式 var response = &#123; &quot;first_name&quot;:req.query.first_name, &quot;last_name&quot;:req.query.last_name &#125;; console.log(response); res.end(JSON.stringify(response));&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port) &#125;) f.文件上传以下我们创建一个用于上传文件的表单，使用 POST 方法，表单 enctype 属性设置为 multipart&#x2F;form-data。 1234567891011121314&lt;html&gt;&lt;head&gt;&lt;title&gt;文件上传表单&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;文件上传：&lt;/h3&gt;选择一个文件上传: &lt;br /&gt;&lt;form action=&quot;/file_upload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;&lt;input type=&quot;file&quot; name=&quot;image&quot; size=&quot;50&quot; /&gt;&lt;br /&gt;&lt;input type=&quot;submit&quot; value=&quot;上传文件&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; server.js 1234567891011121314151617181920212223242526272829303132333435var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);var bodyParser = require(&#x27;body-parser&#x27;);var multer = require(&#x27;multer&#x27;);app.use(&#x27;/public&#x27;, express.static(&#x27;public&#x27;));app.use(bodyParser.urlencoded(&#123; extended: false &#125;));app.use(multer(&#123; dest: &#x27;/tmp/&#x27;&#125;).array(&#x27;image&#x27;));app.get(&#x27;/index.html&#x27;, function (req, res) &#123; res.sendFile( __dirname + &quot;/&quot; + &quot;index.html&quot; );&#125;)app.post(&#x27;/file_upload&#x27;, function (req, res) &#123; console.log(req.files[0]); // 上传的文件信息 var des_file = __dirname + &quot;/&quot; + req.files[0].originalname; fs.readFile( req.files[0].path, function (err, data) &#123; fs.writeFile(des_file, data, function (err) &#123; if( err )&#123; console.log( err ); &#125;else&#123; response = &#123; message:&#x27;File uploaded successfully&#x27;, filename:req.files[0].originalname &#125;; &#125; console.log( response ); res.end( JSON.stringify( response ) ); &#125;); &#125;);&#125;) var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) g.Cookie管理我们可以使用中间件向 Node.js 服务器发送 cookie 信息，以下代码输出了客户端发送的 cookie 信息： 12345678910// express_cookie.js 文件var express = require(&#x27;express&#x27;)var cookieParser = require(&#x27;cookie-parser&#x27;)var util = require(&#x27;util&#x27;);var app = express()app.use(cookieParser())app.get(&#x27;/&#x27;, function(req, res) &#123; console.log(&quot;Cookies: &quot; + util.inspect(req.cookies));&#125;)app.listen(8081) 4.RESTful APIREST即表述性状态传递（英文：Representational State Transfer，简称REST）是Roy Fielding博士在2000年他的博士论文中提出来的一种软件架构风格。 表述性状态转移是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是RESTful。需要注意的是，REST是设计风格而不是标准。REST通常基于使用HTTP，URI，和XML（标准通用标记语言下的一个子集）以及HTML（标准通用标记语言下的一个应用）这些现有的广泛流行的协议和标准。REST 通常使用 JSON 数据格式。 a.RESTful Web ServicesWeb service是一个平台独立的，低耦合的，自包含的、基于可编程的web的应用程序，可使用开放的XML（标准通用标记语言下的一个子集）标准来描述、发布、发现、协调和配置这些应用程序，用于开发分布式的互操作的应用程序。 基于 REST 架构的 Web Services 即是 RESTful。 由于轻量级以及通过 HTTP 直接传输数据的特性，Web 服务的 RESTful 方法已经成为最常见的替代方法。可以使用各种语言（比如 Java 程序、Perl、Ruby、Python、PHP 和 Javascript[包括 Ajax]）实现客户端。 RESTful Web 服务通常可以通过自动客户端或代表用户的应用程序访问。但是，这种服务的简便性让用户能够与之直接交互，使用它们的 Web 浏览器构建一个 GET URL 并读取返回的内容。 b.创建 RESTfuluser.json 1234567891011121314151617181920&#123; &quot;user1&quot; : &#123; &quot;name&quot; : &quot;mahesh&quot;, &quot;password&quot; : &quot;password1&quot;, &quot;profession&quot; : &quot;teacher&quot;, &quot;id&quot;: 1 &#125;, &quot;user2&quot; : &#123; &quot;name&quot; : &quot;suresh&quot;, &quot;password&quot; : &quot;password2&quot;, &quot;profession&quot; : &quot;librarian&quot;, &quot;id&quot;: 2 &#125;, &quot;user3&quot; : &#123; &quot;name&quot; : &quot;ramesh&quot;, &quot;password&quot; : &quot;password3&quot;, &quot;profession&quot; : &quot;clerk&quot;, &quot;id&quot;: 3 &#125;&#125; 基于以上数据，我们创建以下 RESTful API： 序号 URI HTTP 方法 发送内容 结果 1 listUsers GET 空 显示所有用户列表 2 addUser POST JSON 字符串 添加新用户 3 deleteUser DELETE JSON 字符串 删除用户 4 :id GET 空 显示用户详细信息 c.获取用户列表：以下代码，我们创建了 RESTful API listUsers，用于读取用户的信息列表， server.js 文件代码如下所示： 123456789101112131415var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);app.get(&#x27;/listUsers&#x27;, function (req, res) &#123; fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; console.log( data ); res.end( data ); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) d添加用户以下代码，我们创建了 RESTful API addUser， 用于添加新的用户数据，server.js 文件代码如下所示： 1234567891011121314151617181920212223242526var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);//添加的新用户数据var user = &#123; &quot;user4&quot; : &#123; &quot;name&quot; : &quot;mohit&quot;, &quot;password&quot; : &quot;password4&quot;, &quot;profession&quot; : &quot;teacher&quot;, &quot;id&quot;: 4 &#125;&#125;app.get(&#x27;/addUser&#x27;, function (req, res) &#123; // 读取已存在的数据 fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; data = JSON.parse( data ); data[&quot;user4&quot;] = user[&quot;user4&quot;]; console.log( data ); res.end( JSON.stringify(data)); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) 显示用户详情以下代码，我们创建了 RESTful API :id（用户id）， 用于读取指定用户的详细信息，server.js 文件代码如下所示： 1234567891011121314151617var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);app.get(&#x27;/:id&#x27;, function (req, res) &#123; // 首先我们读取已存在的用户 fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; data = JSON.parse( data ); var user = data[&quot;user&quot; + req.params.id] console.log( user ); res.end( JSON.stringify(user)); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) 删除用户以下代码，我们创建了 RESTful API deleteUser， 用于删除指定用户的详细信息，以下实例中，用户 id 为 2，server.js 文件代码如下所示： 123456789101112131415161718var express = require(&#x27;express&#x27;);var app = express();var fs = require(&quot;fs&quot;);var id = 2;app.get(&#x27;/deleteUser&#x27;, function (req, res) &#123; // First read existing users. fs.readFile( __dirname + &quot;/&quot; + &quot;users.json&quot;, &#x27;utf8&#x27;, function (err, data) &#123; data = JSON.parse( data ); delete data[&quot;user&quot; + id]; console.log( data ); res.end( JSON.stringify(data)); &#125;);&#125;)var server = app.listen(8081, function () &#123; var host = server.address().address var port = server.address().port console.log(&quot;应用实例，访问地址为 http://%s:%s&quot;, host, port)&#125;) 5.Node.js 多进程我们都知道 Node.js 是以单线程的模式运行的，但它使用的是事件驱动来处理并发，这样有助于我们在多核 cpu 的系统上创建多个子进程，从而提高性能。 每个子进程总是带有三个流对象：child.stdin, child.stdout 和child.stderr。他们可能会共享父进程的 stdio 流，或者也可以是独立的被导流的流对象。 Node 提供了 child_process 模块来创建子进程，方法有： exec - child_process.exec 使用子进程执行命令，缓存子进程的输出，并将子进程的输出以回调函数参数的形式返回。 spawn - child_process.spawn 使用指定的命令行参数创建新进程。 fork - child_process.fork 是 spawn()的特殊形式，用于在子进程中运行的模块，如 fork(‘.&#x2F;son.js’) 相当于 spawn(‘node’, [‘.&#x2F;son.js’]) 。与spawn方法不同的是，fork会在父进程与子进程之间，建立一个通信管道，用于进程之间的通信。 略（用上再看）","categories":[{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"}],"author":"Sn0wma1n"},{"title":"Node.js学习（二）","slug":"Node-js学习（二）","date":"2024-01-31T16:00:00.000Z","updated":"2024-02-03T17:04:20.106Z","comments":true,"path":"2024/02/01/Node-js学习（二）/","link":"","permalink":"https://snowman12137.github.io/2024/02/01/Node-js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"Node.js学习（二）1.文件系统Node.js 提供一组类似 UNIX（POSIX）标准的文件操作API。 Node 导入文件系统模块(fs)语法如下所示： 1var fs = require(&quot;fs&quot;) a.异步和同步Node.js 文件系统（fs 模块）模块中的方法均有异步和同步版本，例如读取文件内容的函数有异步的 fs.readFile() 和同步的 fs.readFileSync()。 异步的方法函数最后一个参数为回调函数，回调函数的第一个参数包含了错误信息(error)。 建议大家使用异步方法，比起同步，异步方法性能更高，速度更快，而且没有阻塞。 input.txt文件内容如下: 12菜鸟教程官网地址：www.runoob.com文件读取实例 file.js 123456789101112var fs = require(&quot;fs&quot;);// 异步读取fs.readFile(&#x27;input.txt&#x27;, function (err, data) &#123; if (err) &#123; return console.error(err); &#125; console.log(&quot;异步读取: &quot; + data.toString());&#125;);// 同步读取var data = fs.readFileSync(&#x27;input.txt&#x27;);console.log(&quot;同步读取: &quot; + data.toString());console.log(&quot;程序执行完毕。&quot;); 1234567$ node file.js 同步读取: 菜鸟教程官网地址：www.runoob.com文件读取实例程序执行完毕。异步读取: 菜鸟教程官网地址：www.runoob.com文件读取实例 b.打开文件以下为在异步模式下打开文件的语法格式： 1fs.open(path, flags[, mode], callback) 参数使用说明如下： path - 文件的路径。 flags - 文件打开的行为。具体值详见下文。 mode - 设置文件模式(权限)，文件创建默认权限为 0666(可读，可写)。 callback - 回调函数，带有两个参数如：callback(err, fd)。 12345678910var fs = require(&quot;fs&quot;);// 异步打开文件console.log(&quot;准备打开文件！&quot;);fs.open(&#x27;input.txt&#x27;, &#x27;r+&#x27;, function(err, fd) &#123; if (err) &#123; return console.error(err); &#125; console.log(&quot;文件打开成功！&quot;); &#125;); 2.GET&#x2F;POST请求a.获取GET请求内容由于GET请求直接被嵌入在路径中，URL是完整的请求路径，包括了?后面的部分，因此你可以手动解析后面的内容作为GET请求的参数。 node.js 中 url 模块中的 parse 函数提供了这个功能。 12345678var http = require(&#x27;http&#x27;);var url = require(&#x27;url&#x27;);var util = require(&#x27;util&#x27;); http.createServer(function(req, res)&#123; res.writeHead(200, &#123;&#x27;Content-Type&#x27;: &#x27;text/plain; charset=utf-8&#x27;&#125;); res.end(util.inspect(url.parse(req.url, true)));&#125;).listen(3000); 浏览器中访问http://localhost:3000/user?name=Sn0wm1an&amp;url=xiaowublog.top b.获取URL参数我们可以使用 url.parse 方法来解析 URL 中的参数，代码如下： 1234567891011121314var http = require(&#x27;http&#x27;);var url = require(&#x27;url&#x27;);var util = require(&#x27;util&#x27;); http.createServer(function(req, res)&#123; res.setHeader(&#x27;Content-Type&#x27;,&#x27;text/html; charset=utf-8&#x27;); // 解析 url 参数 var params = url.parse(req.url, true).query; res.write(&quot;网站名：&quot; + params.name); res.write(&quot;\\n&quot;); res.write(&quot;网站 URL：&quot; + params.url); res.end(); &#125;).listen(3000); c.获取 POST 请求内容POST 请求的内容全部的都在请求体中，http.ServerRequest 并没有一个属性内容为请求体，原因是等待请求体传输可能是一件耗时的工作。 比如上传文件，而很多时候我们可能并不需要理会请求体的内容，恶意的POST请求会大大消耗服务器的资源，所以 node.js 默认是不会解析请求体的，当你需要的时候，需要手动来做。 12345678910111213141516var http = require(&#x27;http&#x27;);var querystring = require(&#x27;querystring&#x27;);var util = require(&#x27;util&#x27;);http.createServer(function(req, res)&#123; // 定义了一个post变量，用于暂存请求体的信息 var post = &#x27;&#x27;; // 通过req的data事件监听函数，每当接受到请求体的数据，就累加到post变量中 req.on(&#x27;data&#x27;, function(chunk)&#123; post += chunk; &#125;); // 在end事件触发后，通过querystring.parse将post解析为真正的POST请求格式，然后向客户端返回。 req.on(&#x27;end&#x27;, function()&#123; post = querystring.parse(post); res.end(util.inspect(post)); &#125;);&#125;).listen(3000); 以下实例表单通过 POST 提交并输出数据： 12345678910111213141516171819202122232425262728293031323334var http = require(&#x27;http&#x27;);var querystring = require(&#x27;querystring&#x27;); var postHTML = &#x27;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程 Node.js 实例&lt;/title&gt;&lt;/head&gt;&#x27; + &#x27;&lt;body&gt;&#x27; + &#x27;&lt;form method=&quot;post&quot;&gt;&#x27; + &#x27;网站名： &lt;input name=&quot;name&quot;&gt;&lt;br&gt;&#x27; + &#x27;网站 URL： &lt;input name=&quot;url&quot;&gt;&lt;br&gt;&#x27; + &#x27;&lt;input type=&quot;submit&quot;&gt;&#x27; + &#x27;&lt;/form&gt;&#x27; + &#x27;&lt;/body&gt;&lt;/html&gt;&#x27;; http.createServer(function (req, res) &#123; var body = &quot;&quot;; req.on(&#x27;data&#x27;, function (chunk) &#123; body += chunk; &#125;); req.on(&#x27;end&#x27;, function () &#123; // 解析参数 body = querystring.parse(body); // 设置响应头部信息及编码 res.writeHead(200, &#123;&#x27;Content-Type&#x27;: &#x27;text/html; charset=utf8&#x27;&#125;); if(body.name &amp;&amp; body.url) &#123; // 输出提交的数据 res.write(&quot;网站名：&quot; + body.name); res.write(&quot;&lt;br&gt;&quot;); res.write(&quot;网站 URL：&quot; + body.url); &#125; else &#123; // 输出表单 res.write(postHTML); &#125; res.end(); &#125;);&#125;).listen(3000);","categories":[{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"}],"author":"Sn0wma1n"},{"title":"Node.js学习（一）","slug":"Node-js学习（一）","date":"2024-01-29T16:00:00.000Z","updated":"2024-02-03T17:04:03.796Z","comments":true,"path":"2024/01/30/Node-js学习（一）/","link":"","permalink":"https://snowman12137.github.io/2024/01/30/Node-js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"Node.js学习（一）1.Node.js的异步在了解Node.js异步之前，我们先看一看其前身JavaScript的异步编程技巧。 一个异步过程的执行将不再与原有的序列有顺序关系。简单来理解就是：同步按你的代码顺序执行，异步不按照代码顺序执行，异步的执行效率更高。 什么时候用异步编程 在前端编程中（甚至后端有时也是这样），我们在处理一些简短、快速的操作时，例如计算 1 + 1 的结果，往往在主线程中就可以完成。主线程作为一个线程，不能够同时接受多方面的请求。所以，当一个事件没有结束时，界面将无法处理其他请求。 现在有一个按钮，如果我们设置它的 onclick 事件为一个死循环，那么当这个按钮按下，整个网页将失去响应。 为了避免这种情况的发生，我们常常用子线程来完成一些可能消耗时间足够长以至于被用户察觉的事情，比如读取一个大文件或者发出一个网络请求。因为子线程独立于主线程，所以即使出现阻塞也不会影响主线程的运行。但是子线程有一个局限：一旦发射了以后就会与主线程失去同步，我们无法确定它的结束，如果结束之后需要处理一些事情，比如处理来自服务器的信息，我们是无法将它合并到主线程中去的。 为了解决这个问题，JavaScript 中的异步操作函数往往通过回调函数来实现异步任务的结果处理。 a.JavaSript回调函数回调函数就是一个函数，它是在我们启动一个异步任务的时候就告诉它：等你完成了这个任务之后要干什么。这样一来主线程几乎不用关心异步任务的状态了，他自己会善始善终。 1234function print() &#123; document.getElementById(&quot;demo&quot;).innerHTML=&quot;RUNOOB!&quot;;&#125;setTimeout(print, 3000); 这段程序中的 setTimeout 就是一个消耗时间较长（3 秒）的过程，它的第一个参数是个回调函数，第二个参数是毫秒数，这个函数执行之后会产生一个子线程，子线程会等待 3 秒，然后执行回调函数 “print”，在命令行输出 “RUNOOB!”。 而且我们可以不声明函数，用匿名函数放到方法里。 123setTimeout(function () &#123; document.getElementById(&quot;demo&quot;).innerHTML=&quot;RUNOOB!&quot;;&#125;, 3000); b.Nodejs回调函数创建阻塞代码main.js 123456var fs = require(&quot;fs&quot;);var data = fs.readFileSync(&#x27;input.txt&#x27;);console.log(data.toString());console.log(&quot;程序执行结束!&quot;); 非阻塞式代码是将方法中最后一位插入回调函数，形式如下 12345678var fs = require(&quot;fs&quot;);fs.readFile(&#x27;input.txt&#x27;, function (err, data) &#123; if (err) return console.error(err); console.log(data.toString());&#125;);console.log(&quot;程序执行结束!&quot;); 2.事件循环a.事件驱动程序，NodeJS使用的是事件驱动模型，当web server接收到请求，就把它关闭然后进行处理，然后去服务下一个web请求。 当这个请求完成，它被放回处理队列，当到达队列开头，这个结果被返回给用户。 这个模型非常高效可扩展性非常强，因为 webserver 一直接受请求而不等待任何读写操作。（这也称之为非阻塞式IO或者事件驱动IO） (图来源菜鸟) 整个事件驱动的流程就是这么实现的，非常简洁。有点类似于观察者模式，事件相当于一个主题(Subject)，而所有注册到这个事件上的处理函数相当于观察者(Observer)。 123456789// 引入 events 模块var events = require(&#x27;events&#x27;);// 创建 eventEmitter 对象var eventEmitter = new events.EventEmitter();// 绑定事件及事件的处理程序//eventHandler是绑定的函数eventEmitter.on(&#x27;eventName&#x27;, eventHandler);// 触发事件eventEmitter.emit(&#x27;eventName&#x27;); 例子 12345678910111213141516171819202122232425// 引入 events 模块var events = require(&#x27;events&#x27;);// 创建 eventEmitter 对象var eventEmitter = new events.EventEmitter(); // 创建事件处理程序var connectHandler = function connected() &#123; console.log(&#x27;连接成功。&#x27;); // 触发 data_received 事件 eventEmitter.emit(&#x27;data_received&#x27;);&#125; // 绑定 connection 事件处理程序eventEmitter.on(&#x27;connection&#x27;, connectHandler); // 使用匿名函数绑定 data_received 事件eventEmitter.on(&#x27;data_received&#x27;, function()&#123; console.log(&#x27;数据接收成功。&#x27;);&#125;); // 触发 connection 事件 eventEmitter.emit(&#x27;connection&#x27;); console.log(&quot;程序执行完毕。&quot;); b.EventEmitter类Node.js 所有的异步 I&#x2F;O 操作在完成时都会发送一个事件到事件队列。 Node.js 里面的许多对象都会分发事件：一个 net.Server 对象会在每次有新连接时触发一个事件， 一个 fs.readStream 对象会在文件被打开的时候触发一个事件。 所有这些产生事件的对象都是 events.EventEmitter 的实例。 EventEmitter 对象如果在实例化时发生错误，会触发 error 事件。当添加新的监听器时，newListener 事件会触发，当监听器被移除时，removeListener 事件被触发。 下面我们用一个简单的例子说明 EventEmitter 的用法： 123456789//event.js 文件var EventEmitter = require(&#x27;events&#x27;).EventEmitter; var event = new EventEmitter(); event.on(&#x27;some_event&#x27;, function() &#123; //绑定触发语句 console.log(&#x27;some_event 事件触发&#x27;); &#125;); setTimeout(function() &#123; event.emit(&#x27;some_event&#x27;); //触发&#125;, 1000); 可以绑定多个触发语句 12345678910//event.js 文件var events = require(&#x27;events&#x27;); var emitter = new events.EventEmitter(); emitter.on(&#x27;someEvent&#x27;, function(arg1, arg2) &#123; console.log(&#x27;listener1&#x27;, arg1, arg2); &#125;); emitter.on(&#x27;someEvent&#x27;, function(arg1, arg2) &#123; console.log(&#x27;listener2&#x27;, arg1, arg2); &#125;); emitter.emit(&#x27;someEvent&#x27;, &#x27;arg1 参数&#x27;, &#x27;arg2 参数&#x27;); 3.Buffer与Streama.BufferJavaScript 语言自身只有字符串数据类型，没有二进制数据类型。 但在处理像TCP流或文件流时，必须使用到二进制数据。因此在 Node.js中，定义了一个 Buffer 类，该类用来创建一个专门存放二进制数据的缓存区。 字符编码 Buffer 实例一般用于表示编码字符的序列，比如 UTF-8 、 UCS2 、 Base64 、或十六进制编码的数据。 通过使用显式的字符编码，就可以在 Buffer 实例与普通的 JavaScript 字符串之间进行相互转换。 12345const buf = Buffer.from(&#x27;runoob&#x27;, &#x27;ascii&#x27;);// 输出 72756e6f6f62console.log(buf.toString(&#x27;hex&#x27;));// 输出 cnVub29iconsole.log(buf.toString(&#x27;base64&#x27;)); 读写数据 12buf.write(string[, offset[, length]][, encoding])buf.toString([encoding[, start[, end]]]) 转换成JSON对象 当字符串化一个 Buffer 实例时，JSON.stringify()会隐式地调用该 **toJSON()**。 返回JSON对象 1buf.toJSON() 例子 1234567891011121314const buf = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5]);const json = JSON.stringify(buf);// 输出: &#123;&quot;type&quot;:&quot;Buffer&quot;,&quot;data&quot;:[1,2,3,4,5]&#125;console.log(json);const copy = JSON.parse(json, (key, value) =&gt; &#123; return value &amp;&amp; value.type === &#x27;Buffer&#x27; ? Buffer.from(value.data) : value;&#125;);// 输出: &lt;Buffer 01 02 03 04 05&gt;console.log(copy); 输出为 12&#123;&quot;type&quot;:&quot;Buffer&quot;,&quot;data&quot;:[1,2,3,4,5]&#125;&lt;Buffer 01 02 03 04 05&gt; b.Stream读入流 1234567891011121314151617var fs = require(&quot;fs&quot;);var data = &#x27;&#x27;;// 创建可读流var readerStream = fs.createReadStream(&#x27;input.txt&#x27;);// 设置编码为 utf8。readerStream.setEncoding(&#x27;UTF8&#x27;);// 处理流事件 --&gt; data, end, and errorreaderStream.on(&#x27;data&#x27;, function(chunk) &#123; data += chunk;&#125;);readerStream.on(&#x27;end&#x27;,function()&#123; console.log(data);&#125;);readerStream.on(&#x27;error&#x27;, function(err)&#123; console.log(err.stack);&#125;);console.log(&quot;程序执行完毕&quot;); 写入流 12345678910111213141516var fs = require(&quot;fs&quot;);var data = &#x27;菜鸟教程官网地址：www.runoob.com&#x27;;// 创建一个可以写入的流，写入到文件 output.txt 中var writerStream = fs.createWriteStream(&#x27;output.txt&#x27;);// 使用 utf8 编码写入数据writerStream.write(data,&#x27;UTF8&#x27;);// 标记文件末尾writerStream.end();// 处理流事件 --&gt; finish、errorwriterStream.on(&#x27;finish&#x27;, function() &#123; console.log(&quot;写入完成。&quot;);&#125;);writerStream.on(&#x27;error&#x27;, function(err)&#123; console.log(err.stack);&#125;);console.log(&quot;程序执行完毕&quot;); 管道流 123456789var fs = require(&quot;fs&quot;);// 创建一个可读流var readerStream = fs.createReadStream(&#x27;input.txt&#x27;);// 创建一个可写流var writerStream = fs.createWriteStream(&#x27;output.txt&#x27;);// 管道读写操作// 读取 input.txt 文件内容，并将内容写入到 output.txt 文件中readerStream.pipe(writerStream);console.log(&quot;程序执行完毕&quot;); 链式流 链式是通过连接输出流到另外一个流并创建多个流操作链的机制。链式流一般用于管道操作。 接下来我们就是用管道和链式来压缩和解压文件。 1234567var fs = require(&quot;fs&quot;);var zlib = require(&#x27;zlib&#x27;);// 压缩 input.txt 文件为 input.txt.gzfs.createReadStream(&#x27;input.txt&#x27;) .pipe(zlib.createGzip()) .pipe(fs.createWriteStream(&#x27;input.txt.gz&#x27;));console.log(&quot;文件压缩完成。&quot;); 4.模块系统为了让Node.js的文件可以相互调用，Node.js提供了一个简单的模块系统。 模块是Node.js 应用程序的基本组成部分，文件和模块是一一对应的。换言之，一个 Node.js 文件就是一个模块，这个文件可能是JavaScript 代码、JSON 或者编译过的C&#x2F;C++ 扩展。 引入模块 在 Node.js 中，引入一个模块非常简单，如下我们创建一个 main.js 文件并引入 hello 模块，代码如下: 12var hello = require(&#x27;./hello&#x27;);hello.world(); 以上实例中，代码 require(‘.&#x2F;hello’) 引入了当前目录下的 hello.js 文件（.&#x2F; 为当前目录，node.js 默认后缀为 js）。 Node.js 提供了 exports 和 require 两个对象，其中 exports 是模块公开的接口，require 用于从外部获取一个模块的接口，即所获取模块的 exports 对象。 接下来我们就来创建 hello.js 文件，代码如下： 123exports.world = function() &#123; console.log(&#x27;Hello World&#x27;);&#125; 在以上示例中，hello.js 通过 exports 对象把 world 作为模块的访问接口，在 main.js 中通过 require(‘.&#x2F;hello’) 加载这个模块，然后就可以直接访 问 hello.js 中 exports 对象的成员函数了。 有时候我们只是想把一个对象封装到模块中，格式如下： 123module.exports = function() &#123; // ...&#125; 例如 1234567891011//hello.js function Hello() &#123; var name; this.setName = function(thyName) &#123; name = thyName; &#125;; this.sayHello = function() &#123; console.log(&#x27;Hello &#x27; + name); &#125;; &#125;; module.exports = Hello; 这样就可以直接获得这个对象了： 12345//main.js var Hello = require(&#x27;./hello&#x27;); hello = new Hello(); hello.setName(&#x27;BYVoid&#x27;); hello.sayHello(); 5.函数函数可以作为值进行传递如下 1234567function say(word) &#123; console.log(word);&#125;function execute(someFunction, value) &#123; someFunction(value);&#125;execute(say, &quot;Hello&quot;); 或者使用匿名函数，直接在另一个函数的括号中定义和传递这个函数： 1234function execute(someFunction, value) &#123; someFunction(value);&#125;execute(function(word)&#123; console.log(word) &#125;, &quot;Hello&quot;); 6.NodeJS路由我们要为路由提供请求的 URL 和其他需要的 GET 及 POST 参数，随后路由需要根据这些数据来执行相应的代码。 因此，我们需要查看 HTTP 请求，从中提取出请求的 URL 以及 GET&#x2F;POST 参数。这一功能应当属于路由还是服务器（甚至作为一个模块自身的功能）确实值得探讨，但这里暂定其为我们的HTTP服务器的功能。 我们需要的所有数据都会包含在 request 对象中，该对象作为 onRequest() 回调函数的第一个参数传递。但是为了解析这些数据，我们需要额外的 Node.JS 模块，它们分别是 url 和 querystring 模块。 12345678910111213url.parse(string).query | url.parse(string).pathname | | | | | ------ -------------------http://localhost:8888/start?foo=bar&amp;hello=world --- ----- | | | | querystring.parse(queryString)[&quot;foo&quot;] | | querystring.parse(queryString)[&quot;hello&quot;] (菜鸟这个图做的太棒了) 现在我们来给 onRequest() 函数加上一些逻辑，用来找出浏览器请求的 URL 路径： server.js代码 1234567891011121314151617181920var http = require(&quot;http&quot;);var url = require(&quot;url&quot;); function start(route) &#123; function onRequest(request, response) &#123; var pathname = url.parse(request.url).pathname; console.log(&quot;Request for &quot; + pathname + &quot; received.&quot;); route(pathname); response.writeHead(200, &#123;&quot;Content-Type&quot;: &quot;text/plain&quot;&#125;); response.write(&quot;Hello World&quot;); response.end(); &#125; http.createServer(onRequest).listen(8888); console.log(&quot;Server has started.&quot;);&#125; exports.start = start; router.js代码 12345function route(pathname) &#123; console.log(&quot;About to route a request for &quot; + pathname);&#125; exports.route = route; index.js代码 123var server = require(&quot;./server&quot;);var router = require(&quot;./router&quot;);server.start(router.route); 7.全局对象JavaScript 中有一个特殊的对象，称为全局对象（Global Object），它及其所有属性都可以在程序的任何地方访问，即全局变量。 在浏览器 JavaScript 中，通常 window 是全局对象， 而 Node.js 中的全局对象是 global，所有全局变量（除了 global 本身以外）都是 global 对象的属性。 在 Node.js 我们可以直接访问到 global 的属性，而不需要在应用中包含它。 略","categories":[{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"}],"author":"Sn0wma1n"},{"title":"metaGEM使用小记(解决各种问题)2024.1(二)","slug":"metaGEM使用小记-解决各种问题-2024-2（二）-daf9e5675b4d49cc855dd652b38a99e5","date":"2024-01-28T16:00:00.000Z","updated":"2024-02-03T17:01:32.311Z","comments":true,"path":"2024/01/29/metaGEM使用小记-解决各种问题-2024-2（二）-daf9e5675b4d49cc855dd652b38a99e5/","link":"","permalink":"https://snowman12137.github.io/2024/01/29/metaGEM%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0-%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E9%97%AE%E9%A2%98-2024-2%EF%BC%88%E4%BA%8C%EF%BC%89-daf9e5675b4d49cc855dd652b38a99e5/","excerpt":"","text":"metaGEM使用小记(解决各种问题)2024.2（二）书接上文，貌似配置还有点问题。跑了半天以后迟迟看不到结果，htop一看CPU占用几乎为0，应该是没跑起来，squeue 再一看，状态是PD，意思是等待，正常情况应该是R。于是我有网上找了下原因，说只有空闲条件满足job的运行条件才会运行。 我做了以下操作： 先scancle 掉所有队列里的人物 修改slurm.config 的参数稍大一些 重新启动sudo systemctl restart slurmd &amp; sudo systemctl restart slurmctld 更改bash metaGEM.sh -t fastp -j 2 -c 2 -m 10 -h 2 中m(内存)的大小和j(任务数量)的大小 运行报错 12sbatch: error: Memory specification can not be satisfiedsbatch: error: Batch job submission failed: Requested node configuration is not available 再排查一下scontrol show nodes 我们熟悉的报错又回来了（状态又是drain） 查了好久好久在sudo cat /var/log/slurm-llnl/slurmctld.log 里面找到了错误 就是NodeName那一行的配置Sockets和CoresPerSocket不仅要小于上一章讲的bash cxc.sh 的文件中的值，还要小于日志文件中数字前面的值，如(20&lt;SocketsCoresPerSocket) 那么SocketsCoresPerSocket的积要小于20才可以。 如果日志长这样那么八成就可以启动了 然后我们输入，改变节点的状态成空闲(注意一定要是sudo权限) 1sudo scontrol update NodeName=your_node_name State=idle 1.问题:成功运行程序但是瞬间完成，并无输出结果排查！ 在/var/log/slurm-llnl/slurmctld.log 里面显示 123[2024-01-29T15:37:19.292] sched: Allocate JobId=73 NodeList=your_computer #CPUs=8 Partition=your_computer[2024-01-29T15:37:20.367] _job_complete: JobId=73 WEXITSTATUS 1[2024-01-29T15:37:20.367] _job_complete: JobId=73 done 再排查/var/log/slurm-llnl/slurmd.log 无法创建输出文件 这属于slurm的BUG之一，只能先创建文件然后再创建文件夹 所以我们是能手动mkdir log 然后再运行 2.运行时日志文件中报错&#x2F;usr&#x2F;bin&#x2F;bash: line 2: activate: No such file or directory 猜测是这里面envs/metagem 的工作路径有变化，所以我们改成绝对路径试一下 1set +u;source activate envs/metagem;set -u; 如何更改，提交给slurm处理的脚本是由snakemake生成的，因此我们找到这一句 1source activate &#123;config[envs][metagem]&#125; 发现读取的config是config.ymal文件 应该在config.yaml文件里改这里，改成绝对路径 原来不是这个问题（服了） 当使用source activate env_name时，设置conda路径到环境变量即可 1export PATH=&quot;/home/gc/anaconda3/bin:$PATH&quot; 另外再shell脚本中启用conda环境一定要使用source不能使用conda。 成功运行（泪目）！！！ 成功运行（泪目）！！ 成功运行（泪目）！ 成功运行（泪目 成功运行（泪 成功运行（ 成功运行 成功运 成功 成","categories":[{"name":"MAGs云分析","slug":"MAGs云分析","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/"}],"tags":[{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"}],"author":"Sn0wma1n"},{"title":"metaGEM使用小记(解决各种问题)2024.1(一)","slug":"metaGEM使用小记-解决各种问题-2024-1-2a55eec867d5441b9829bd5428024882","date":"2024-01-27T16:00:00.000Z","updated":"2024-02-03T17:01:19.289Z","comments":true,"path":"2024/01/28/metaGEM使用小记-解决各种问题-2024-1-2a55eec867d5441b9829bd5428024882/","link":"","permalink":"https://snowman12137.github.io/2024/01/28/metaGEM%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0-%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E9%97%AE%E9%A2%98-2024-1-2a55eec867d5441b9829bd5428024882/","excerpt":"","text":"metaGEM使用小记(解决各种问题)2024.1(一)前言，如何把数据从百度云上下载到linux服务器上可以直接通过pip下载：pip install bypy -y 第一次使用时需要随便输入一个命令以激活授权界面，如输入 bypy info 然后打开提示的连接 将复制的内容粘贴到终端后回车，等待即可。 登陆成功后会提示如下信息 登录百度网盘(我的应用数据&#x2F;bypy&#x2F;你的文件名&#x2F;*&#x2F;文件) 有多个文件建议十个放到一个文件夹里，这样下载出错方便排查 我写了一个脚本可以自动安装download.sh 1234567bypy downdir /RAW-NCBI/0/ ./0/bypy downdir /RAW-NCBI/1/ ./1/bypy downdir /RAW-NCBI/2/ ./2/bypy downdir /RAW-NCBI/3/ ./3/bypy downdir /RAW-NCBI/4/ ./4/bypy downdir /RAW-NCBI/5/ ./5/bypy downdir /RAW-NCBI/6/ ./6/ 因为我有74个文件，因此分成了7组进行下载 在你想存储的文件夹里输入 1nohup bash [download.sh](http://download.sh) &gt; temp.txt &amp; (nohup是在Linux中永久运行的命令，&amp;和其他方式均会因为终端退出而中断。&gt;把下载的过程信息存储到temp.txt，&amp;并放在后台运行，这样下载文件的任务就会自动放到后台了) A.安装123git clone https://github.com/franciscozorrilla/metaGEM.gitcd metaGEMbash env_setup.sh 1.找不到env_setup.sh路径env_setup.sh放在了metaGEM&#x2F;workflow&#x2F;scripts&#x2F;env_setup.sh 但是env_setup.sh里面所有的文件路径是在metaGEM&#x2F;workflow&#x2F;下面的，所以要把env_setup.sh复制到metaGEM&#x2F;workflow&#x2F;下面运行 详解env_setup.sh文件(提取其中关键部分)(已安装anaconda) 123456789conda create -n mamba mamba -c conda-forge #创建新环境下载mabaconda activate mamba #进入mamba环境mamba env create --prefix ./envs/metagem -f envs/metaGEM_env.yml &amp;&amp; source activate envs/metagem &amp;&amp; pip install --user memote carveme smetana #创建metaGEM环境mamba env create --prefix ./envs/metawrap -f envs/metaWRAP_env.yml #创建metaWRAP环境mamba env create --prefix ./envs/prokkaroary -f envs/prokkaroary_env.yml #创建prokkaroary环境#不重要:download-db.sh &amp;&amp; source deactivate &amp;&amp; source activate mamba#下载GTDB-tk database (~25 Gb)数据集(丢失download-db.sh文件无法下载)wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz #下载CheckM(275MB) 数据集 因为conda install 下载速度很慢，所以脚本里使用了mamba方式进行下载。使用方法为下载mamba包，然后在此环境下进行下载，如 mamba install requests 上述文件中mamba env create为创建新环境的语句，-f后面的.yml文件为导出的conda标准环境文件，–prefix 为新创建环境的路径 2.在安装metagem时以下界面卡住 在正常加载时，以下界面至少卡住了20h以上，因此排除网络问题 我们看一下对应的metaGEM_env.yml文件内容 该文件为conda标准创建环境的文件格式 123456789101112131415161718192021name: metagemchannels: - conda-forge - bioconda - defaultsdependencies: - bedtools&gt;=2.29.2 - bwa&gt;=0.7.17 - concoct&gt;=1.1.0 - diamond&gt;=2.0.6 - fastp&gt;=0.20.1 - gtdbtk&gt;=1.4.0 - maxbin2&gt;=2.2.7 - megahit&gt;=1.2.9 - metabat2&gt;=2.15 - r-base&gt;=3.5.1 - r-gridextra&gt;=2.2.1 - r-tidyverse - r-tidytext - samtools&gt;=1.9 - snakemake&gt;=5.10.0,&lt;5.31.1 name指的是创建环境的名称，channels指的是下载通道，其中这个conda-forge是比较重要的一个，其是一个用于托管和发布科学计算、数据分析和机器学习的Python 包的社区项目。在conda-forge通道中，您可以找到为conda构建但尚未成为官方Anaconda发行版一部分的包。有一些Python库不能用简单的conda install安装，因为除非应用conda-forge，否则它们的通道是不可用的。根据我的经验，pip比conda更适合研究不同的通道源。例如，如果你想安装python-constraint，你可以通过pip install来安装，但是要通过cond 来安装。您必须指定通道- conda-forge。 我在网上看到有人说用forge走的是外网，因此很慢导致加载卡住，因此，我把channels全部注释掉(默认没有channels项)然后测试，依然失败。 注：如果安装出现了Solving environment: failed with initial frozen solve. Retrying with flexible solve 错误 解决博客 12conda update -n base condaconda update --all 即可解决 在后面的测试中，我发现metawrap和prokkaroary的安装 于是我们把后面的大于等于去掉在进行测试，等了一个小时也没有结果也失败了。 于是我们用土办法，创建环境然后一个一个手动输入加载 12345conda create -p ./envs/metagem python=3.10conda info --envconda activate /home/gc/metaGEM-master/workflow/envs/metagem conda install -c conda-forge bedtools bwa concoct diamond fastp gtdbtk maxbin2 megahit metabat2 r-base r-gridextra r-tidyverse r-tidytext samtools snakemake=5.10.0 -ypip install --user memote carveme smetana 或者以下脚本 因为我每次下载都会报错JASON错误，网上一查是因为缓存的问题，因此我在每一个软件包安装之后会清理缓存，暂时我对这个问题没有很好的解决方法，如果没有这个问题的同学可以删除所有conda clean -i -y 语句，当然保留也没有任何问题。其次bioconda:: 这里面的库都是用bioconda 通道下载的，因此安装时要加上这个语句，不加的话很多包安装不上去。 123456789101112131415161718192021222324252627282930313233conda create -p ./envs/metagem python=3.10 -yconda activate ./envs/metagemconda clean -i -yconda install bioconda::bedtools -yconda clean -i -yconda install bioconda::bwa -yconda clean -i -yconda install bioconda::concoct -yconda clean -i -yconda install bioconda::diamond -yconda clean -i -yconda install bioconda::fastp -yconda clean -i -yconda install bioconda::gtdbtk -yconda clean -i -yconda install bioconda::maxbin2 -yconda clean -i -yconda install bioconda::megahit -yconda clean -i -yconda install bioconda::metabat2 -yconda clean -i -yconda install bioconda::r-base -yconda clean -i -yconda install bioconda::r-gridextra -yconda clean -i -yconda install bioconda::r-tidyverse -yconda clean -i -yconda install bioconda::r-tidytext -yconda clean -i -yconda install bioconda::samtools -yconda clean -i -yconda install bioconda::snakemake=5.10.0 -ypip install --user memote carveme smetana B.执行metaGEM.sh在dataset文件夹中的子目录中存放paierd-end的fastq数据，如下所示。MetGEM 将基于dataset文件夹中存在的子文件夹读取示例 ID，并将这些 ID 提供给 Snakefile 作为作业提交的通配符。 我的运行文件树如下图所示 12345678910111213141516171819202122232425262728├── colab│ └── assemblies├── config.yaml├── dataset│ ├── L1EFG190305--AM43│ ├── L1EFG190306--AM51│ ├── L1EFG190309_L1EFG190309--AM61│ ├── L1EFG190324--AW1│ ├── L1EFG190325--AW2│ ├── L1EFG190326--AW3│ └── L1EFG190327--AW4├── envs│ ├── metagem│ ├── metaGEM_env_long.yml│ ├── metaGEM_env.yml│ ├── metaWRAP_env.yml│ ├── prokkaroary│ └── prokkaroary_env.yml├── env_setup.sh├── metaGEM.sh├── rules│ ├── kallisto2concoctTable.smk│ ├── maxbin_single.smk│ ├── metabat_single.smk│ ├── Snakefile_experimental.smk.py│ └── Snakefile_single_end.smk.py├── scripts/└── Snakefile 使用fastp质量过滤reads 每个样本提交一个质量过滤工作，每个过滤工作有2个CPU和20GB 内存，最大运行时间为2小时 1bash metaGEM.sh -t fastp -j 2 -c 2 -m 20 -h 2 1.报错找不到路径 更改config/config.yaml 文件第一行的执行路径即可 如果还是找不到路径则在Snakefile中第一行更改config的路径 如果还是读取不到，则把config.yaml 文件复制到当前文件夹下 2.报错Error parsing number of cores (–cores, -c, -j): must be integer, empty, or ‘all’.第一种情况snakemake版本过高，降低到5.10.0即可解决 第二种情况未执行pip install –user memote carveme smetana 3.报错找不到分析的数据在snakefile文件中，我们可以看到第16&#x2F;17行表示在dataset文件夹下，每一个文件名下面有两个同名加_R1,_R2的文件夹，因此我们要现将文件进行标准化操作， 以下是我们的原始文件数据 我们要把他们进行分类，并且进行改名操作，因此我写了一个自动化分类脚本 123456789101112131415161718192021dataset_path=&quot;/home/gc/bash_all/0&quot; #标准数据的文件夹path_name=&quot;z&quot;tar_path=&quot;/home/gc/metaGEM-master/workflow/dataset&quot; #目标dataset文件夹temp=0for file in `ls $&#123;dataset_path&#125;/`do echo $&#123;file&#125; if (($temp==0)) then mid_temp=$file temp=1 else #命令执行处 #echo $&#123;mid_temp:0:-16&#125; #!!!注意这里，因为我的文件里面后16个字符是固定的，需要替换成如下格式，因此我们把字符串截到16$&#123;mid_temp:0:-16&#125;，如果你的文件后缀不一样，请按照需要更改所有&#x27;16&#x27;的地方 mkdir $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125; mv $&#123;dataset_path&#125;/$&#123;mid_temp:0:-16&#125;.R1.raw.fastq.gz $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125;/$&#123;mid_temp:0:-16&#125;_R1.fastq.gz mv $&#123;dataset_path&#125;/$&#123;mid_temp:0:-16&#125;.R2.raw.fastq.gz $&#123;tar_path&#125;/$&#123;mid_temp:0:-16&#125;/$&#123;mid_temp:0:-16&#125;_R2.fastq.gz temp=0 fidone 4.提交任务后，nohup.out提示sbatch: error: s_p_parse_file: unable to status file &#x2F;etc&#x2F;slurm-llnl&#x2F;slurm.conf: No such file or directory, retrying in 1sec up to 60sec(有的同学会提示&#x2F;bin&#x2F;sh: sbatch: command not found 之类的，这是没有安装slurm导致的看到有一篇文章写到需要这样安装https://www.thegeekdiary.com/sbatch-command-not-found/) Distribution Command Debian apt-get install slurm-client Ubuntu apt-get install slurm-client Kali Linux apt-get install slurm-client Fedora dnf install slurm OS X brew install slurm Raspbian apt-get install slurm-client 配置slurm有问题，slurm是一个linux服务器中的集群管理和作业调度系统，是项目里很关键的一点，因此要好好学习这里的配置信息 先看看文件 1ls -l /etc/slurm/ 报错没有此文件，表明还没有安装slurm 弯路(后面还有未解决的报错) ———————————————分割线以内的不要使用！！————————————————— (失败的教程) https://blog.csdn.net/r1141207831/article/details/125272108 先从https://www.schedmd.com/这里下载，选择对应的版本 12345678910#编译安装前需安装gccyum -y install gcc#接着解压安装tar -jxvf slurm-16.05.11.tar.bz2cd /root/slurm-16.05.11./configuremakemake install#安装成功！ 在make和make install时出现 Ld 返回的1退出状态错误是以前错误的结果。有一个更早的错误ーー对‘hdf5各种方法的未定义引用造成的，因此我猜测是linux版本与slurm版本不同造成的，因此我找了一个适用于Ubuntu20.04 的slurm安装教程 最全slurm安装包列表如下https://src.fedoraproject.org/lookaside/extras/slurm/ a、安装必要文件 12sudo suapt-get install make hwloc libhwloc-dev libmunge-dev libmunge2 munge mariadb-server libmysqlclient-dev -y b、启动启动munge服务 123systemctl enable munge // 设置munge开机自启动systemctl start munge // 启动munge服务systemctl status munge // 查看munge状态 c、编译安装slurm 1234567# 将slurm-21.08.6.tar.bz2源码包放置在/home/fz/package目录下cd /home/fz/packagetar -jxvf slurm-21.08.6.tar.bz2cd slurm-21.08.6/./configure --prefix=/opt/slurm/21.08.6 --sysconfdir=/opt/slurm/21.08.6/etcmake -j16make install 在make时发现缺少hdf5包 我又尝试使用spack高效的包管理器安装hdf5 https://hpc.pku.edu.cn/_book&#x2F;guide&#x2F;soft_env&#x2F;spack.html（教程） 但是报错如下，只能进行手动下载编译 官网下载hdf5 https://support.hdfgroup.org/ftp/HDF5/releases/ 12345678910111213141516171819sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压cd hdf5-1.8.21/ #sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压#依次执行sudo ./configure --prefix=/usr/local/hdf5sudo make #会有很多五颜六色的警告，忽略掉，sudo make check sudo make install#安装成功后，在安装目录/usr/local下出现hdf5文件夹，打开后再切换到该目录下cd /usr/local/hdf5/share/hdf5_examples/csudo ./run-c-ex.sh#执行命令sudo h5cc -o h5_extend h5_extend #如果显示错误，则安装：sudo apt install hdf5-helperssudo apt-get install libhdf5-serial-dev#再执行 sudo h5cc -o h5_extend h5_extend.c #直到执行后没有错误显示#执行命令 sudo ./h5_extend 12345678910111213141516171819sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压cd hdf5-1.8.21/ #sudo tar -xvf hdf5-1.8.21.tar.gz #执行解压#依次执行sudo ./configure --prefix=/usr/local/hdf5sudo make #会有很多五颜六色的警告，忽略掉，sudo make check sudo make install#安装成功后，在安装目录/usr/local下出现hdf5文件夹，打开后再切换到该目录下cd /usr/local/hdf5/share/hdf5_examples/csudo ./run-c-ex.sh#执行命令sudo h5cc -o h5_extend h5_extend #如果显示错误，则安装：sudo apt install hdf5-helperssudo apt-get install libhdf5-serial-dev#再执行 sudo h5cc -o h5_extend h5_extend.c #直到执行后没有错误显示#执行命令 sudo ./h5_extend 装完hdf5后继续make slurm 没有报错！！！完成安装 完成安装，下面进行配置 d、启动数据库 后面是无限的hostname报错，我又重新找了一个教程，这个环境被污染了，如果使用另一个教程装slurm会冲突，但是这个又卸载不干净，于是重做了系统，在另一个教程上面成功了 ———————————————分割线以内的不要使用！！————————————————— 正确的教程如下来自https://wxyhgk.com/article%2Fubuntu-slurm(这个文章讲的太好了) 安装与配置1234#安装slurmsudo apt install slurm-wlm slurm-wlm-doc -y#检查是否安装成功slurmd --version 配置slurm 配置文件是放在 /etc/slurm-llnl/ 下面的，使用命令 1sudo vi /etc/slurm-llnl/slurm.conf 填写如下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687ClusterName=coolControlMachine=master#ControlAddr=#BackupController=#BackupAddr=#MailProg=/usr/bin/s-nailSlurmUser=root#SlurmdUser=rootSlurmctldPort=6817SlurmdPort=6818AuthType=auth/munge#JobCredentialPrivateKey=#JobCredentialPublicCertificate=StateSaveLocation=/var/spool/slurmctldSlurmdSpoolDir=/var/spool/slurmdSwitchType=switch/noneMpiDefault=noneSlurmctldPidFile=/var/run/slurmctld.pidSlurmdPidFile=/var/run/slurmd.pidProctrackType=proctrack/pgid#PluginDir=#FirstJobId=ReturnToService=0#MaxJobCount=#PlugStackConfig=#PropagatePrioProcess=#PropagateResourceLimits=#PropagateResourceLimitsExcept=#Prolog=#Epilog=#SrunProlog=#SrunEpilog=#TaskProlog=#TaskEpilog=#TaskPlugin=#TrackWCKey=no#TreeWidth=50#TmpFS=#UsePAM=## TIMERSSlurmctldTimeout=300SlurmdTimeout=300InactiveLimit=0MinJobAge=300KillWait=30Waittime=0## SCHEDULINGSchedulerType=sched/backfill#SchedulerAuth=#SelectType=select/linear#PriorityType=priority/multifactor#PriorityDecayHalfLife=14-0#PriorityUsageResetPeriod=14-0#PriorityWeightFairshare=100000#PriorityWeightAge=1000#PriorityWeightPartition=10000#PriorityWeightJobSize=1000#PriorityMaxAge=1-0## LOGGINGSlurmctldDebug=infoSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.logSlurmdDebug=infoSlurmdLogFile=/var/log/slurm-llnl/slurmd.logJobCompType=jobcomp/none#JobCompLoc=## ACCOUNTING#JobAcctGatherType=jobacct_gather/linux#JobAcctGatherFrequency=30##AccountingStorageType=accounting_storage/slurmdbd#AccountingStorageHost=#AccountingStorageLoc=#AccountingStoragePass=#AccountingStorageUser=## COMPUTE NODESPartitionName=master Nodes=master Default=NO MaxTime=INFINITE State=UP#NodeName=master State=UNKNOWNNodeName=master Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN 上面的代码中的 ControlMachine&#x3D;master PartitionName&#x3D;master Nodes&#x3D;master Default&#x3D;NO MaxTime&#x3D;INFINITE State&#x3D;UP#NodeName&#x3D;master State&#x3D;UNKNOWNNodeName&#x3D;master Sockets&#x3D;2 CoresPerSocket&#x3D;16 ThreadsPerCore&#x3D;1 State&#x3D;UNKNOWN 我 黑体 和 斜体 的地方需要修改，这两部分是是需要修改的，其他的别动。 黑体部分修改 使用 hostname 命令可以查看到你的名字，然后把你的到的名字替换上面的 master 斜体部分修改 这部分稍微有点复杂，首先来解释各个名字的意思 Sockets 代表你服务器cpu的个数 CoresPerSocket 代表每个cpu的核数 ThreadsPerCore 代表是否开启超线程，如果开启了超线程就是2，没有开启就是1 使用vi [cxc.sh](http://cxc.sh/) 写以下脚本 1234567891011121314#!/bin/bashcpunum=`cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l`echo &quot;CPU 个数: $cpunum&quot;;cpuhx=`cat /proc/cpuinfo | grep &quot;cores&quot; | uniq | awk -F&quot;:&quot; &#x27;&#123;print $2&#125;&#x27;`echo &quot;CPU 核心数：$cpuhx&quot; ; cpuxc=`cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l`echo &quot;CPU 线程数：$cpuxc&quot; ;if [[ `expr $cpunum\\*$[cpuhx*2] ` -eq $cpuxc ]];then echo &quot;开启了超线程&quot;else echo &quot;未开启超线程&quot;fi 然后使用命令 bash [cxc.sh](http://cxc.sh/) 运行脚本，看看线程数是不是核心数的两倍，如果是就开启了，没有就没开启。 完成上面的之后吧对应的数字填写上去就可以了。 完成上述所有的设置之后就能启动服务了shell 12sudo systemctl enable slurmctld --nowsudo systemctl enable slurmd --now 查看slurm队列信息 1sinfo 如果这部分是 idle 就说明是可以的,如果不是 idle 请看这个 如果还是解决不了 比如是drain 其意思是用尽资源 解决文章 sinfo -R 报错Low socket***core***thre 那么直接把Sockets=2 CoresPerSocket=16 这两个参数减少，比如说除以2，留出一定的资源给系统使用，问题就解决了 确定目前队列里没有程序时，执行下列语句就好了（NodeName是上面设置的） 1scontrol update NodeName=m1 State=idle 至此就已经安装完成了 到这里配置slurm就已经结束了 5.提交后sbatch: error: Batch job submission failed: No partition specified or system default partition这个错误也是排查了好久，排查到这个文章 1234[username@master1 ~]# sbatch example.sh --partition computeq #Note that ordering matters here!sbatch: error: Batch job submission failed: No partition specified or system default partition[username@master1 ~]# sbatch --partition=computeq example.shSubmitted batch job 114499 猜测是运行顺序错误导致的问题，于是我们到metaGEM.sh 中排查一下，核心的运行语句如下 1echo &quot;nohup snakemake all -j $njobs -k --cluster-config ../config/cluster_config.json -c &#x27;sbatch -A &#123;cluster.account&#125; -t &#123;cluster.time&#125; --mem &#123;cluster.mem&#125; -n &#123;cluster.n&#125; --ntasks &#123;cluster.tasks&#125; --cpus-per-task &#123;cluster.n&#125; --output &#123;cluster.output&#125;&#x27; &amp;&quot;|bash; break;; 可以看到在后面的sbatch里面没有关于–partition的语句，于是我们手动添加，partition后面的名字就是前面我们设置的主机名 1--partition=你的主机名 1sbatch --partition=的主机名 -A &#123;cluster.account&#125; -t &#123;cluster.time&#125; --mem &#123;cluster.mem&#125; -n &#123;cluster.n&#125; --ntasks &#123;cluster.tasks&#125; --cpus-per-task &#123;cluster.n&#125; --output &#123;cluster.output&#125; 代码中搜索sbatch 有三个地方需要添加，添加后即可正常运行 然后我们运行文章开头的语句 -j 任务数量 -c 每个任务CPU数量 -m 每个任务分配的内存大小 -h 每个任务运行的时间 注:注意CPU过大也不行 1bash metaGEM.sh -t fastp -j 5 -c 4 -m 20 -h 20 然后我们输入squeue 查看刚才提交的任务 到这里我们的环境配置完毕","categories":[{"name":"MAGs云分析","slug":"MAGs云分析","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/"}],"tags":[{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"}],"author":"Sn0wma1n"},{"title":"解决Gitee图床不显示问题(使用阿里云OSS搭建图床)","slug":"解决Gitee图床不显示问题","date":"2023-09-02T08:51:50.000Z","updated":"2024-02-03T17:05:10.725Z","comments":true,"path":"2023/09/02/解决Gitee图床不显示问题/","link":"","permalink":"https://snowman12137.github.io/2023/09/02/%E8%A7%A3%E5%86%B3Gitee%E5%9B%BE%E5%BA%8A%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题前段时间搞博客弄了一个Gitee图床，图片可以上传成功，外部链接可以打开图片，但是hexo s后本地都打开不了图片，但是相应的源码标签里依然有图片链接地址。这就让我很疑惑。 报错是class=lazyload error 可以看到下图这里是有图片的，就是不能显示。 在网上并没有找到相关的报错解释，然后我又查找了lazyload是懒加载，但是也没有查明相关的报错解决方法，于是我又怀疑是主题的问题。 在_config.yml里注释掉theme以后发现报错 错误是不能加载，有可能是Gitee的权限问题(是公共库)，导致服务器不能访问库里面的图片。 然后我找到了这样一篇文章： gitee图床不能用了，心态崩了 原来是Gitee从去年开始已经不支持图床了。然后发现阿里云是比较好用的一个图床，毕竟是付费的(40G 1年 9RMB) 搭建阿里云OSS图床+PicGO1.购买阿里云OSS服务 登录阿里云 打开侧边栏选择对象存储OSS 进入管理控制台 点击创建Bucket 有的人在区域一栏没有买过流量包，会提醒购买 读写权限注意是公共读，博客需要读取你上传后的图片 购买资源包 2.添加用户权限我们需要添加一个可以操作OSS的用户，在配置好PicGO后使用这个用户对图片进行自动添加。 点击访问控制 点击用户，新建用户 输入名称，注意勾选OpenAPI调用访问 这里要记下AccessKey ID 和 AccessKeySecret，之后配置PICGO用到，因为这个界面关掉之后就不好找了，所以最好 记在记事本里 设置用户权限 选择管理对象存储OSS服务权限，点击确定，如下图所示： 3.配置PICGO 下载PICGO里面有相应操作系统的安装包文件以及源码，点击下载安装文件即可。 安装完成后，打开图床设置，点击阿里云OSS，得到如下界面 注意设定keyid，就是创建用户的AccessKey ID，KeySecret 就是AccessKeySecret，存储空间名就是创建Bucket的名字，存储区域也是创建时设定的， 忘记的可以通过Bucket概览查看，如下图所示： 存储路径默认设置img&#x2F;即可，如果自己有已经备案的域名，可以填写设定自定义域名，如果没有不填即可。 可以看到PICGO能够文件上传，也支持剪贴板上传。上传过程为： 拖拽文件或点击上传文件或点击剪贴板图片上传。 上传完成后电脑剪贴板里就有了所选链接格式的图片链接。 到相应的地方粘贴即可。 开始可能有疑问，我的图片存到哪里了呢？很简单，点击文件管理，如下图所示： 看到img文件夹了吗？就在里面了，你可以在文件夹里对图片进行删除等操作。 4.配置Typora 打开typora点击偏好设置 然后配置如下选项 1.点击图像 2.选择上传图片 3.上传服务选择PICGO 4.找到PICGO安装的位置，即PicGo.exe的位置 点击验证图片上传选项即可 注意可以1.右键桌面图标 2.点击属性 3.负值路径 Over 下课","categories":[{"name":"其他","slug":"其他","permalink":"https://snowman12137.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"博客问题","slug":"博客问题","permalink":"https://snowman12137.github.io/tags/%E5%8D%9A%E5%AE%A2%E9%97%AE%E9%A2%98/"}],"author":"Sn0wma1n"},{"title":"SGX(Software Guard eXtensions)_linux构建可信执行环境(一)","slug":"SGX-Software-Guard-eXtensions-构建可信执行环境-一","date":"2023-09-02T08:42:16.000Z","updated":"2023-09-06T09:55:34.893Z","comments":true,"path":"2023/09/02/SGX-Software-Guard-eXtensions-构建可信执行环境-一/","link":"","permalink":"https://snowman12137.github.io/2023/09/02/SGX-Software-Guard-eXtensions-%E6%9E%84%E5%BB%BA%E5%8F%AF%E4%BF%A1%E6%89%A7%E8%A1%8C%E7%8E%AF%E5%A2%83-%E4%B8%80/","excerpt":"","text":"SGX(Software Guard eXtensions)_linux构建可信执行环境(一)：如何开发第一个最简单的 SGX 应用 HelloWorld一、了解SGX1.1SGX定义 Intel 软件防护扩展SGX（Software Guard Extension）是一项针对台式机和服务器平台的旨在满足可信计算需求的技术。Intel SGX是Intel架构新的扩展，在原有架构上增加了一组新的指令集和内存访问机制。 Intel在2015年从第6代Intel酷睿处理器平台开始引入了Intel软件防护扩展新指令集，使用特殊指令和软件可将应用程序代码放入一个enclave中执行。Enclave可以提供一个隔离的可信执行环境，可以在BIOS、虚拟机监控器、主操作系统和驱动程序均被恶意代码攻陷的情况下，仍对enclave内的代码和内存数据提供保护，防止恶意软件影响enclave内的代码和数据，从而保障用户的关键代码和数据的机密性和完整性。 1.2基本原理 SGX应用由两部分组成： untrusted 不可信区：代码和数据运行在普通非加密内存区域，程序main入口必须在非可信区。 trusted 可信区：代码和数据运行在硬件加密内存区域，此区域由CPU创建的且只有CPU有权限访问。 当一个安全区域函数被调用时，只有安全区域内的代码才能看到其数据，外部访问总是被拒绝；返回时，安全区数据将保留在受保护的内存中。 非可信区只能通过ECALL函数调用可信区内的函数，可信区只能通过OCALL函数调用非可信区的函数，ECALL函数和OCALL函数通过EDL文件声明。 1.3两大机制1.3.1保护机制针对enclave的保护机制主要包括两个部分：一是enclave内存访问语义的变化，二是应用程序地址映射关系的保护。这两项功能共同完成对enclave的机密性和完整性的保护。 1.3.2 认证机制SGX 提出了两种类型的身份认证方式：一种是平台内部 enclave 间的认证，用来认证进行报告的 enclave 和自己是否运行在同一个平台上；另一种是平台间的远程认证，用于远程的认证者认证 enclave 的身份信息。 本地证明：同一平台的两个Enclave之间的证明过程。 远程证明：Enclave和不在平台上的第三方之间的证明过程。 二、Linux下安装SGX 简介：我们需要安装几个东西。第一是驱动Drive，有了驱动才可以调用intel芯片里面的硬件部分。第二个是PSW(平台软件)，其是允许支持SGX的应用程序在目标平台上执行的软件栈，包含四部分1.提供对硬件功能进行访问的驱动程序；2.为执行和认证提供多个支持库；3.运行所必需的架构型enclave；4.加载并与enclave进行通信的服务；。第三个是软件开发工具包（SDK）为开发人员提供了开发支持SGX的应用程序所需的一切，它由一个生成应用程序和enclave之间的接口函数的工具，一个在使用它之前对enclave进行签名的工具，一个调试它的工具以及一个性能检查的工具组成。另外，它还包含模板和样本参数，用于在Windows下使用Visual Studio开发enclave，或在Linux下使用Makefile。 有了这三样东西我们才可以进行代码开发 以Ubuntu20.04 LTS为例 硬件需求仅当安装sgx驱动和PSW时需要，安装sgx sdk并不需要硬件支持。 硬件不支持的情况下，可以在模拟环境下编写测试SGX程序，其中makefile里SGX_MODE&#x3D;SIM。（虽然 SGX是基于硬件的，但是依旧可以使用软件条件进行模拟） 先安装驱动、PSW、SDK所需依赖 123sudo apt-get updatesudo apt-get install libssl-dev libcurl4-openssl-dev libprotobuf-devsudo apt-get install build-essential python 下载Intel SGX驱动并安装 注意所有版本的驱动、SPW、SDK都在这里https://download.01.org/intel-sgx/sgx-linux/2.21/distro/ 1234wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/sgx_linux_x64_driver_2.6.0_4f5bb63.binchmod +x sgx_linux_x64_driver_2.6.0_4f5bb63.binsudo ./sgx_linux_x64_driver_2.6.0_4f5bb63.binsudo reboot 注意安装完重启才生效 下载Intel SGX PSW并安装 注意：有的版本的没有PSW文件，也可以不装 12wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/libsgx-enclave-common_2.7.101.3-bionic1_amd64.debsudo dpkg -i ./libsgx-enclave-common_2.7.101.3-bionic1_amd64.deb 下载并安装Intel SGX SDK 安装过程中可以手动输入SDK要安装到的目标位置 123wget https://download.01.org/intel-sgx/sgx-linux/2.7.1/distro/ubuntu18.04-server/sgx_linux_x64_sdk_2.7.101.3.binchmod +x ./sgx_linux_x64_sdk_2.7.101.3.bin./sgx_linux_x64_sdk_2.7.101.3.bin 添加环境变量，执行上一步结束时输出的命令 1source /path/to/sgxsdk/environment 接下来进入到sgxsdk&#x2F;SampleCode&#x2F;SampleEnclave文件夹里 1cd /path/to/sgxsdk/SampleCode/SampleEnclave 编辑一下Makefile 12345678# Intel SGX SDK 的安装位置SGX_SDK ?= /home/luoyhang003/SGX/sgxsdk# 运行类型：HW 真实环境；SIM 模拟器环境SGX_MODE ?= SIM# 运行架构：仅支持 64 位SGX_ARCH ?= x64# 是否为：Debug 调试模式SGX_DEBUG ?= 1 退出然后运行示例 1234# 编译sudo make# 运行./app 注意一定要添加环境变量source &#x2F;path&#x2F;to&#x2F;sgxsdk&#x2F;environment，这行命令是临时环境变量，即存活时间为一个终端的市场，一旦重启或开启新的终端环境变量即会失效，需要手动添加。也可以永久化环境变量。 成功运行截图 三、文件结构与代码目录结构 编译&amp;运行 1234567891011121314151617181920212223242526272829303132$ makeGEN =&gt; App/Enclave_u.hCC &lt;= App/Enclave_u.cCXX &lt;= App/App.cppLINK =&gt; appGEN =&gt; Enclave/Enclave_t.hCC &lt;= Enclave/Enclave_t.cCXX &lt;= Enclave/Enclave.cppLINK =&gt; enclave.so&lt;EnclaveConfiguration&gt; &lt;ProdID&gt;0&lt;/ProdID&gt; &lt;ISVSVN&gt;0&lt;/ISVSVN&gt; &lt;StackMaxSize&gt;0x40000&lt;/StackMaxSize&gt; &lt;HeapMaxSize&gt;0x100000&lt;/HeapMaxSize&gt; &lt;TCSNum&gt;10&lt;/TCSNum&gt; &lt;TCSPolicy&gt;1&lt;/TCSPolicy&gt; &lt;!-- Recommend changing &#x27;DisableDebug&#x27; to 1 to make the enclave undebuggable for enclave release --&gt; &lt;DisableDebug&gt;0&lt;/DisableDebug&gt; &lt;MiscSelect&gt;0&lt;/MiscSelect&gt; &lt;MiscMask&gt;0xFFFFFFFF&lt;/MiscMask&gt;&lt;/EnclaveConfiguration&gt;tcs_num 10, tcs_max_num 10, tcs_min_pool 1The required memory is 3960832B.The required memory is 0x3c7000, 3868 KB.Succeed.SIGN =&gt; enclave.signed.soThe project has been built in debug hardware mode.$ ./appHello worldInfo: SampleEnclave successfully returned.Enter a character before exit ... 编译流程(Makefile) 通过 sgx_edger8r 工具在 App/ 目录下生成不可信代码(Enclave_u.c 和 Enclave_u.h)，这部分生成代码主要会调用 ECALL (sgx_ecall)； 编译不可信部分 Binary: app； 通过sgx_edger8r 工具在 Enclave/ 目录下生成可信代码(Enclave_t.c 和 Enclave_t.h)； 编译可信动态链接库(enclave.so)； 通过sgx_sing工具签名可信动态链接库(enclave.signed.so)； 结束。 编译后目录结构 123456789101112131415161718192021222324HelloWorld├── app├── App│ ├── App.cpp│ ├── App.h│ ├── App.o #[generated]│ ├── Enclave_u.c #[generated] │ ├── Enclave_u.h #[generated] │ └── Enclave_u.o #[generated]├── Enclave│ ├── Enclave.config.xml│ ├── Enclave.cpp│ ├── Enclave.edl│ ├── Enclave.h│ ├── Enclave.lds│ ├── Enclave.o #[generated]│ ├── Enclave_private.pem│ ├── Enclave_t.c #[generated]│ ├── Enclave_t.h #[generated]│ └── Enclave_t.o #[generated]├── enclave.signed.so #[generated]├── enclave.so #[generated]├── Include└── Makefile HelloWorld 大概流程如下 1.添加可信函数Encalve&#x2F;Enclave.edl 12345enclave &#123; trusted &#123; public void ecall_hello_from_enclave([out, size=len] char* buf, size_t len); &#125;;&#125;; 2.在可信区域定义可信函数Enclave&#x2F;Enclave.cpp 1234567891011121314151617#include &quot;Enclave.h&quot;#include &quot;Enclave_t.h&quot; /* print_string */#include &lt;string.h&gt;void ecall_hello_from_enclave(char *buf, size_t len)&#123; const char *hello = &quot;Hello world&quot;; size_t size = len; if(strlen(hello) &lt; len) &#123; size = strlen(hello) + 1; &#125; memcpy(buf, hello, size - 1); buf[size-1] = &#x27;\\0&#x27;;&#125; 3.调用可信函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;assert.h&gt;# include &lt;unistd.h&gt;# include &lt;pwd.h&gt;# define MAX_PATH FILENAME_MAX#include &quot;sgx_urts.h&quot;#include &quot;App.h&quot;#include &quot;Enclave_u.h&quot;/* Global EID shared by multiple threads */sgx_enclave_id_t global_eid = 0;int initialize_enclave(void)&#123; sgx_status_t ret = SGX_ERROR_UNEXPECTED; /* 调用 sgx_create_enclave 创建一个 Enclave 实例 */ /* Debug Support: set 2nd parameter to 1 */ ret = sgx_create_enclave(ENCLAVE_FILENAME, SGX_DEBUG_FLAG, NULL, NULL, &amp;global_eid, NULL); if (ret != SGX_SUCCESS) &#123; printf(&quot;Failed to create enclave, ret code: %d\\n&quot;, ret); return -1; &#125; return 0;&#125;/* 应用程序入口 */int SGX_CDECL main(int argc, char *argv[])&#123; (void)(argc); (void)(argv); const size_t max_buf_len = 100; char buffer[max_buf_len] = &#123;0&#125;; /* 创建并初始化 Enclave */ if(initialize_enclave() &lt; 0)&#123; printf(&quot;Enter a character before exit ...\\n&quot;); getchar(); return -1; &#125;//-------------------------添加代码区域------------------------------------ /* ECALL 调用 */ ecall_hello_from_enclave(global_eid, buffer, max_buf_len); printf(&quot;%s\\n&quot;, buffer);//------------------------------------------------------------------------ /* 销毁 Enclave */ sgx_destroy_enclave(global_eid); printf(&quot;Info: SampleEnclave successfully returned.\\n&quot;); printf(&quot;Enter a character before exit ...\\n&quot;); getchar(); return 0;&#125; 总结即便最简单的 SGX HelloWold 也比较复杂，当然“安全性”和“成本”（技术壁垒门槛、开发成本、维护成本、物料成本等）总是成正比的，和“效率”成反比的。希望这篇文章对那些想入门开发 SGX 应用的用户有所帮助。","categories":[{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/categories/SGX/"}],"tags":[{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/tags/SGX/"},{"name":"Linux","slug":"Linux","permalink":"https://snowman12137.github.io/tags/Linux/"}],"author":"Sn0wma1n"},{"title":"RSA数学原理解析","slug":"RSA数学原理解析","date":"2023-05-15T03:18:16.000Z","updated":"2023-09-02T14:32:10.629Z","comments":true,"path":"2023/05/15/RSA数学原理解析/","link":"","permalink":"https://snowman12137.github.io/2023/05/15/RSA%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/","excerpt":"","text":"1.引言 RSA算法(RSA algorithm)是一种非对称加密算法, 广泛应用在互联网和电子商务中. 它使用一对密钥进行加密和解密, 分别称为公钥(public key)和私钥(private key). 使用公钥加密的内容只能用私钥解密, 使用私钥加密的内容只能用公钥解密, 并且不能通过公钥在可行的时间内计算出私钥. 这使得加密通信不需要交换私钥, 保证了通信的安全. 那么它是怎么做到这一点的呢? 背后有哪些数学原理? 这篇文章我们来讨论这个问题. 本文会先介绍RSA算法中用到的数论概念和定理: 模算术, 最大公约数与贝祖定理, 线性同余方程, 中国余数定理, 费马小定理; 然后再介绍RSA算法的原理, 并证明其是有效的. 本文会假设你了解数论的基本概念, 如素数, 最大公约数, 互素等 2.模算术2.1整数除法用一个正整数去除一个整数，可以得到一个商和一个余数，数学符号定义为： 定理1：令 a 为整数, d 为正整数, 则存在唯一的整数 q 和 r, 满足 $0\\leq r&lt;d$, 使得 $a&#x3D;dq+r$ 当r&#x3D;0时，我们称 d 整除 a, 记作$d|a$; 否则称 d 不整除 a, 记作 $d\\nmid a$，整除有以下基本性质: 定理2：令a,b,c,为整数，其中$a\\neq0$,则：如果$a|b$且$a|c$则$a|(a+b)$ 2.2模算术在数论中我们特别关心一个整数被一个正整数除时的余数. 我们用 $a\\bmod m&#x3D;b$ 表示整数 a 除以正整数 m 的余数是 b. 为了表示两个整数被一个正整数除时的余数相同, 人们又提出了同余式(congruence). 定义1:如果 a 和 b 是整数而 m 是正整数, 则当 m 整除 a - b 时称 a 模 m 同余 b. 记作 $a\\equiv b(\\bmod m)$ $a\\equiv b(\\bmod m)$ 和 $a\\bmod m&#x3D;b$很相似. 事实上, 如果 $a\\bmod m&#x3D;b$, 则 $a\\equiv b(\\bmod m)$. 但他们本质上是两个不同的概念.$a\\bmod m&#x3D;b$ 表达的是一个函数, 而 $a\\equiv b(\\bmod m)$ 表达的是两个整数之间的关系. 另外，同余式$a\\equiv b(\\bmod m)$还可以表示为$m|(b-a)$，同时符合上式的式子可以转化为同余式 模算术的性质： 定理3：如果m是正整数，a,b是整数，则有$$\\begin{aligned}(a+b)\\bmod m &amp;&#x3D;((a\\bmod m)+(b\\bmod m))\\bmod m\\ab\\bmod m &amp;&#x3D;(a\\bmod m)(b\\bmod m)\\bmod m\\end{aligned}$$根据定理3，可得一下推论 推论1：设m是正整数，a,b,c是整数；如果$a\\equiv b(\\bmod m)$则$ac\\equiv bc(\\bmod m)$ 证明： $\\because a\\equiv b(\\bmod m)$ $\\therefore m|(b-a)$ 所以右端再乘以任何整数m都可以整除 即$m|(b-a)c$ 同理，既然$m|(b-a)$且$c|c$ 也可以推出$mc|(b-a)c$ 结论：若$a\\equiv b(\\bmod m)$，则$ac\\equiv bc(\\bmod m)$且$ac\\equiv bc(\\bmod mc)$同时成立 推论2设 m 是正整数, a, b 是整数, c 是不能被 m 整除的整数; 如果 $ac\\equiv bc(\\bmod m)$, 则$a\\equiv b(\\bmod m)$,依据推论1的证明，也是显而易见的。 3.最大公约数如果一个整数 d 能够整除另一个整数 a, 则称 d 是 a 的一个约数(divisor); 如果 d 既能整除 a 又能整除 b, 则称 d 是 a 和 b 的一个公约数(common divisor). 能整除两个整数的最大整数称为这两个整数的最大公约数(greatest common divisor). 定义2：令a和b是不全为0的两个整数，能使$d|a$和$d|b$的最大整数d成为a和b的最大公约数，记作$gcd(a,b)$ 3.1 求最大公约数如何求两个已知整数的最大公约数呢? 这里我们讨论一个高效的求最大公约数的算法, 称为辗转相除法. 因为这个算法是欧几里得发明的, 所以也称为欧几里得算法. 辗转相除法基于以下定理: 引理 1 令 $a&#x3D;bq+r$, 其中 a, b, q 和 r 均为整数. 则有$gcd(a,b)&#x3D;gcd(b,r)$ 证明：我们假设 d 是 a 和 b 的公约数, 即 $d|a$且 $d|b$, 那么根据定理2, d 也能整除 $a-bq&#x3D;r$. 所以 a 和 b 的任何公约数也是 b 和 r 的公约数; 类似地, 假设 d 是 b 和 r 的公约数, 即 $d|b$且 $d|r$, 那么根据定理2, d 也能整除 $a&#x3D;bq+r$.. 所以 b 和 r 的任何公约数也是 a 和 b 的公约数; 因此, a 与 b 和 b 与 r 拥有相同的公约数. 所以 $gcd(a,b)&#x3D;gcd(b,r)$ 辗转相除法就是利用引理1, 把大数转换成小数. 例如, 求 $gcd(287,91)$, 我们就把用较大的数除以较小的数. 首先用 287 除以 91, 得$$287&#x3D;91\\cdot3+14$$我们有$gcd(287,91)&#x3D;gcd(91,14)$,问题转化成求$gcd(91,14)$,同样的，用91除以14得$$91&#x3D;14\\cdot6+7$$有$gcd(91,14)&#x3D;gcd(14,7)$,用14除以7得$$14&#x3D;7\\cdot2+0$$所以$gcd(297,91)&#x3D;gcd(91,14)&#x3D;gcd(14,7)&#x3D;7$ 代码是这样的(两个都可以) 12345678910def gcd(a,b): while b!=0 : r = a%b a = b b = r return adef gcd_new(a,b): if b==0: return a return gcd_new(b,a%b) 3.2 贝祖定理现在我们讨论最大公约数的一个重要性质: 定理 4 贝祖定理 如果整数 a, b 不全为零, 则 $gcd(a,b)$是 a 和 b 的线性组合集$ax+by|x,y\\in Z$ 中最小的元素. 这里的 x 和 y 被称为贝祖系数 证明 令 $A&#x3D;ax+by|x,y\\in Z$ 设存在 $x_0$ ,$y_0$ 使 $d_0$ 是 A 中的最小正元素, $d_0&#x3D;ax_0+by_0$; 现在用 $d_0$去除 a, 这就得到唯一的整数 q(商) 和 r(余数) 满足$$\\begin{aligned}d_0q+r &amp;&#x3D; a \\qquad 0\\leq r&lt;d_0\\(ax_0+by_0)q+r&amp;&#x3D;a\\r&amp;&#x3D;a-aqx_0-bqy_0\\r&amp;&#x3D;a(1-qx_0)+b(-qy_0)\\in A\\end{aligned}$$又$0\\leq r &lt;d_0$,$d_0$是A中最小的正元素 $\\therefore r&#x3D;0,d|a$ 同理, 用 $d_0$去除 b, 可得 $d_0|b$. 所以说 $d_0$ 是 a 和 b 的公约数. 设 a 和 b 的最大公约数是 d, 那么$d|(ax_0+by_0)$即 $d|d_0$ $\\therefore d_0$是a和b的最大公约数 我们可以对辗转相除法稍作修改, 让它在计算出最大公约数的同时计算出贝祖系数. 1234def gcd_new_2(a,b): if b==0: return a,1,0 d,x,y = gcd_new_2(b,a%b) return d,y,x-(int(a/b))*y 大家是否还记得辗转相除法：我们换一个步骤多一点的例子$gcd(963,657)$$$\\begin{aligned}963&amp;&#x3D;1\\cdot657+306 \\qquad\\qquad&amp;9&amp;&#x3D;7\\cdot657-15\\cdot(963-657)&#x3D;22\\cdot657-15\\cdot963 \\657&amp;&#x3D;2\\cdot306+45 &amp;9&amp;&#x3D;7\\cdot(657-2\\cdot306)-306&#x3D;7\\cdot657-15\\cdot963 \\306&amp;&#x3D;6\\cdot45+36 &amp;9&amp;&#x3D;45-(306-6\\cdot45)&#x3D;7\\cdot45-306\\45 &amp;&#x3D;1\\cdot36+9 &amp;9&amp;&#x3D;45-36\\36 &amp;&#x3D;4\\cdot9\\end{aligned}$$ $gcd(963,657)&#x3D;9&#x3D;22\\cdot657-15\\cdot963,x_0&#x3D;-15,y_0&#x3D;22$就是二元一次方程$963x+657y&#x3D;9$的一组解且是$963x+657y$方程的正整数中的最小解。 4.线性同余方程现在我们来讨论求解形如 $ax\\equiv b(\\bmod m)$ 的线性同余方程. 求解这样的线性同余方程是数论研究及其应用中的一项基本任务. 如何求解这样的方程呢? 我们要介绍的一个方法是通过求使得方程 $\\overline{a}a\\equiv 1(\\bmod m)$ 成立的整数 $\\overline{a}$. 我们称 $\\overline{a}$为 a 模 m 的逆. 下面的定理指出, 当 a 和 m 互素时, a 模 m 的逆必然存在. 定理5：如果 a 和 m 为互素的整数且 $m&gt;1$, 则 a 模 m 的逆存在, 并且是唯一的. 证明 由贝祖定理可知, $\\because gcd(a,m)&#x3D;1$ , ∴ 存在整数 x 和 y 使得$$ax+my&#x3D;1$$这蕴含着$$ax+my\\equiv 1(\\bmod m)\\\\because my\\equiv0(\\bmod m)，所以有\\ax\\equiv 1(\\bmod m)\\\\therefore x为a模的逆$$这样我们就可以调用辗转相除法 gcd(a, m) 求得 a 模 m 的逆. 求得了 a 模 m 的逆 �¯, 现在我们可以来解线性同余方程了. 具体的做法是这样的: 对于方程$ax\\equiv b( \\bmod m)$同时乘以$\\overline{a}$得,$$\\overline{a}ax\\equiv \\overline{a}b(\\bmod m)$$$把\\overline{a}a\\equiv 1(\\bmod m )代入上式，得\\$$$x\\equiv \\overline{a}b(\\bmod m)$$ $x\\equiv \\overline{a}b(\\bmod m)$就是方程的解，注意同余方程会有无数个W整数解, 所以我们用同余式来表示同余方程的解. 例1:求$34x\\equiv 77(\\bmod 89)$ 解调用$gcd(34,89)$,得$gcd(34,89)&#x3D;1&#x3D;1389-3434$,所以34模89的逆为-34，方程两边同时乘 -34 得$$\\begin{aligned}-34\\cdot34x&amp;\\equiv-34\\cdot77(\\bmod89)\\x&amp;\\equiv-34\\cdot77(\\bmod89)\\x&amp;\\equiv-2618\\equiv52(\\bmod89)\\end{aligned}$$ 5.中国剩余定理中国南北朝时期数学著作 孙子算经 中提出了这样一个问题: 有物不知其数，三三数之剩二，五五数之剩三，七七数之剩二。问物几何？ 用现代的数学语言表述就是: 下列同余方程组的解释多少?$$\\begin{cases}x\\equiv2(\\bmod3)\\x\\equiv3(\\bmod5)\\x\\equiv2(\\bmod7)\\\\end{cases}$$孙子算经 中首次提到了同余方程组问题及其具体解法. 因此中国剩余定理称为孙子定理. 定理 6 中国余数定理 令 $m_1,m_2,…..,m_n$为大于 1 且两两互素的正整数, $a_1,a_2,….a_n$ 是任意整数. 则同余方程组$$\\begin{cases}x\\equiv a_1(\\bmod m_1)\\x\\equiv a_2(\\bmod m_2)\\…..\\x\\equiv a_n(\\bmod m_n)\\\\end{cases}$$有唯一的模$m&#x3D;m_1m_2….m_n$的解 证明：我们使用构造证明法, 构造出这个方程组的解. 首先对于 $i&#x3D;1,2,….,n$, 令$$M_i&#x3D;\\frac{m}{m_i}$$ 即,$M_i$ 是除了$m_i$ 之外所有模数的积.$\\because m_1,m_2….m_n$两两互素, $\\therefore gcd(m_i,M_i)&#x3D;1$ 由定理5 可知, 存在整数 $y_i$ 是 $M_i$模$m_i$的逆. 即$$M_iy_i\\equiv1(\\bmod m_i)$$同时乘以$a_i$得$$a_iM_iy_i\\equiv a_i(\\bmod m_i)$$就是第 i 个方程的一个解; 那么怎么构造出方程组的解呢? 我们注意到, 根据 $M_i$ 的定义可得, 对所有的 $j\\neq i$, 都有 $a_iM_iy_i\\equiv0(\\bmod m_j)$. 因此我们令$$x&#x3D;a_1M_1y_1+a_2M_2y_2+….+a_nM_ny_n&#x3D;\\sum_{i&#x3D;1}^{n}{a_iM_iy_i}$$有了这个结论, 我们可以解答 孙子算经 中的问题了: 对方程组的每个方程, 求出 $M_i$ , 然后调用 gcd(M_i, m_i) 求出 $y_i$:$$\\begin{cases}\\begin{aligned}&amp;x\\equiv2(\\bmod3)\\quad &amp;M_1&amp;&#x3D;35\\quad y_1&#x3D;-1\\&amp;x\\equiv3(\\bmod5)\\quad &amp;M_2&amp;&#x3D;21\\quad y_2&#x3D;1\\&amp;x\\equiv2(\\bmod7)\\quad &amp;M_3&amp;&#x3D;15\\quad y_3&#x3D;-1\\\\end{aligned}\\end{cases}$$最后求出$x&#x3D;-235+321+2*15&#x3D;23\\equiv23(\\bmod 105)$ 6.费马小定理现在我们来看数论中另外一个重要的定理, 费马小定理(Fermat’s little theorem) 定理7费马小定理：如果a是一个整数，p是一个素数，那么$$a^p\\equiv a(\\bmod p)$$特别的当a不是p的倍数时（即$gcd(a,p)&#x3D;1$），有$$a^{p-1}\\equiv1(\\bmod p)\\$$ 7欧拉函数再来看一看欧拉函数的知识。对于正整数n，欧拉函数$\\varphi(n)$是小于或等于n的正整数中与n互质的数的数目 如$\\varphi(8)&#x3D;4$，1,3,5,7均与8互质 定理8：n,m为整数,$(n,m)&#x3D;1$，如果$a_1,a_2,….a_n$和$b_1,b_2,….b_n$分别是模n,m的一个完整系，则有形如$nb_i+ma_j(1\\leq i\\leq s,1\\leq j\\leq t)$的数构成模mn的一个完整缩系，特别地有$\\varphi(nm)&#x3D;\\varphi(n)\\varphi(m)$ 定理9：当n$\\geq2$时，设$n&#x3D;p_1^{e_1}….p_s^{e_s}$是n的标准分解式，则$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}(p_l^{e_l}-p_l^{e_l-1})&#x3D;n\\prod_{l&#x3D;1}^{s}(1-\\frac{1}{p})$$证明：当$(n.m)&#x3D;1$时$$\\varphi(n,m)&#x3D;\\varphi(n)\\varphi(m)$$当$n_1,n_2……n_s$两两互素时$$\\varphi(\\prod_{l&#x3D;1}^{s}n_l)&#x3D;\\prod_{l&#x3D;1}^{s}n_l$$因此$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}\\varphi(p_l^{e_l})$$问题转化成如何求$\\varphi(p_l^{e_l})$,其中$p_l$要么$p_l|a$,要么$(p_l,a)&#x3D;1$。在1,2….$p_l$当中可以被$p_l$整除的整数共有$\\frac{p_l^{e_l}}{p_l}&#x3D;p_l^{e_l-1}$个，故其中与$p_l^{e_l}$互素的个数共有$p_l^{e_l}-p_l^{e_l-1}$个，于是$$\\varphi(p_l^{e_l})&#x3D;p_l^{e_l}-p_l^{e_l-1}$$这样一来$$\\varphi(n)&#x3D;\\prod_{l&#x3D;1}^{s}(p_l^{e_l}-p_l^{e_l-1})&#x3D;\\prod_{l&#x3D;1}^{s}p_l^{e_l}(1-\\frac{1}{p_l})&#x3D;n\\prod_{l&#x3D;1}^{s}(1-\\frac{1}{p})$$其中若p为素数，则$$\\varphi(p)&#x3D;p-1&#x3D;p(1-\\frac{1}{p})$$ 8.RSA算法我们终于可以来看 RSA 算法了. 先来看 RSA 算法是怎么运作的: RSA 算法按照以下过程创建公钥和私钥: 1.随机算取两个大素数p和q，$p\\neq q$ 2.计算$n&#x3D;pq$ 3.选取一个与$(p-1)(q-1)$互素的小整数e 4.求e模$(p-1)(q-1)$的逆，记作d，即$de\\equiv 1\\bmod(p-1)(q-1)$ 5.将$P&#x3D;(e,n)$公开，是公钥。 6.将$S&#x3D;(d,n)$保密，作为私钥 想 要把明文$M $加密成密文$C$,计算$$C&#x3D;M^e\\bmod n$$要把密文解密成明文$C$$M $,计算$$M&#x3D;C^d \\bmod n$$ 下面证明RSA算法是有效的： 证明：要证明其有效性，只需要证明$C\\equiv M^e\\bmod n$也就是$M^{ed}\\equiv M(\\bmod n)$,注意到d为e模$(p-1)(1-1)$的逆，所以有$$ed\\equiv 1(\\bmod(p-1)(q-1))$$ 。","categories":[{"name":"CTF入门","slug":"CTF入门","permalink":"https://snowman12137.github.io/categories/CTF%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"现代密码学","slug":"现代密码学","permalink":"https://snowman12137.github.io/tags/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"author":"Sn0wma1n"},{"title":"大三网络安全人工智能实验报告","slug":"大三网络安全人工智能实验报告","date":"2023-05-04T12:09:16.000Z","updated":"2024-02-03T17:05:00.554Z","comments":true,"path":"2023/05/04/大三网络安全人工智能实验报告/","link":"","permalink":"https://snowman12137.github.io/2023/05/04/%E5%A4%A7%E4%B8%89%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/","excerpt":"","text":"《人工智能》课程实验报告网络与信息安全学院班 级： 20180XX 姓 名： XXX 学 号： 20009XXXX 提交时间： 2023. 4. 20 基于神经网络的MNIST手写数字识别一、实验目的 掌握运用神经网络模型解决有监督学习问题 掌握机器学习中常用的模型训练测试方法 了解不同训练方法的选择对测试结果的影响 二、实验内容MNIST数据集​ 本实验采用的数据集MNIST是一个手写数字图片数据集，共包含图像和对应的标签。数据集中所有图片都是28x28像素大小，且所有的图像都经过了适当的处理使得数字位于图片的中心位置。MNIST数据集使用二进制方式存储。图片数据中每个图片为一个长度为784（28x28x1，即长宽28像素的单通道灰度图）的一维向量，而标签数据中每个标签均为长度为10的一维向量。 分层采样方法​ 分层采样（或分层抽样，也叫类型抽样）方法，是将总体样本分成多个类别，再分别在每个类别中进行采样的方法。通过划分类别，采样出的样本的类型分布和总体样本相似，并且更具有代表性。在本实验中，MNIST数据集为手写数字集，有0~9共10种数字，进行分层采样时先将数据集按数字分为10类，再按同样的方式分别进行采样。 神经网络模型评估方法​ 通常，我们可以通过实验测试来对神经网络模型的误差进行评估。为此，需要使用一个测试集来测试模型对新样本的判别能力，然后以此测试集上的测试误差作为误差的近似值。两种常见的划分训练集和测试集的方法： ​ 留出法（hold-out）直接将数据集按比例划分为两个互斥的集合。划分时为尽可能保持数据分布的一致性，可以采用分层采样（stratified sampling）的方式，使得训练集和测试集中的类别比例尽可能相似。需要注意的是，测试集在整个数据集上的分布如果不够均匀还可能引入额外的偏差，所以单次使用留出法得到的估计结果往往不够稳定可靠。在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。 ​ k折交叉验证法（k-fold cross validation）先将数据集划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即也采用分层采样（stratified sampling）的方法。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练集和测试集，从而可以进行k次训练和测试。最终返回的是这k个测试结果的均值。显然，k折交叉验证法的评估结果的稳定性和保真性在很大程度上取决于k的取值。k最常用的取值是10，此外常用的取值还有5、20等。 三、实验方法设计 实验环境 1.VSCODE 2.anaconda&#x3D;&#x3D;4.14.0 3.python&#x3D;&#x3D;3.7 4.TensorFlow–gpu&#x3D;&#x3D;1.15.0 5.Keras&#x3D;&#x3D;2.3.1 6.实验报告编辑器：typora 介绍实验中程序的总体设计方案、关键步骤的编程方法及思路，主要包括: 因为之前用过pytorch进行机器学习的训练和学习，所以本作业使用pytorch进行建模和训练。 标准训练流程如下：导入包-&gt;设定初始值-&gt;加载数据集（预处理）-&gt;建立模型-&gt;训练-&gt;测试-&gt;评估 其中需要对加载数据集进行处理，把留出法的比例进行调整来观察结果。 其次要使用k折交叉验证法进行对比测试。 为了表明k折交叉验证法与留出法的效果对比，我建立了连个模型，一个是按照标准流程建立的优秀的模型。一个是用作对比k折交叉验证法与留出法效果对比的劣质模型。 含有tensorflow部分代码，在四、4中。 设置初始值 123456&gt;mean = [0.5]&gt;std = [0.5]&gt;# batch size&gt;BATCH_SIZE =128&gt;Iterations = 1 # epoch&gt;learning_rate = 0.01 优化器与损失函数 12&gt;criterion = torch.nn.CrossEntropyLoss() &gt;optimizer = torch.optim.SGD(model.parameters(),learning_rate) 训练代码 12345678910111213&gt;def train(model, optimizer,criterion,epoch): model.train() # setting up for training for batch_idx, (data, target) in enumerate(train_loader): # data contains the image and target contains the label = 0/1/2/3/4/5/6/7/8/9 data = data.view(-1, 28*28).requires_grad_() optimizer.zero_grad() # setting gradient to zero output = model(data) # forward loss = criterion(output, target) # loss computation loss.backward() # back propagation here pytorch will take care of it optimizer.step() # updating the weight values if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) 测试代码 1234567891011121314151617181920212223&gt;def test(model, criterion, val_loader, epoch,train= False): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for batch_idx, (data, target) in enumerate(val_loader): data = data.view(-1, 28*28).requires_grad_() output = model(data) test_loss += criterion(output, target).item() # sum up batch loss pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() # if pred == target then correct +=1 test_loss /= len(val_loader.dataset) # average test loss if train == False: print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.4f&#125;%)\\n&#x27;.format( test_loss, correct, val_loader.sampler.__len__(), 100. * correct / val_loader.sampler.__len__() )) if train == True: print(&#x27;\\nTrain set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.4f&#125;%)\\n&#x27;.format( test_loss, correct, val_loader.sampler.__len__(), 100. * correct / val_loader.sampler.__len__() )) return 100. * correct / val_loader.sampler.__len__() 1）模型构建的程序设计（伪代码或源代码截图）及说明解释 （10分）训练模型 12345678910111213141516class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output 我用了一层conv和一层pool来获取cherng图片的特征。之后把这些特征减小为10个层，所以用flatten把特征集中成vector后，再用一个全连接层连接到输出层。 使用留出法原始的训练比例，两个Epoch，得到的结果很好。达到97%。 为了对比留出法及K折验证法建立的简陋模型 1model = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),nn.Linear(256, 10)) 可以看到这个简陋模型得到的结果很差，准确率只有83%，用于之后的K折校验法的对比组。预处理和上述相同，Epoch只有一组。 2）模型迭代训练的程序设计（伪代码或源代码截图）及说明解释 （10分）123456789101112131415def train(model, optimizer,criterion,epoch): model.train() # setting up for training for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28).requires_grad_() optimizer.zero_grad() # setting gradient to zero output = model(data) # forward loss = criterion(output, target) # loss computation loss.backward() # back propagation here pytorch will take care of it optimizer.step() # updating the weight values if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) 先注意到因為 training 和 testing 時 model 會有不同行為，所以用 model.train() 把 model 調成 training 模式。 接著 iterate 過 batch_idx，每個 batch_idx會 train 過整個 training set。每個 dataset 會做 batch training。 接下來就是重點了。基本的步驟：zero_grad、model(data)、取 loss、back propagation 算 gradient、最後 update parameter。前面都介紹過了，還不熟的可以往前翻。 3）模型训练过程中周期性测试的程序设计（伪代码或源代码截图）及说明解释（周期性测试指的是每训练n个step就对模型进行一次测试，得到准确率和loss值）（10分） 我选用了每100步进行一个打印的频率，打印训练进度和Loss值，最后打印平均损失值和准确率。 4）分层采样的程序设计（伪代码或源代码截图）及说明解释 （10分）12345678910111213141516171819train_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std) ]) test_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std) ]) train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;./mnist&#x27;, train=True, download=True, transform=train_transform), batch_size=BATCH_SIZE, shuffle=True) # train dataset test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;./mnist&#x27;, train=False, transform=test_transform), batch_size=BATCH_SIZE, shuffle=False) # test dataset 123456789&gt;def hold_out(images, labels, train_percentage): test_acc = torch.zeros([Iterations]) train_acc = torch.zeros([Iterations]) ## training the logistic model for i in range(Iterations): train(model, optimizer,criterion,i) train_acc[i] = test(model, criterion, train_loader, i,train=True) #Testing the the current CNN test_acc[i] = test(model, criterion, test_loader, i) torch.save(model,&#x27;perceptron.pt&#x27;) 使用了系统自带的minist数据分类器。 5）k折交叉验证法的程序设计（伪代码或源代码截图）及说明解释 （10分） mnist数据集的训练集和测试集的合并 123456789train_init = datasets.MNIST(&#x27;./mnist&#x27;, train=True, transform=train_transform) test_init = datasets.MNIST(&#x27;./mnist&#x27;, train=False, transform=test_transform) # the dataset for k fold cross validation dataFold = torch.utils.data.ConcatDataset([train_init, test_init]) 使用Sklearn中的KFold进行数据集划分，并且转换回pytorch类型的Dataloader 123456789kf = KFold(n_splits=k_split_value,shuffle=True, random_state=0) # init KFold for train_index , test_index in kf.split(dataFold): # split # get train, val 根据索引划分 train_fold = torch.utils.data.dataset.Subset(dataFold, train_index) test_fold = torch.utils.data.dataset.Subset(dataFold, test_index) train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True) 完整的代码 1234567891011121314151617181920212223242526def train_flod_Mnist(k_split_value): different_k_mse = [] kf = KFold(n_splits=k_split_value,shuffle=True, random_state=0) # init KFold for train_index , test_index in kf.split(dataFold): # split # get train, val train_fold = torch.utils.data.dataset.Subset(dataFold, train_index) test_fold = torch.utils.data.dataset.Subset(dataFold, test_index) # package type of DataLoader train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True) # train model test_acc = torch.zeros([Iterations]) train_acc = torch.zeros([Iterations]) ## training the logistic model for i in range(Iterations): train(model, optimizer,criterion,i) train_acc[i] = test(model, criterion, train_loader, i,train=True) #Testing the the current CNN test_acc[i] = test(model, criterion, test_loader, i) #torch.save(model,&#x27;perceptron.pt&#x27;) # one epoch, all acc different_k_mse.append(np.array(test_acc)) return different_k_mse 按循序打印结果 123456testAcc_compare_map = &#123;&#125;for k_split_value in range(2, 10+1): print(&#x27;now k_split_value is:&#x27;, k_split_value) testAcc_compare_map[k_split_value] = train_flod_Mnist(k_split_value)for key in testAcc_compare_map:print(np.mean(testAcc_compare_map[key])) testAcc_compare_map是将不同k值下训练的结果保存起来，之后我们可以通过这个字典变量，计算出rmse ，比较不同k值下，实验结果的鲁棒性。 四、实验结果展示展示程序界面设计、运行结果及相关分析等，主要包括： 1）模型在验证集下的准确率（输出结果并截图）（10分） 下面的实验是k值为[2,10]下的结果，训练模型为简陋模型。 对照组：简陋模型，epoch为1，分层抽样（正确率只有83.79%） k折校验：简陋模型，epoch为1，K折交叉验证(K值为2到10)准确率越来越大 2）不同模型参数（隐藏层数、隐藏层节点数）对准确率的影响和分析 （10分） 本次实验中只探讨了简陋版模型与卷积模型的对比： 其中简陋版模型如下，先把图片变为一个以为张量，然后由一个全连接层链接，接入到ReLu层中，然后接入全连接层，可以看到，并没有使用卷积层，在epoch&#x3D;1的情况下只有83%的准确率。 1&gt;model = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),nn.Linear(256, 10)) 卷积模型如下，先定义卷积卷积层，输入通道为1，输出为32，核大小为3，一个Dropout2d层，以0.25的概率将通道输入置零，防止过拟合。然后是一个全连接层，输入为5408，输出为10，映射到10个分类结果。在forward中首先通过卷积层进行卷积，然后通过ReLU进行非线性变换，然后使用最大池化层进行采样，将图签尺寸缩小一半，然后用Dropout2d防止过拟合，接着把输出的张良展平为一维，并传入全连接层。 在 12345678910111213141516&gt;class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output 可以看到在epoch&#x3D;1的情况下准确率达到97%，拟合效果非常好 3）不同训练参数（batch size、epoch num、学习率）对准确率的影响和分析 （10分） 注：默认值：在讨论某一变化时，其他值不变 123&gt;BATCH_SIZE =64&gt;Iterations = 1 # epoch&gt;learning_rate = 0.01 原始结果（83%） BATCH_SIZE讨论（可以发现当BATCH_SIZE越大时，准确率直线下降） 1&gt;&#123;&quot;16&quot;:&quot;90.85%&quot;,&quot;32&quot;:&quot;90.1000%&quot;,&quot;64&quot;:&quot;88.2200%&quot;,&quot;128&quot;:&quot;83.7100%&quot;,&quot;256&quot;:&quot;70.7300%&quot;,&quot;512&quot;:&quot;48.2400%&quot;&#125; epoch讨论(可以发现随着epoch的增大，准确率有较大提升，但是随着epoch越来越大，准确率增长越来越慢) 12[1,2,4,6,10,15][83.83,88.370,90.290,91.260,92.420,93.36] 学习率 可以看到，当学习率增大时，准确率有所增加，但是当学习率大于0.2时，准确率急速下滑到11%左右，也就是说，10个手写体正确率只有1个，趋于随机分布，是一个非常不好的模型，可见，学习率的选择至关重要。 12&gt;[0.01,0.02,0.05,0.1,0.2,0.5,1]&gt;[83.83,88.230,90.78,91.79,91.78,11.35,11.35] 4）留出法不同比例对结果的影响和分析 （10分） 因为pytorch中的训练集是固定输出的，对其更改较难，所以本小节使用TensorFlow进行实验： 数据集划分:其中a为训练比率，总共有70000个样本，按照比例进行训练和测试，结果如下 1234567891011121314151617181920212223&gt;np.random.seed(10)&gt;a = 0.8&gt;from keras.datasets import mnist&gt;(x_train_image,y_train_label),(x_test_image,y_test_label)=mnist.load_data()&gt;# x_all = x_train_image + x_test_image&gt;temp = np.append( x_train_image , x_test_image)&gt;x_all = temp.reshape(70000,28,28)&gt;print(len(y_train_label))&gt;y_lable = np.append(y_train_label,y_test_label)&gt;train_num = int(60000*a)&gt;x_train_image = x_all[:train_num]&gt;y_train_label = y_lable[:train_num]&gt;x_test_image = x_all[train_num:]&gt;y_test_label = y_lable[train_num:]&gt;x_Train=x_train_image.reshape(train_num,784).astype(&#x27;float32&#x27;)&gt;x_Test=x_test_image.reshape(70000-train_num,784).astype(&#x27;float32&#x27;) 12&gt;[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.9999]&gt;[0.9213, 0.9381, 0.9528, 0.9620, 0.9666, 0.9673, 0.9709, 0.9752, 0.9758, 0.9784, 0.9780] 可以看到随着训练样本的比率上升，总体的准确率也对应的明显的上升了，但是最后一组0.9999比率的组，较前一组0.95有所下降，这表明过大的训练比率对结果也会产生损害。 5）k折交叉验证法不同k值对结果的影响和分析 （10分） 把k值从2-10进行迭代计算，其他参数不变，结果为： 12345678&gt;testAcc_compare_map = &#123;&#125;&gt;for k_split_value in range(2, 10+1): print(&#x27;now k_split_value is:&#x27;, k_split_value) testAcc_compare_map[k_split_value] = cross_validation(k_split_value) &gt;for key in testAcc_compare_map: print(np.mean(testAcc_compare_map[key])) 可见K值对结果影响很大，且在一定范围内，越大越好。 五、实验总结及心得 本次实验熟知了pytorch和TensorFlow的使用，还有机器学习的整体流程和处理概况，解决了出现的诸多问题，尤其是在配置TensorFlow版本时出现的问题，熟知了基本的图像处理模型，以及卷积模型的基本构建。 在参数调配方面，详细了解了分层取样法，k折交叉验证法的使用以及效果还有比例的调试。还有在关键参数如batch size、epoch num、学习率方面有着较好的经验总结。","categories":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://snowman12137.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"author":"Sn0wma1n"},{"title":"大三网络安全智能终端实验一","slug":"大三网络安全智能终端实验一","date":"2023-05-04T12:09:16.000Z","updated":"2024-02-03T17:04:32.365Z","comments":true,"path":"2023/05/04/大三网络安全智能终端实验一/","link":"","permalink":"https://snowman12137.github.io/2023/05/04/%E5%A4%A7%E4%B8%89%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E6%99%BA%E8%83%BD%E7%BB%88%E7%AB%AF%E5%AE%9E%E9%AA%8C%E4%B8%80/","excerpt":"","text":"西安电子科技大学网信院 智能终端实验实验报告（一）班级：2018XX学号：20009XXXX日期：2023.4.24一、实验摘要研究不断推动机器学习模型更快，更精确，更有效。然而，设计和训练模型的一个经常被忽视的方面是安全性和鲁棒性，特别是在面对希望欺骗模型的对手时。向图像添加不可察觉的扰动可以导致完全不同的模型性能。我们将通过一个图像分类器的例子来探讨这个主题。具体来说，我们将使用第一个和最流行的攻击方法之一，快速梯度符号攻击(FGSM) ，以欺骗 MNIST 分类器。（摘自https://pytorch.org/tutorials/beginner/ fgsm_tutorial.htm l?highlight&#x3D;a dversarial% 20example%20generation） 二、实验内容a) 实验思路#### 简述 有许多类别的对抗性攻击，每种攻击都有不同的目标和对攻击者知识的假设。然而，一般来说，总体目标是向输入数据添加最少量的扰动，从而导致所需的错误分类。有几种假设攻击者的知识，其中两个是: 白盒和黑盒。白盒攻击假设攻击者对模型有完整的知识和访问权限，包括体系结构、输入、输出和权重。黑盒攻击假设攻击者只能访问模型的输入和输出，并且对底层架构或权重一无所知。还有几种类型的目标，包括错误分类和源&#x2F;目标错误分类。错误分类的目标意味着对手只希望输出分类是错误的，而不关心新的分类是什么。源&#x2F;目标错误分类意味着对手想要更改原来属于特定源类的图像，以便将其归类为特定目标类。在这种情况下，FGSM 攻击是以错误分类为目标的白盒攻击。有了这些背景信息，我们现在可以详细讨论这次攻击了。 快速梯度 原理简述快速梯度符号攻击（FGSA），是通过基于相同的反向传播梯度来调整输入数据使损失最大化，简言之，共计使用了丢失的梯度值w.r.t输入数据，然后调整输入使数据丢失最大化。下图是实现的例子： 可以看出x是正常“熊猫”的原始输入图像，y是真实的图片输入， θ表示模型参数，J(θ,x,y)是损失函数，∇x​J(θ,x,y)是攻击将梯度反向传播回要计算的输入数据。然后，它通过一个小步骤调整输入数据，如0.007倍的sign(∇x​J(θ,x,y))添加到里面会最大化损失函数。然后得到扰动图像x’，然后被目标网络错误地归类为“长臂猿”，而它显然仍然是一只“熊猫”。 b) 实现过程1.输入只有三个输入 Epsilons-运行时使用的 epsilon 值列表。在列表中保留0很重要，因为它表示原始测试集上的模型性能。此外，直观地，我们期望更大的 ε，更明显的扰动，但更有效的攻击方面的退化模型的准确性。因为这里的数据范围是 [ 0 , 1 ] [0,1] ，ε 值不应超过1。 Pretrain _ model-path加载预训练的模型 Use _ CUDA-boolean 标志来使用 CUDA，如果需要的话。 123epsilons = [0, .05, .1, .15, .2, .25, .3]pretrained_model = &quot;mnist_cnn.pt&quot;use_cuda=True 2.模型建立a)训练模型我們用了一層 convolution layer 和 pooling layer 來擷取 image 的 feature，之後要把這些 feature map 成 10 個 node 的 output（因為有 10 個 class 要 predict），所以用 flatten 把 feature 集中成 vector 後，再用 fully-connected layer map 到 output layer。b 是 batch size，一次 train 幾張 image。 12345678910111213141516class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv = nn.Conv2d(1, 32, 3) self.dropout = nn.Dropout2d(0.25) self.fc = nn.Linear(5408, 10) def forward(self, x): x = self.conv(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout(x) x = torch.flatten(x, 1) x = self.fc(x) output = F.log_softmax(x, dim=1) return output b)训练函数123456789101112131415161718192021222324def train(model, train_loader, optimizer, epochs, log_interval): model.train() for epoch in range(1, epochs + 1): for batch_idx, (data, target) in enumerate(train_loader): # Clear gradient optimizer.zero_grad() # Forward propagation output = model(data) # Negative log likelihood loss loss = F.nll_loss(output, target) # Back propagation loss.backward() # Parameter update optimizer.step() # Log training info if batch_idx % log_interval == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) c)训练中的测试1234567891011121314151617181920def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): # disable gradient calculation for efficiency for data, target in test_loader: # Prediction output = model(data) # Compute loss &amp; accuracy test_loss += F.nll_loss(output, target, reduction=&#x27;sum&#x27;).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) # Log testing info print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\\n&#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) d)训练的主函数（把模型保存）12345678910111213141516171819202122232425262728293031323334def main(): # Training settings BATCH_SIZE = 64 EPOCHS = 2 LOG_INTERVAL = 10 # Define image transform transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) # mean and std for the MNIST training set ]) # Load dataset train_dataset = datasets.MNIST(&#x27;./data&#x27;, train=True, download=True, transform=transform) test_dataset = datasets.MNIST(&#x27;./data&#x27;, train=False, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE) # Create network &amp; optimizer model = Net() optimizer = optim.Adam(model.parameters()) # Train train(model, train_loader, optimizer, EPOCHS, LOG_INTERVAL) # Save and load model torch.save(model.state_dict(), &quot;mnist_cnn.pt&quot;) model = Net() model.load_state_dict(torch.load(&quot;mnist_cnn.pt&quot;)) # Test test(model, test_loader) 训练参数储存 e)训练结果 3.FGSM攻击a)模型加载加载模型 1234567891011121314151617181920# MNIST Test dataset and dataloader declarationtransform = transforms.Compose([ transforms.ToTensor(), ])train_dataset = datasets.MNIST(&#x27;./data&#x27;, train=True, download=True, transform=transform)test_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)# Define what device we are usingprint(&quot;CUDA Available: &quot;,torch.cuda.is_available())device = torch.device(&quot;cuda&quot; if (use_cuda and torch.cuda.is_available()) else &quot;cpu&quot;)# Initialize the networkmodel = Net().to(device)# Load the pretrained modelmodel.load_state_dict(torch.load(&quot;mnist_cnn.pt&quot;, map_location=&#x27;cpu&#x27;))# Set the model in evaluation mode. In this case this is for the Dropout layersmodel.eval() b)攻击函数创建现在，我们可以通过扰动原始输入来定义创建对抗性示例的函数。Fgsm 攻击函数有三个输入，图像是原始清晰图像(xx) ，ε 是像素级扰动量(εε) ，data _ grad 是损失的梯度，输入图像(∇xJ(θ,x,y))。然后，该函数创建扰动图像$$perturbed_image&#x3D;image+epsilon∗sign(data_grad)&#x3D;x+ϵ∗sign(∇x​ J(θ,x,y))$$ 12345678910# FGSM attack codedef fgsm_attack(image, epsilon, data_grad): # Collect the element-wise sign of the data gradient sign_data_grad = data_grad.sign() # Create the perturbed image by adjusting each pixel of the input image perturbed_image = image + epsilon*sign_data_grad # Adding clipping to maintain [0,1] range perturbed_image = torch.clamp(perturbed_image, 0, 1) # Return the perturbed image return perturbed_image c)开始攻击12345678accuracies = []examples = []# Run test for each epsilonfor eps in epsilons: acc, ex = test(model, device, test_loader, eps) accuracies.append(acc) examples.append(ex) d) 实验结果截图第一个结果是精度对 ε 图。正如前面提到的，随着 ε 的增加，我们预计测试的准确性会降低。这是因为更大的 ε 意味着我们在将损失最大化的方向上迈出了更大的一步。注意曲线中的趋势不是线性的，即使 ε 值是线性间隔的。例如，ε &#x3D; 0.05 ε &#x3D; 0.05的准确性仅比 ε &#x3D; 0ε &#x3D; 0低约4% ，但是 ε &#x3D; 0.2 ε &#x3D; 0.2的准确性比 ε &#x3D; 0.15 ε &#x3D; 0.15低25% 。另外，请注意模型的精度在 ε &#x3D; 0.25 ε &#x3D; 0.25和 ε &#x3D; 0.3 ε &#x3D; 0.3之间的随机精度。 打印上述数据 12345678910111213141516# Plot several examples of adversarial samples at each epsilon cnt = 0 plt.figure(figsize=(8,10)) for i in range(len(epsilons)): for j in range(len(examples[i])): cnt += 1 plt.subplot(len(epsilons),len(examples[0]),cnt) plt.xticks([], []) plt.yticks([], []) if j == 0: plt.ylabel(&quot;Eps: &#123;&#125;&quot;.format(epsilons[i]), fontsize=14) orig,adv,ex = examples[i][j] plt.title(&quot;&#123;&#125; -&gt; &#123;&#125;&quot;.format(orig, adv)) plt.imshow(ex, cmap=&quot;gray&quot;) plt.tight_layout() plt.show() 打印几个代表性的例子 三、实验结果分析分析从结果来看，随着 ε 的增加，测试精度下降，但扰动变得更容易察觉。实际上，在攻击者必须考虑的精确度下降和可感知性之间存在权衡。在这里，我们展示了一些成功的对抗例子在每个 ε 值。图的每一行显示不同的 ε 值。第一行是 ε &#x3D; 0 ε &#x3D; 0的例子，它表示原始的“干净”图像，没有扰动。每张图片的标题都显示了“原始分类-&gt; 敌对分类”注意，当 ε &#x3D; 0.15 ε &#x3D; 0.15时，扰动开始变得明显，当 ε &#x3D; 0.3 ε &#x3D; 0.3时，扰动非常明显。然而，在所有情况下，人们仍然能够识别正确的类别，尽管增加了噪音。","categories":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}]}],"categories":[{"name":"Electron-Vue-WebRTC客户端学习","slug":"Electron-Vue-WebRTC客户端学习","permalink":"https://snowman12137.github.io/categories/Electron-Vue-WebRTC%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"MAGs云分析","slug":"MAGs云分析","permalink":"https://snowman12137.github.io/categories/MAGs%E4%BA%91%E5%88%86%E6%9E%90/"},{"name":"其他","slug":"其他","permalink":"https://snowman12137.github.io/categories/%E5%85%B6%E4%BB%96/"},{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/categories/SGX/"},{"name":"CTF入门","slug":"CTF入门","permalink":"https://snowman12137.github.io/categories/CTF%E5%85%A5%E9%97%A8/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/categories/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}],"tags":[{"name":"Nodejs学习","slug":"Nodejs学习","permalink":"https://snowman12137.github.io/tags/Nodejs%E5%AD%A6%E4%B9%A0/"},{"name":"metaGEM","slug":"metaGEM","permalink":"https://snowman12137.github.io/tags/metaGEM/"},{"name":"博客问题","slug":"博客问题","permalink":"https://snowman12137.github.io/tags/%E5%8D%9A%E5%AE%A2%E9%97%AE%E9%A2%98/"},{"name":"SGX","slug":"SGX","permalink":"https://snowman12137.github.io/tags/SGX/"},{"name":"Linux","slug":"Linux","permalink":"https://snowman12137.github.io/tags/Linux/"},{"name":"现代密码学","slug":"现代密码学","permalink":"https://snowman12137.github.io/tags/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","permalink":"https://snowman12137.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"学校作业","slug":"学校作业","permalink":"https://snowman12137.github.io/tags/%E5%AD%A6%E6%A0%A1%E4%BD%9C%E4%B8%9A/"}]}